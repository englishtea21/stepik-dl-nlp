{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Свёрточные нейросети и POS-теггинг\n\nPOS-теггинг - определение частей речи (снятие частеречной неоднозначности)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/englishtea21/stepik-dl-nlp.git\n!pip install -r stepik-dl-nlp/requirements.txt\nimport sys;","metadata":{"execution":{"iopub.status.busy":"2024-07-29T14:59:27.650943Z","iopub.execute_input":"2024-07-29T14:59:27.651325Z","iopub.status.idle":"2024-07-29T15:00:46.589419Z","shell.execute_reply.started":"2024-07-29T14:59:27.651297Z","shell.execute_reply":"2024-07-29T15:00:46.588218Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Cloning into 'stepik-dl-nlp'...\nremote: Enumerating objects: 357, done.\u001b[K\nremote: Counting objects: 100% (63/63), done.\u001b[K\nremote: Compressing objects: 100% (51/51), done.\u001b[K\nremote: Total 357 (delta 28), reused 40 (delta 11), pack-reused 294\u001b[K\nReceiving objects: 100% (357/357), 101.38 MiB | 13.89 MiB/s, done.\nResolving deltas: 100% (162/162), done.\nUpdating files: 100% (62/62), done.\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.2.2)\nRequirement already satisfied: spacy-udpipe in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\nCollecting pymorphy2 (from -r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: torch>=1.2 in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (2.1.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.7.5)\nCollecting ipymarkup (from -r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (5.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.66.4)\nCollecting youtokentome (from -r stepik-dl-nlp/requirements.txt (line 11))\n  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.12.2)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (6.28.0)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (8.20.0)\nRequirement already satisfied: pyconll in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 15)) (3.2.0)\nCollecting gensim==3.8.1 (from -r stepik-dl-nlp/requirements.txt (line 16))\n  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wget (from -r stepik-dl-nlp/requirements.txt (line 17))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting livelossplot==0.5.3 (from -r stepik-dl-nlp/requirements.txt (line 18))\n  Downloading livelossplot-0.5.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.26.4)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.16.0)\nRequirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (6.4.0)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.10/site-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.5)\nRequirement already satisfied: ufal.udpipe>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.3.1.1)\nCollecting dawg-python>=0.7.1 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2024.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.9.0.post0)\nCollecting intervaltree>=3 (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.4)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.10/site-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (8.1.7)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.8.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.5.8)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (3.11.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.13)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.5.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (69.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (6.0.1)\nRequirement already satisfied: xyzservices>=2021.09.1 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2.1.3)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.4)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.2)\nDownloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\nDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\nDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hBuilding wheels for collected packages: gensim, youtokentome, wget, intervaltree\n  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.1-cp310-cp310-linux_x86_64.whl size=23986009 sha256=a69dbf857b7542770206f85abbd2dea75171f77b4d0e22f7b659f121ada554b3\n  Stored in directory: /root/.cache/pip/wheels/92/23/5d/b5ce54b3760acfebee170a8fe4d91cb303fafbefd8f93f3723\n  Building wheel for youtokentome (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=187956 sha256=d1e45f1fb4a4c14b8620c3c3d6e4662c943e6eb916a80531c24e68c663a031dd\n  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=9c58ef6cdbcfaef6d5a66183881a44f2a69364d9e93487330b24c30b8f742529\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26095 sha256=64b4f478cc5103040cad4f59d918a38119b1fb6f45bfeb1001948d3f4c01bb27\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built gensim youtokentome wget intervaltree\nInstalling collected packages: wget, pymorphy2-dicts-ru, dawg-python, youtokentome, pymorphy2, intervaltree, ipymarkup, gensim, livelossplot\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.2\n    Uninstalling gensim-4.3.2:\n      Successfully uninstalled gensim-4.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.19 requires gensim>=4.0.0, but you have gensim 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 wget-3.2 youtokentome-1.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/stepik-dl-nlp","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:00:46.591734Z","iopub.execute_input":"2024-07-29T15:00:46.592632Z","iopub.status.idle":"2024-07-29T15:00:46.636441Z","shell.execute_reply.started":"2024-07-29T15:00:46.592591Z","shell.execute_reply":"2024-07-29T15:00:46.635465Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/stepik-dl-nlp\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:01:11.345830Z","iopub.execute_input":"2024-07-29T15:01:11.346356Z","iopub.status.idle":"2024-07-29T15:01:11.382626Z","shell.execute_reply.started":"2024-07-29T15:01:11.346320Z","shell.execute_reply":"2024-07-29T15:01:11.381628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:01:12.111346Z","iopub.execute_input":"2024-07-29T15:01:12.112168Z","iopub.status.idle":"2024-07-29T15:01:14.436418Z","shell.execute_reply.started":"2024-07-29T15:01:12.112131Z","shell.execute_reply":"2024-07-29T15:01:14.435271Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"englishtea21@mail.ru\"\n!git config --global user.name \"englishtea21\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:01:14.438608Z","iopub.execute_input":"2024-07-29T15:01:14.438963Z","iopub.status.idle":"2024-07-29T15:01:16.493964Z","shell.execute_reply.started":"2024-07-29T15:01:14.438932Z","shell.execute_reply":"2024-07-29T15:01:16.492683Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!pip install pyconll\n!pip install spacy_udpipe","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:42:57.976431Z","start_time":"2019-10-29T19:42:57.959538Z"},"execution":{"iopub.status.busy":"2024-07-29T15:01:16.495628Z","iopub.execute_input":"2024-07-29T15:01:16.496030Z","iopub.status.idle":"2024-07-29T15:01:41.375574Z","shell.execute_reply.started":"2024-07-29T15:01:16.496000Z","shell.execute_reply":"2024-07-29T15:01:41.374436Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyconll in /opt/conda/lib/python3.10/site-packages (3.2.0)\nRequirement already satisfied: spacy_udpipe in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (3.7.5)\nRequirement already satisfied: ufal.udpipe>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (1.3.1.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import classification_report\n\nimport numpy as np\n\nimport pyconll\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\n\nimport dlnlputils\nfrom dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n    character_tokenize, pos_corpus_to_tensor, POSTagger\nfrom dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n\ninit_random_seed()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:34.549739Z","start_time":"2019-10-29T19:49:32.179692Z"},"execution":{"iopub.status.busy":"2024-07-29T15:01:41.378566Z","iopub.execute_input":"2024-07-29T15:01:41.378957Z","iopub.status.idle":"2024-07-29T15:01:44.383559Z","shell.execute_reply.started":"2024-07-29T15:01:41.378921Z","shell.execute_reply":"2024-07-29T15:01:44.382583Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Загрузка текстов и разбиение на обучающую и тестовую подвыборки","metadata":{}},{"cell_type":"code","source":"!wget -O datasets/ru_pos_tagging_samples.gz https://github.com/Koziev/rupostagger/blob/master/tmp/samples.gz","metadata":{"execution":{"iopub.status.busy":"2024-07-29T17:15:13.605786Z","iopub.execute_input":"2024-07-29T17:15:13.606247Z","iopub.status.idle":"2024-07-29T17:15:15.267525Z","shell.execute_reply.started":"2024-07-29T17:15:13.606214Z","shell.execute_reply":"2024-07-29T17:15:15.266338Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"--2024-07-29 17:15:14--  https://github.com/Koziev/rupostagger/blob/master/tmp/samples.gz\nResolving github.com (github.com)... 20.248.137.48\nConnecting to github.com (github.com)|20.248.137.48|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: 'datasets/ru_pos_tagging_samples.gz'\n\ndatasets/ru_pos_tag     [ <=>                ] 163.20K  --.-KB/s    in 0.01s   \n\n2024-07-29 17:15:15 (16.7 MB/s) - 'datasets/ru_pos_tagging_samples.gz' saved [167112]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import gzip\nimport shutil\n\ndef extract_gzip(input_path, output_path):\n    with gzip.open(input_path, 'rb') as f_in:\n        with open(output_path, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T17:08:53.768960Z","iopub.execute_input":"2024-07-29T17:08:53.769960Z","iopub.status.idle":"2024-07-29T17:08:53.833381Z","shell.execute_reply.started":"2024-07-29T17:08:53.769924Z","shell.execute_reply":"2024-07-29T17:08:53.832514Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"input_gzip_file = 'datasets/ru_pos_tagging_samples.gz'\noutput_file = 'datasets/ru_pos_tagging_samples.txt'\n\nextract_gzip(input_gzip_file, output_file)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T17:11:59.354651Z","iopub.execute_input":"2024-07-29T17:11:59.355131Z","iopub.status.idle":"2024-07-29T17:11:59.513238Z","shell.execute_reply.started":"2024-07-29T17:11:59.355070Z","shell.execute_reply":"2024-07-29T17:11:59.511931Z"},"trusted":true},"execution_count":118,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[118], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_gzip_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/ru_pos_tagging_samples.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/ru_pos_tagging_samples.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mextract_gzip\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gzip_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[115], line 7\u001b[0m, in \u001b[0;36mextract_gzip\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(input_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_out\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/shutil.py:195\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n","File \u001b[0;32m/opt/conda/lib/python3.10/gzip.py:488\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/gzip.py:436\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\037\u001b[39;00m\u001b[38;5;130;01m\\213\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadGzipFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot a gzipped file (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m magic)\n\u001b[1;32m    438\u001b[0m (method, flag,\n\u001b[1;32m    439\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_mtime) \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BBIxx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_exact(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m:\n","\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'\\n\\n')"],"ename":"BadGzipFile","evalue":"Not a gzipped file (b'\\n\\n')","output_type":"error"}]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# !wget -O datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n# !wget -O datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:08.433599Z","start_time":"2019-10-29T19:46:05.110693Z"},"execution":{"iopub.status.busy":"2024-07-29T07:10:30.037378Z","iopub.execute_input":"2024-07-29T07:10:30.037810Z","iopub.status.idle":"2024-07-29T07:10:33.843307Z","shell.execute_reply.started":"2024-07-29T07:10:30.037783Z","shell.execute_reply":"2024-07-29T07:10:33.842222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git status","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:10:38.278266Z","iopub.execute_input":"2024-07-29T07:10:38.278616Z","iopub.status.idle":"2024-07-29T07:10:39.976484Z","shell.execute_reply.started":"2024-07-29T07:10:38.278589Z","shell.execute_reply":"2024-07-29T07:10:39.975327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git add datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:10:44.368637Z","iopub.execute_input":"2024-07-29T07:10:44.369046Z","iopub.status.idle":"2024-07-29T07:10:46.415163Z","shell.execute_reply.started":"2024-07-29T07:10:44.369014Z","shell.execute_reply":"2024-07-29T07:10:46.414079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git commit -m 'pos-tagging datasets downloaded'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:10:48.164649Z","iopub.execute_input":"2024-07-29T07:10:48.165049Z","iopub.status.idle":"2024-07-29T07:10:49.458947Z","shell.execute_reply.started":"2024-07-29T07:10:48.165016Z","shell.execute_reply":"2024-07-29T07:10:49.457757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git push -u origin main","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:10:54.743172Z","iopub.execute_input":"2024-07-29T07:10:54.744220Z","iopub.status.idle":"2024-07-29T07:11:01.174363Z","shell.execute_reply.started":"2024-07-29T07:10:54.744171Z","shell.execute_reply":"2024-07-29T07:11:01.173219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nfull_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\nfull_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.525561Z","start_time":"2019-10-29T19:49:37.315213Z"},"execution":{"iopub.status.busy":"2024-07-29T15:01:44.384548Z","iopub.execute_input":"2024-07-29T15:01:44.385001Z","iopub.status.idle":"2024-07-29T15:02:00.081292Z","shell.execute_reply.started":"2024-07-29T15:01:44.384972Z","shell.execute_reply":"2024-07-29T15:02:00.080281Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for sent in full_train[:2]:\n    for token in sent:\n        print(token.form, token.upos)\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.548127Z","start_time":"2019-10-29T19:49:56.527559Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:00.082613Z","iopub.execute_input":"2024-07-29T15:02:00.082895Z","iopub.status.idle":"2024-07-29T15:02:00.137024Z","shell.execute_reply.started":"2024-07-29T15:02:00.082864Z","shell.execute_reply":"2024-07-29T15:02:00.136098Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Анкета NOUN\n. PUNCT\n\nНачальник NOUN\nобластного ADJ\nуправления NOUN\nсвязи NOUN\nСемен PROPN\nЕремеевич PROPN\nбыл AUX\nчеловек NOUN\nпростой ADJ\n, PUNCT\nприходил VERB\nна ADP\nработу NOUN\nвсегда ADV\nвовремя ADV\n, PUNCT\nздоровался VERB\nс ADP\nсекретаршей NOUN\nза ADP\nруку NOUN\nи CCONJ\nиногда ADV\nдаже PART\nписал VERB\nв ADP\nстенгазету NOUN\nзаметки NOUN\nпод ADP\nпсевдонимом NOUN\n\" PUNCT\nМуха NOUN\n\" PUNCT\n. PUNCT\n\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_SENT_LEN = max(len(sent) for sent in full_train)\nMAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\nprint('Наибольшая длина предложения', MAX_SENT_LEN)\nprint('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.916262Z","start_time":"2019-10-29T19:49:56.549806Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:00.138287Z","iopub.execute_input":"2024-07-29T15:02:00.138611Z","iopub.status.idle":"2024-07-29T15:02:00.374196Z","shell.execute_reply.started":"2024-07-29T15:02:00.138579Z","shell.execute_reply":"2024-07-29T15:02:00.373208Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Наибольшая длина предложения 194\nНаибольшая длина токена 31\n","output_type":"stream"}]},{"cell_type":"code","source":"all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\nprint('\\n'.join(all_train_texts[:10]))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:57.251433Z","start_time":"2019-10-29T19:49:56.919818Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:00.375323Z","iopub.execute_input":"2024-07-29T15:02:00.375594Z","iopub.status.idle":"2024-07-29T15:02:00.614665Z","shell.execute_reply.started":"2024-07-29T15:02:00.375571Z","shell.execute_reply":"2024-07-29T15:02:00.613474Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Анкета .\nНачальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\nВ приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\nОднако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\nПриемная была обставлена просто , но по-деловому .\nУ двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\nВ углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\nКабинет отличался скромностью , присущей Семену Еремеевичу .\nВ глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\nСправа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Для решения задачи pos-теггинга сверточными сетями будем применять посимвольную токенизацию текста. Этот подход оправдан, потому что принадлежность слова к той или иной части речи определяется наличием суффиксов, приставок и вида окончаний. При токенизации по более большим токенам мы бы не смогли уловить структуру слов.","metadata":{}},{"cell_type":"markdown","source":"Здесь нам также нужен фиктивной символ \"отсутсвтие символа\" для уравнивания длины всех предложений","metadata":{}},{"cell_type":"code","source":"train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\nchar_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\nprint(\"Количество уникальных символов\", len(char_vocab))\nprint(list(char_vocab.items())[:10])","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:00.618340Z","iopub.execute_input":"2024-07-29T15:02:00.618661Z","iopub.status.idle":"2024-07-29T15:02:01.121052Z","shell.execute_reply.started":"2024-07-29T15:02:00.618633Z","shell.execute_reply":"2024-07-29T15:02:01.120130Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Количество уникальных символов 142\n[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Аналогично назначим id меткам частей речи <br>\nТакже понадобится фиктивная метка части речи отсутствия части речи","metadata":{}},{"cell_type":"code","source":"UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\nlabel2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\nlabel2id","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.524125Z","start_time":"2019-10-29T19:49:58.125577Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:01.122726Z","iopub.execute_input":"2024-07-29T15:02:01.123066Z","iopub.status.idle":"2024-07-29T15:02:01.326480Z","shell.execute_reply.started":"2024-07-29T15:02:01.123039Z","shell.execute_reply":"2024-07-29T15:02:01.325533Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'<NOTAG>': 0,\n 'ADJ': 1,\n 'ADP': 2,\n 'ADV': 3,\n 'AUX': 4,\n 'CCONJ': 5,\n 'DET': 6,\n 'INTJ': 7,\n 'NOUN': 8,\n 'NUM': 9,\n 'PART': 10,\n 'PRON': 11,\n 'PROPN': 12,\n 'PUNCT': 13,\n 'SCONJ': 14,\n 'SYM': 15,\n 'VERB': 16,\n 'X': 17}"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут pos_corpus_to_tensor в качестве предложений принимает предложения в формате conllu где про каждое слово известна метка части речи. ","metadata":{}},{"cell_type":"markdown","source":"max_token_len + 2 нужно чтобы указать нейросети что определенная n-грамма символов встречается именно в начале токена или именно в конце токена, но не в середине\nэто нужно т.к. в процессе обучения свертки создают n-граммы символов и важно понимать отвечают эти символы началу слова или концу","metadata":{}},{"cell_type":"markdown","source":"Почему может понадобиться отличать начало и конец слова от середины слова?\n<br>\nОдна и та же последовательность символов может быть как частью суффикса, так и частью приставки, при этом неся разную функцию","metadata":{}},{"cell_type":"code","source":"def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            if token.form is None:\n                continue\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            for char_i, char in enumerate(token.form):\n                # тут мы сдвигаем заполнение символами на один, чтобы были незначащие нули в конце и в начале слова\n                # - показывают нейросети границы слова при разбиении его на n-граммы с помощью сверток\n                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n                            \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:02:01.327523Z","iopub.execute_input":"2024-07-29T15:02:01.327786Z","iopub.status.idle":"2024-07-29T15:02:01.380079Z","shell.execute_reply.started":"2024-07-29T15:02:01.327764Z","shell.execute_reply":"2024-07-29T15:02:01.379193Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:01.381200Z","iopub.execute_input":"2024-07-29T15:02:01.381561Z","iopub.status.idle":"2024-07-29T15:02:32.738907Z","shell.execute_reply.started":"2024-07-29T15:02:01.381526Z","shell.execute_reply":"2024-07-29T15:02:32.738079Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_inputs[1][:5]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.754883Z","start_time":"2019-10-29T19:49:40.582Z"},"scrolled":true,"execution":{"iopub.status.busy":"2024-07-29T07:12:44.801087Z","iopub.execute_input":"2024-07-29T07:12:44.801818Z","iopub.status.idle":"2024-07-29T07:12:44.878444Z","shell.execute_reply.started":"2024-07-29T07:12:44.801779Z","shell.execute_reply":"2024-07-29T07:12:44.877480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels[1]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.756496Z","start_time":"2019-10-29T19:49:40.711Z"},"execution":{"iopub.status.busy":"2024-07-29T07:12:44.879655Z","iopub.execute_input":"2024-07-29T07:12:44.880047Z","iopub.status.idle":"2024-07-29T07:12:44.934033Z","shell.execute_reply.started":"2024-07-29T07:12:44.880018Z","shell.execute_reply":"2024-07-29T07:12:44.933067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Вспомогательная свёрточная архитектура","metadata":{}},{"cell_type":"markdown","source":"Своего рода resnet, сумма реализует skip-connection","metadata":{}},{"cell_type":"code","source":"class StackedConv1d(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for _ in range(layers_n):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.316516Z","start_time":"2019-10-29T19:46:17.539Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:32.739993Z","iopub.execute_input":"2024-07-29T15:02:32.740285Z","iopub.status.idle":"2024-07-29T15:02:32.792272Z","shell.execute_reply.started":"2024-07-29T15:02:32.740260Z","shell.execute_reply":"2024-07-29T15:02:32.791495Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне отдельных токенов","metadata":{}},{"cell_type":"markdown","source":"эта модель никак не учитывает контекст, в котором встречается слово","metadata":{}},{"cell_type":"markdown","source":"потому будет ошибаться в разметке например в следующем предложении: <br>\n\nТри да три будет шесть или три да три будет дырка","metadata":{}},{"cell_type":"markdown","source":"Физический смысл модели - рассмотреть все возможные n-грамы и по ним определить часть речи для токена <br>\nТ.к. backbone использует skip-connection то учитываются n-грамы разной размерности, получаемые засчет различных сверток <br>\nНапример, если мы используем размер ядра свёртки, равный 3, то первый блок учитывает трёхграммы, второй блок уже учитывает пятиграммы, а третий — семиграммы, соответственно. При этом, благодаря тому, что есть \"skip connection\", информация о трёхграммах не теряется, она пробрасывать до самого конца.","metadata":{}},{"cell_type":"code","source":"class SingleTokenPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.backbone = StackedConv1d(embedding_size, **kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Linear(embedding_size, labels_num)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        \n        features = self.backbone(char_embeddings)\n        \n        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n        \n        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.317452Z","start_time":"2019-10-29T19:46:23.135Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:32.793353Z","iopub.execute_input":"2024-07-29T15:02:32.793683Z","iopub.status.idle":"2024-07-29T15:02:32.846334Z","shell.execute_reply.started":"2024-07-29T15:02:32.793659Z","shell.execute_reply":"2024-07-29T15:02:32.845260Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\nprint('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.318497Z","start_time":"2019-10-29T19:46:23.764Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:32.847538Z","iopub.execute_input":"2024-07-29T15:02:32.847958Z","iopub.status.idle":"2024-07-29T15:02:32.928798Z","shell.execute_reply.started":"2024-07-29T15:02:32.847926Z","shell.execute_reply":"2024-07-29T15:02:32.927898Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Количество параметров 47314\n","output_type":"stream"}]},{"cell_type":"code","source":"(best_val_loss,\n best_single_token_model) = train_eval_loop(single_token_model,\n                                            train_dataset,\n                                            test_dataset,\n                                            F.cross_entropy,\n                                            lr=5e-3,\n                                            epoch_n=10,\n                                            batch_size=64,\n                                            device='cuda',\n                                            early_stopping_patience=5,\n                                            max_batches_per_epoch_train=500,\n                                            max_batches_per_epoch_val=100,\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                       factor=0.5,\n                                                                                                                       verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.319470Z","start_time":"2019-10-29T19:46:25.552Z"},"execution":{"iopub.status.busy":"2024-07-29T14:52:48.877379Z","iopub.status.idle":"2024-07-29T14:52:48.877817Z","shell.execute_reply.started":"2024-07-29T14:52:48.877604Z","shell.execute_reply":"2024-07-29T14:52:48.877622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# newpath = r'models/task3_cnn_postag' \n# if not os.path.exists(newpath):\n#     os.makedirs(newpath)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:25:41.166786Z","iopub.execute_input":"2024-07-29T07:25:41.167692Z","iopub.status.idle":"2024-07-29T07:25:41.222517Z","shell.execute_reply.started":"2024-07-29T07:25:41.167658Z","shell.execute_reply":"2024-07-29T07:25:41.221641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# torch.save(best_single_token_model.state_dict(), 'models/task3_cnn_postag/baseline_single_token_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.320568Z","start_time":"2019-10-29T19:46:47.579Z"},"execution":{"iopub.status.busy":"2024-07-29T07:25:43.745381Z","iopub.execute_input":"2024-07-29T07:25:43.745762Z","iopub.status.idle":"2024-07-29T07:25:43.804783Z","shell.execute_reply.started":"2024-07-29T07:25:43.745732Z","shell.execute_reply":"2024-07-29T07:25:43.803685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git add models\n# !git commit -m 'pos-tagging baseline model added'\n# !git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:26:27.051945Z","iopub.execute_input":"2024-07-29T07:26:27.052363Z","iopub.status.idle":"2024-07-29T07:26:31.161254Z","shell.execute_reply.started":"2024-07-29T07:26:27.052331Z","shell.execute_reply":"2024-07-29T07:26:31.160029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsingle_token_model.load_state_dict(torch.load('models/task3_cnn_postag/baseline_single_token_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:32.930199Z","iopub.execute_input":"2024-07-29T15:02:32.930640Z","iopub.status.idle":"2024-07-29T15:02:33.115460Z","shell.execute_reply.started":"2024-07-29T15:02:32.930607Z","shell.execute_reply":"2024-07-29T15:02:33.114256Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"train_pred = predict_with_model(single_token_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(single_token_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:33.116603Z","iopub.execute_input":"2024-07-29T15:02:33.116916Z","iopub.status.idle":"2024-07-29T15:03:10.242600Z","shell.execute_reply.started":"2024-07-29T15:02:33.116891Z","shell.execute_reply":"2024-07-29T15:03:10.241509Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"767it [00:11, 68.80it/s]                             \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на обучении 0.022323843091726303\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   4330443\n         ADJ       0.89      0.92      0.91     43357\n         ADP       1.00      0.98      0.99     39344\n         ADV       0.90      0.88      0.89     22733\n         AUX       0.84      0.88      0.86      3537\n       CCONJ       0.88      0.98      0.93     15168\n         DET       0.76      0.91      0.83     10781\n        INTJ       0.00      0.00      0.00        50\n        NOUN       0.98      0.90      0.94    103538\n         NUM       0.91      0.92      0.91      5640\n        PART       0.93      0.80      0.86     13556\n        PRON       0.95      0.76      0.85     18734\n       PROPN       0.76      0.96      0.85     14854\n       PUNCT       1.00      1.00      1.00     77972\n       SCONJ       0.79      0.95      0.86      8057\n         SYM       1.00      0.99      0.99       420\n        VERB       0.90      0.96      0.93     47731\n           X       0.98      0.65      0.78       189\n\n    accuracy                           0.99   4756104\n   macro avg       0.86      0.86      0.85   4756104\nweighted avg       0.99      0.99      0.99   4756104\n\n\n","output_type":"stream"},{"name":"stderr","text":"279it [00:03, 76.75it/s]                              \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на валидации 0.02625696174800396\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   1574439\n         ADJ       0.86      0.90      0.88     15103\n         ADP       1.00      0.98      0.99     13717\n         ADV       0.88      0.86      0.87      7783\n         AUX       0.84      0.87      0.85      1390\n       CCONJ       0.89      0.98      0.93      5672\n         DET       0.76      0.87      0.81      4265\n        INTJ       0.00      0.00      0.00        24\n        NOUN       0.97      0.89      0.93     36238\n         NUM       0.82      0.86      0.84      1734\n        PART       0.93      0.78      0.85      5125\n        PRON       0.94      0.78      0.85      7444\n       PROPN       0.73      0.95      0.83      5473\n       PUNCT       1.00      1.00      1.00     29186\n       SCONJ       0.77      0.96      0.85      2865\n         SYM       1.00      0.85      0.92        62\n        VERB       0.88      0.95      0.91     17110\n           X       1.00      0.63      0.78       134\n\n    accuracy                           0.99   1727764\n   macro avg       0.85      0.84      0.84   1727764\nweighted avg       0.99      0.99      0.99   1727764\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Несмотря на простоту сверточной модели, результат на валидации не так далек от теста и при этом высок - 85%","metadata":{}},{"cell_type":"markdown","source":"Напомним, что наша модель никак не учитывает контекст токена при его pos-tagging'е, потому модель не справляется с задачей при частеречной омонимии","metadata":{}},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне предложений (с учётом контекста)","metadata":{}},{"cell_type":"markdown","source":"Тут используется 2 resnet'а - первый для построения векторного представления токена с учетом того из каких символов он составлен, второй же уже учитывает контекст токенов в предложении","metadata":{}},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.246289Z","iopub.execute_input":"2024-07-29T15:03:10.246577Z","iopub.status.idle":"2024-07-29T15:03:10.303292Z","shell.execute_reply.started":"2024-07-29T15:03:10.246554Z","shell.execute_reply":"2024-07-29T15:03:10.302450Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.326925Z","start_time":"2019-10-29T19:46:50.310Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.304513Z","iopub.execute_input":"2024-07-29T15:03:10.304812Z","iopub.status.idle":"2024-07-29T15:03:10.359627Z","shell.execute_reply.started":"2024-07-29T15:03:10.304786Z","shell.execute_reply":"2024-07-29T15:03:10.358754Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Количество параметров 84370\n","output_type":"stream"}]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model) = train_eval_loop(sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              F.cross_entropy,\n                                              lr=5e-3,\n                                              epoch_n=10,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2024-07-29T07:29:09.744280Z","iopub.execute_input":"2024-07-29T07:29:09.744955Z","iopub.status.idle":"2024-07-29T07:41:57.613976Z","shell.execute_reply.started":"2024-07-29T07:29:09.744923Z","shell.execute_reply":"2024-07-29T07:41:57.613009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# torch.save(best_sentence_level_model.state_dict(), 'models/task3_cnn_postag/baseline_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T07:42:23.045376Z","iopub.execute_input":"2024-07-29T07:42:23.046288Z","iopub.status.idle":"2024-07-29T07:42:23.103738Z","shell.execute_reply.started":"2024-07-29T07:42:23.046252Z","shell.execute_reply":"2024-07-29T07:42:23.102934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !git add models\n# !git commit -m 'pos-tagging sentence_level baseline model added'\n# !git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:42:46.099246Z","iopub.execute_input":"2024-07-29T07:42:46.099640Z","iopub.status.idle":"2024-07-29T07:42:50.571955Z","shell.execute_reply.started":"2024-07-29T07:42:46.099609Z","shell.execute_reply":"2024-07-29T07:42:50.571031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/baseline_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.564926Z","start_time":"2019-08-29T13:56:16.544481Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.360812Z","iopub.execute_input":"2024-07-29T15:03:10.361172Z","iopub.status.idle":"2024-07-29T15:03:10.415168Z","shell.execute_reply.started":"2024-07-29T15:03:10.361145Z","shell.execute_reply":"2024-07-29T15:03:10.414228Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"train_pred = predict_with_model(sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.092139Z","start_time":"2019-08-29T13:56:16.567242Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.416419Z","iopub.execute_input":"2024-07-29T15:03:10.416835Z","iopub.status.idle":"2024-07-29T15:03:45.807254Z","shell.execute_reply.started":"2024-07-29T15:03:10.416803Z","shell.execute_reply":"2024-07-29T15:03:45.806290Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"767it [00:10, 74.78it/s]                             \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на обучении 0.012924276292324066\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   4330443\n         ADJ       0.92      0.93      0.92     43357\n         ADP       1.00      0.99      0.99     39344\n         ADV       0.91      0.88      0.89     22733\n         AUX       0.90      0.84      0.87      3537\n       CCONJ       0.94      0.97      0.95     15168\n         DET       0.82      0.96      0.88     10781\n        INTJ       0.91      0.20      0.33        50\n        NOUN       0.96      0.97      0.96    103538\n         NUM       0.96      0.85      0.90      5640\n        PART       0.95      0.87      0.91     13556\n        PRON       0.98      0.84      0.90     18734\n       PROPN       0.94      0.96      0.95     14854\n       PUNCT       1.00      1.00      1.00     77972\n       SCONJ       0.84      0.96      0.90      8057\n         SYM       1.00      1.00      1.00       420\n        VERB       0.94      0.96      0.95     47731\n           X       0.99      0.58      0.73       189\n\n    accuracy                           1.00   4756104\n   macro avg       0.94      0.88      0.89   4756104\nweighted avg       1.00      1.00      1.00   4756104\n\n\n","output_type":"stream"},{"name":"stderr","text":"279it [00:03, 74.57it/s]                              \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на валидации 0.017020050436258316\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   1574439\n         ADJ       0.90      0.91      0.90     15103\n         ADP       0.99      0.99      0.99     13717\n         ADV       0.88      0.86      0.87      7783\n         AUX       0.90      0.81      0.85      1390\n       CCONJ       0.94      0.97      0.96      5672\n         DET       0.82      0.91      0.86      4265\n        INTJ       1.00      0.29      0.45        24\n        NOUN       0.95      0.96      0.96     36238\n         NUM       0.90      0.78      0.83      1734\n        PART       0.94      0.84      0.89      5125\n        PRON       0.97      0.84      0.90      7444\n       PROPN       0.91      0.94      0.92      5473\n       PUNCT       1.00      1.00      1.00     29186\n       SCONJ       0.83      0.96      0.89      2865\n         SYM       0.91      0.97      0.94        62\n        VERB       0.92      0.95      0.94     17110\n           X       0.98      0.30      0.46       134\n\n    accuracy                           0.99   1727764\n   macro avg       0.93      0.85      0.87   1727764\nweighted avg       0.99      0.99      0.99   1727764\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Применение полученных теггеров и сравнение","metadata":{}},{"cell_type":"code","source":"single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nsentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:58.350633Z","iopub.execute_input":"2024-07-29T15:03:58.351409Z","iopub.status.idle":"2024-07-29T15:03:58.400950Z","shell.execute_reply.started":"2024-07-29T15:03:58.351357Z","shell.execute_reply":"2024-07-29T15:03:58.399935Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.125540Z","start_time":"2019-08-29T13:56:42.106771Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:59.058225Z","iopub.execute_input":"2024-07-29T15:03:59.058910Z","iopub.status.idle":"2024-07-29T15:03:59.109927Z","shell.execute_reply.started":"2024-07-29T15:03:59.058868Z","shell.execute_reply":"2024-07-29T15:03:59.108902Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.126930Z"},"execution":{"iopub.status.busy":"2024-07-29T15:04:00.221060Z","iopub.execute_input":"2024-07-29T15:04:00.221475Z","iopub.status.idle":"2024-07-29T15:04:00.286352Z","shell.execute_reply.started":"2024-07-29T15:04:00.221446Z","shell.execute_reply":"2024-07-29T15:04:00.285429Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"1it [00:00, 155.07it/s]                    ","output_type":"stream"},{"name":"stdout","text":"мама-NOUN мыла-VERB раму-NOUN\n\nкосил-VERB косой-ADJ косой-ADJ косой-ADJ\n\nглокая-ADJ куздра-NOUN штеко-ADV будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n\nсяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n\nпирожки-NOUN поставлены-VERB в-ADP печь-VERB мама-NOUN любит-VERB печь-VERB\n\nведро-NOUN дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n\nтри-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n\nтри-NUM да-CCONJ три-NUM будет-AUX шесть-VERB\n\nсорок-NOUN сорок-NOUN\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.168810Z","start_time":"2019-08-29T13:56:42.149698Z"},"execution":{"iopub.status.busy":"2024-07-29T15:04:00.706884Z","iopub.execute_input":"2024-07-29T15:04:00.707282Z","iopub.status.idle":"2024-07-29T15:04:00.770287Z","shell.execute_reply.started":"2024-07-29T15:04:00.707250Z","shell.execute_reply":"2024-07-29T15:04:00.769335Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"1it [00:00, 178.94it/s]                    ","output_type":"stream"},{"name":"stdout","text":"мама-NOUN мыла-NOUN раму-NOUN\n\nкосил-VERB косой-ADJ косой-ADJ косой-NOUN\n\nглокая-ADJ куздра-NOUN штеко-NOUN будланула-VERB бокра-ADV и-CCONJ куздрячит-VERB бокрёнка-NOUN\n\nсяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n\nпирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n\nведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n\nтри-NUM да-CCONJ три-NUM будет-AUX дырка-NOUN\n\nтри-NUM да-CCONJ три-NUM будет-AUX шесть-VERB\n\nсорок-NOUN сорок-NOUN\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Как-то наша sentence based модель не очень справляется, показывает себя хуже, чем предыдущая более простая. Скорее всего она переобучается. Далее возьмем dilated conv вместо обычный и увеличим dropout.","metadata":{}},{"cell_type":"markdown","source":"## Свёрточный модуль своими руками","metadata":{}},{"cell_type":"code","source":"# class MyConv1d(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n#         super().__init__()\n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.kernel_size = kernel_size\n#         self.padding = padding\n#         self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n#                                    requires_grad=True)\n#         self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n    \n#     def forward(self, x):\n#         \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n\n#         batch_size, src_channels, sequence_len = x.shape        \n#         if self.padding > 0:\n#             pad = x.new_zeros(batch_size, src_channels, self.padding)\n#             x = torch.cat((pad, x, pad), dim=-1)\n#             sequence_len = x.shape[-1]\n\n#         chunks = []\n#         chunk_size = sequence_len - self.kernel_size + 1\n#         for offset in range(self.kernel_size):\n#             chunks.append(x[:, :, offset:offset + chunk_size])\n\n#         in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n#         in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n#         out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n#         out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n#         return out_features","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.193140Z","start_time":"2019-08-29T13:56:42.170233Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n#                                                       single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n#                                                       context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.210013Z","start_time":"2019-08-29T13:56:42.194620Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (best_val_loss,\n#  best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n#                                                       train_dataset,\n#                                                       test_dataset,\n#                                                       F.cross_entropy,\n#                                                       lr=5e-3,\n#                                                       epoch_n=10,\n#                                                       batch_size=64,\n#                                                       device='cuda',\n#                                                       early_stopping_patience=5,\n#                                                       max_batches_per_epoch_train=500,\n#                                                       max_batches_per_epoch_val=100,\n#                                                       lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n#                                                                                                                                  factor=0.5,\n#                                                                                                                                  verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:00.233326Z","start_time":"2019-08-29T13:56:42.211456Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n# train_loss = F.cross_entropy(torch.tensor(train_pred),\n#                              torch.tensor(train_labels))\n# print('Среднее значение функции потерь на обучении', float(train_loss))\n# print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n# print()\n\n# test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n# test_loss = F.cross_entropy(torch.tensor(test_pred),\n#                             torch.tensor(test_labels))\n# print('Среднее значение функции потерь на валидации', float(test_loss))\n# print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:39.145214Z","start_time":"2019-08-29T14:06:00.234936Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Улучшим качество pos-tagging'а, учитывающего контекст","metadata":{}},{"cell_type":"markdown","source":"Идеи:\n- Взвешивание классов\n- Батч нормализация в архитектуре сети\n- Подключение прореженных сверток\n- использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)\n- Поиграться с численными параметрами модели - с размерностью скрытого представления, числом stacked conv, силой dropout","metadata":{}},{"cell_type":"markdown","source":"Т.к. учить долго лень сравнивать будем при обучении на 5 эпохах","metadata":{}},{"cell_type":"markdown","source":"##  Взвесим классы","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import class_weight","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:27.599776Z","iopub.execute_input":"2024-07-29T15:04:27.600466Z","iopub.status.idle":"2024-07-29T15:04:27.650035Z","shell.execute_reply.started":"2024-07-29T15:04:27.600432Z","shell.execute_reply":"2024-07-29T15:04:27.649140Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:28.569237Z","iopub.execute_input":"2024-07-29T15:04:28.570109Z","iopub.status.idle":"2024-07-29T15:04:28.623166Z","shell.execute_reply.started":"2024-07-29T15:04:28.570063Z","shell.execute_reply":"2024-07-29T15:04:28.622104Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([[ 8, 13,  0,  ...,  0,  0,  0],\n        [ 8,  1,  8,  ...,  0,  0,  0],\n        [ 2,  8, 11,  ...,  0,  0,  0],\n        ...,\n        [ 3, 16, 13,  ...,  0,  0,  0],\n        [ 3, 14, 16,  ...,  0,  0,  0],\n        [ 2,  8,  8,  ...,  0,  0,  0]])"},"metadata":{}}]},{"cell_type":"code","source":"train_labels.flatten()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:29.244905Z","iopub.execute_input":"2024-07-29T15:04:29.245572Z","iopub.status.idle":"2024-07-29T15:04:29.297975Z","shell.execute_reply.started":"2024-07-29T15:04:29.245539Z","shell.execute_reply":"2024-07-29T15:04:29.296998Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([ 8, 13,  0,  ...,  0,  0,  0])"},"metadata":{}}]},{"cell_type":"code","source":"# Compute class weights using sklearn\nclass_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels.flatten().numpy()), y=train_labels.flatten().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:10.699533Z","iopub.execute_input":"2024-07-29T15:05:10.699921Z","iopub.status.idle":"2024-07-29T15:05:12.394601Z","shell.execute_reply.started":"2024-07-29T15:05:10.699892Z","shell.execute_reply":"2024-07-29T15:05:12.393730Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class_weights.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:13.692222Z","iopub.execute_input":"2024-07-29T15:05:13.693172Z","iopub.status.idle":"2024-07-29T15:05:13.745169Z","shell.execute_reply.started":"2024-07-29T15:05:13.693130Z","shell.execute_reply":"2024-07-29T15:05:13.744142Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(18,)"},"metadata":{}}]},{"cell_type":"code","source":"class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:15.025177Z","iopub.execute_input":"2024-07-29T15:05:15.026004Z","iopub.status.idle":"2024-07-29T15:05:15.077355Z","shell.execute_reply.started":"2024-07-29T15:05:15.025972Z","shell.execute_reply":"2024-07-29T15:05:15.076439Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class_weights_tensor = class_weights_tensor.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:16.185699Z","iopub.execute_input":"2024-07-29T15:05:16.186057Z","iopub.status.idle":"2024-07-29T15:05:16.236861Z","shell.execute_reply.started":"2024-07-29T15:05:16.186027Z","shell.execute_reply":"2024-07-29T15:05:16.235953Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Для эмбеддинга начала и конца токена используем ненулевые значения","metadata":{}},{"cell_type":"code","source":"def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.ones((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            if token.form is None:\n                continue\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            for char_i, char in enumerate(token.form):\n                # тут мы сдвигаем заполнение символами на один, чтобы были незначащие нули в конце и в начале слова\n                # - показывают нейросети границы слова при разбиении его на n-граммы с помощью сверток\n                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n                            \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:01:48.071544Z","iopub.execute_input":"2024-07-29T16:01:48.071961Z","iopub.status.idle":"2024-07-29T16:01:48.132355Z","shell.execute_reply.started":"2024-07-29T16:01:48.071935Z","shell.execute_reply":"2024-07-29T16:01:48.131231Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2024-07-29T16:01:51.247183Z","iopub.execute_input":"2024-07-29T16:01:51.247871Z","iopub.status.idle":"2024-07-29T16:02:20.226463Z","shell.execute_reply.started":"2024-07-29T16:01:51.247840Z","shell.execute_reply":"2024-07-29T16:02:20.225590Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"## Включим взвешивание и batch norm","metadata":{}},{"cell_type":"code","source":"class StackedConv1dBatchNorm(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for i in range(1, layers_n+1):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2, dilation=1),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(features_num),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:24.745405Z","iopub.execute_input":"2024-07-29T16:02:24.746392Z","iopub.status.idle":"2024-07-29T16:02:24.805472Z","shell.execute_reply.started":"2024-07-29T16:02:24.746348Z","shell.execute_reply":"2024-07-29T16:02:24.804606Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTaggerBatchNorm(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1dBatchNorm(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1dBatchNorm(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:26.144696Z","iopub.execute_input":"2024-07-29T16:02:26.145058Z","iopub.status.idle":"2024-07-29T16:02:26.205816Z","shell.execute_reply.started":"2024-07-29T16:02:26.145030Z","shell.execute_reply":"2024-07-29T16:02:26.204945Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_sentence_level_model = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:31.983460Z","iopub.execute_input":"2024-07-29T16:02:31.984302Z","iopub.status.idle":"2024-07-29T16:02:32.046109Z","shell.execute_reply.started":"2024-07-29T16:02:31.984268Z","shell.execute_reply":"2024-07-29T16:02:32.045062Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Количество параметров 85138\n","output_type":"stream"}]},{"cell_type":"code","source":"# (best_val_loss,\n#  best_batch_norm_class_weighting_sentence_level_model) = train_eval_loop(batch_norm_class_weighting_sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               nn.CrossEntropyLoss(weight=class_weights_tensor),\n#                                               lr=5e-3,\n#                                               epoch_n=5,\n#                                               batch_size=64,\n#                                               device='cuda',\n#                                               early_stopping_patience=5,\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=100,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n#                                                                                                                          factor=0.5,\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:37.028329Z","iopub.execute_input":"2024-07-29T16:02:37.028717Z","iopub.status.idle":"2024-07-29T16:13:44.930595Z","shell.execute_reply.started":"2024-07-29T16:02:37.028684Z","shell.execute_reply":"2024-07-29T16:13:44.929535Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"Эпоха 0\nЭпоха: 384 итераций, 130.56 сек\nСреднее значение функции потерь на обучении 1.489722478358696\nСреднее значение функции потерь на валидации 0.9170214262339148\nНовая лучшая модель!\n\nЭпоха 1\nЭпоха: 384 итераций, 130.56 сек\nСреднее значение функции потерь на обучении 0.7395414276979864\nСреднее значение функции потерь на валидации 0.9251023501452833\n\nЭпоха 2\nЭпоха: 384 итераций, 130.57 сек\nСреднее значение функции потерь на обучении 0.5325855107124274\nСреднее значение функции потерь на валидации 0.5427643520997303\nНовая лучшая модель!\n\nЭпоха 3\nЭпоха: 384 итераций, 130.55 сек\nСреднее значение функции потерь на обучении 0.41359463578555733\nСреднее значение функции потерь на валидации 0.5219365089836687\nНовая лучшая модель!\n\nЭпоха 4\nЭпоха: 384 итераций, 130.55 сек\nСреднее значение функции потерь на обучении 0.39389619064362097\nСреднее значение функции потерь на валидации 0.4099343696443161\nНовая лучшая модель!\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.save(best_batch_norm_class_weighting_sentence_level_model.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:13:49.609341Z","iopub.execute_input":"2024-07-29T16:13:49.609732Z","iopub.status.idle":"2024-07-29T16:13:49.670369Z","shell.execute_reply.started":"2024-07-29T16:13:49.609701Z","shell.execute_reply":"2024-07-29T16:13:49.669378Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"# !git add models\n# !git commit -m 'pos-tagging class_weight_batch_norm_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:13:56.953766Z","iopub.execute_input":"2024-07-29T16:13:56.954215Z","iopub.status.idle":"2024-07-29T16:13:59.251277Z","shell.execute_reply.started":"2024-07-29T16:13:56.954182Z","shell.execute_reply":"2024-07-29T16:13:59.250125Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"[main b33846e] pos-tagging class_weight_batch_norm_sentence_level_pos added\n 1 file changed, 0 insertions(+), 0 deletions(-)\n rewrite models/task3_cnn_postag/class_weight_batch_norm_sentence_level_pos.pth (92%)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:14:04.098952Z","iopub.execute_input":"2024-07-29T16:14:04.099369Z","iopub.status.idle":"2024-07-29T16:14:07.493403Z","shell.execute_reply.started":"2024-07-29T16:14:04.099337Z","shell.execute_reply":"2024-07-29T16:14:07.492212Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"Enumerating objects: 9, done.\nCounting objects: 100% (9/9), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 317.80 KiB | 13.24 MiB/s, done.\nTotal 5 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\nTo https://github.com/englishtea21/stepik-dl-nlp.git\n   3269a91..b33846e  main -> main\n","output_type":"stream"}]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nbest_batch_norm_class_weighting_sentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T16:14:34.984332Z","iopub.execute_input":"2024-07-29T16:14:34.985186Z","iopub.status.idle":"2024-07-29T16:14:35.059982Z","shell.execute_reply.started":"2024-07-29T16:14:34.985150Z","shell.execute_reply":"2024-07-29T16:14:35.059103Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:14:41.962750Z","iopub.execute_input":"2024-07-29T16:14:41.963596Z","iopub.status.idle":"2024-07-29T16:15:19.377694Z","shell.execute_reply.started":"2024-07-29T16:14:41.963560Z","shell.execute_reply":"2024-07-29T16:15:19.376689Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"767it [00:11, 64.25it/s]                             \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на обучении 0.09282194077968597\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   4330443\n         ADJ       0.72      0.88      0.79     43357\n         ADP       0.98      0.96      0.97     39344\n         ADV       0.60      0.78      0.68     22733\n         AUX       0.69      1.00      0.81      3537\n       CCONJ       0.94      0.90      0.92     15168\n         DET       0.81      0.75      0.78     10781\n        INTJ       0.03      0.98      0.05        50\n        NOUN       0.97      0.65      0.78    103538\n         NUM       0.72      0.96      0.82      5640\n        PART       0.86      0.87      0.86     13556\n        PRON       0.78      0.92      0.85     18734\n       PROPN       0.86      0.90      0.88     14854\n       PUNCT       1.00      1.00      1.00     77972\n       SCONJ       0.80      0.89      0.84      8057\n         SYM       0.97      0.99      0.98       420\n        VERB       0.73      0.88      0.80     47731\n           X       0.41      0.86      0.55       189\n\n    accuracy                           0.99   4756104\n   macro avg       0.77      0.90      0.80   4756104\nweighted avg       0.99      0.99      0.99   4756104\n\n\n","output_type":"stream"},{"name":"stderr","text":"279it [00:04, 64.50it/s]                              \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на валидации 0.09504839777946472\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   1574439\n         ADJ       0.69      0.87      0.77     15103\n         ADP       0.98      0.95      0.97     13717\n         ADV       0.56      0.76      0.65      7783\n         AUX       0.72      1.00      0.83      1390\n       CCONJ       0.95      0.91      0.93      5672\n         DET       0.79      0.70      0.74      4265\n        INTJ       0.02      0.67      0.04        24\n        NOUN       0.97      0.64      0.77     36238\n         NUM       0.65      0.91      0.76      1734\n        PART       0.86      0.84      0.85      5125\n        PRON       0.77      0.91      0.84      7444\n       PROPN       0.85      0.87      0.86      5473\n       PUNCT       1.00      1.00      1.00     29186\n       SCONJ       0.79      0.89      0.83      2865\n         SYM       0.98      0.89      0.93        62\n        VERB       0.73      0.87      0.80     17110\n           X       0.44      0.75      0.55       134\n\n    accuracy                           0.99   1727764\n   macro avg       0.76      0.86      0.78   1727764\nweighted avg       0.99      0.99      0.99   1727764\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Увеличим число слоев и размер эмбеддинга","metadata":{}},{"cell_type":"code","source":"batch_norm_class_weighting_sentence_level_model_more_layers = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=128,\n                                              single_backbone_kwargs=dict(layers_n=5, kernel_size=3, dropout=0.1),\n                                              context_backbone_kwargs=dict(layers_n=5, kernel_size=3, dropout=0.1))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:15:31.877788Z","iopub.execute_input":"2024-07-29T16:15:31.878881Z","iopub.status.idle":"2024-07-29T16:15:31.945120Z","shell.execute_reply.started":"2024-07-29T16:15:31.878827Z","shell.execute_reply":"2024-07-29T16:15:31.944243Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Количество параметров 85138\n","output_type":"stream"}]},{"cell_type":"code","source":"# (best_val_loss,\n#  best_batch_norm_class_weighting_sentence_level_model_more_layers) = train_eval_loop(batch_norm_class_weighting_sentence_level_model_more_layers,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               nn.CrossEntropyLoss(weight=class_weights_tensor),\n#                                               lr=5e-3,\n#                                               epoch_n=5,\n#                                               batch_size=64,\n#                                               device='cuda',\n#                                               early_stopping_patience=5,\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=100,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n#                                                                                                                          factor=0.5,\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:15:50.036664Z","iopub.execute_input":"2024-07-29T16:15:50.037569Z","iopub.status.idle":"2024-07-29T16:29:10.819165Z","shell.execute_reply.started":"2024-07-29T16:15:50.037536Z","shell.execute_reply":"2024-07-29T16:29:10.818100Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Эпоха 0\nЭпоха: 384 итераций, 151.57 сек\nСреднее значение функции потерь на обучении 2.0472767004588\nСреднее значение функции потерь на валидации 0.6891701504735663\nНовая лучшая модель!\n\nЭпоха 1\nЭпоха: 384 итераций, 151.54 сек\nСреднее значение функции потерь на обучении 0.6392523761993895\nСреднее значение функции потерь на валидации 0.5712530756055718\nНовая лучшая модель!\n\nЭпоха 2\nЭпоха: 384 итераций, 151.58 сек\nСреднее значение функции потерь на обучении 0.4530828347584854\nСреднее значение функции потерь на валидации 0.5464011913183893\nНовая лучшая модель!\n\nЭпоха 3\nЭпоха: 384 итераций, 151.56 сек\nСреднее значение функции потерь на обучении 0.3647496024495922\nСреднее значение функции потерь на валидации 0.37523321696732304\nНовая лучшая модель!\n\nЭпоха 4\nЭпоха: 384 итераций, 151.52 сек\nСреднее значение функции потерь на обучении 0.3471770300335872\nСреднее значение функции потерь на валидации 0.415866459286449\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.save(best_batch_norm_class_weighting_sentence_level_model_more_layers.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_more_layers_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:29:24.311858Z","iopub.execute_input":"2024-07-29T16:29:24.312623Z","iopub.status.idle":"2024-07-29T16:29:24.379454Z","shell.execute_reply.started":"2024-07-29T16:29:24.312588Z","shell.execute_reply":"2024-07-29T16:29:24.378584Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# !git add models\n# !git commit -m 'pos-tagging class_weight_batch_norm_more_layers_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:28.273918Z","iopub.execute_input":"2024-07-29T16:29:28.274674Z","iopub.status.idle":"2024-07-29T16:29:30.662390Z","shell.execute_reply.started":"2024-07-29T16:29:28.274641Z","shell.execute_reply":"2024-07-29T16:29:30.661274Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"[main d3ac6b1] pos-tagging class_weight_batch_norm_more_layers_sentence_level_pos added\n 1 file changed, 0 insertions(+), 0 deletions(-)\n rewrite models/task3_cnn_postag/class_weight_batch_norm_more_layers_sentence_level_pos.pth (73%)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:33.381617Z","iopub.execute_input":"2024-07-29T16:29:33.381996Z","iopub.status.idle":"2024-07-29T16:29:37.873244Z","shell.execute_reply.started":"2024-07-29T16:29:33.381968Z","shell.execute_reply":"2024-07-29T16:29:37.872125Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Enumerating objects: 9, done.\nCounting objects: 100% (9/9), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 1.85 MiB | 3.56 MiB/s, done.\nTotal 5 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\nTo https://github.com/englishtea21/stepik-dl-nlp.git\n   b33846e..d3ac6b1  main -> main\n","output_type":"stream"}]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nbest_batch_norm_class_weighting_sentence_level_model_more_layers.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_more_layers_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T15:57:12.832951Z","iopub.execute_input":"2024-07-29T15:57:12.833803Z","iopub.status.idle":"2024-07-29T15:57:12.913392Z","shell.execute_reply.started":"2024-07-29T15:57:12.833768Z","shell.execute_reply":"2024-07-29T15:57:12.912452Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model_more_layers, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model_more_layers, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:29:55.663989Z","iopub.execute_input":"2024-07-29T16:29:55.664407Z","iopub.status.idle":"2024-07-29T16:31:03.678840Z","shell.execute_reply.started":"2024-07-29T16:29:55.664375Z","shell.execute_reply":"2024-07-29T16:31:03.677809Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stderr","text":"767it [00:33, 22.94it/s]                             \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на обучении 0.03236206993460655\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   4330443\n         ADJ       0.93      0.73      0.82     43357\n         ADP       0.99      0.96      0.98     39344\n         ADV       0.55      0.91      0.68     22733\n         AUX       0.73      0.99      0.84      3537\n       CCONJ       0.95      0.94      0.94     15168\n         DET       0.87      0.88      0.87     10781\n        INTJ       0.14      0.96      0.24        50\n        NOUN       0.95      0.81      0.87    103538\n         NUM       0.71      0.96      0.82      5640\n        PART       0.94      0.82      0.88     13556\n        PRON       0.87      0.91      0.89     18734\n       PROPN       0.78      0.95      0.86     14854\n       PUNCT       1.00      1.00      1.00     77972\n       SCONJ       0.82      0.97      0.89      8057\n         SYM       0.87      1.00      0.93       420\n        VERB       0.86      0.88      0.87     47731\n           X       0.09      0.94      0.16       189\n\n    accuracy                           0.99   4756104\n   macro avg       0.78      0.92      0.81   4756104\nweighted avg       0.99      0.99      0.99   4756104\n\n\n","output_type":"stream"},{"name":"stderr","text":"279it [00:12, 22.84it/s]                              \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на валидации 0.03587852418422699\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   1574439\n         ADJ       0.91      0.71      0.80     15103\n         ADP       0.99      0.96      0.98     13717\n         ADV       0.51      0.90      0.66      7783\n         AUX       0.75      0.99      0.86      1390\n       CCONJ       0.95      0.94      0.95      5672\n         DET       0.86      0.83      0.84      4265\n        INTJ       0.13      0.75      0.22        24\n        NOUN       0.95      0.79      0.86     36238\n         NUM       0.62      0.92      0.74      1734\n        PART       0.92      0.79      0.85      5125\n        PRON       0.85      0.90      0.88      7444\n       PROPN       0.78      0.93      0.85      5473\n       PUNCT       1.00      1.00      1.00     29186\n       SCONJ       0.80      0.96      0.87      2865\n         SYM       0.75      1.00      0.86        62\n        VERB       0.85      0.88      0.87     17110\n           X       0.13      0.87      0.22       134\n\n    accuracy                           0.99   1727764\n   macro avg       0.76      0.90      0.79   1727764\nweighted avg       0.99      0.99      0.99   1727764\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Добавим dilated convs и увеличим число слоев","metadata":{}},{"cell_type":"code","source":"class StackedConv1dBatchNormDilatedConvs(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for i in range(0, layers_n):\n            dilation_rate = 2 ** i\n            padding=(kernel_size // 2) * dilation_rate\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=padding, dilation=dilation_rate),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(features_num),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:31:38.283450Z","iopub.execute_input":"2024-07-29T16:31:38.284114Z","iopub.status.idle":"2024-07-29T16:31:38.344105Z","shell.execute_reply.started":"2024-07-29T16:31:38.284063Z","shell.execute_reply":"2024-07-29T16:31:38.343206Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTaggerBatchNormDilatedConvs(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1dBatchNormDilatedConvs(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1dBatchNormDilatedConvs(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:31:46.094807Z","iopub.execute_input":"2024-07-29T16:31:46.095190Z","iopub.status.idle":"2024-07-29T16:31:46.155379Z","shell.execute_reply.started":"2024-07-29T16:31:46.095159Z","shell.execute_reply":"2024-07-29T16:31:46.154286Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_dilated_convs_sentence_level_model = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=128,\n                                              single_backbone_kwargs=dict(layers_n=6, kernel_size=3, dropout=0.2),\n                                              context_backbone_kwargs=dict(layers_n=6, kernel_size=3, dropout=0.2))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:12.765241Z","iopub.execute_input":"2024-07-29T16:32:12.766183Z","iopub.status.idle":"2024-07-29T16:32:12.834648Z","shell.execute_reply.started":"2024-07-29T16:32:12.766148Z","shell.execute_reply":"2024-07-29T16:32:12.833721Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"Количество параметров 85138\n","output_type":"stream"}]},{"cell_type":"code","source":"# (best_val_loss,\n#  best_batch_norm_class_weighting_dilated_convs_sentence_level_model) = train_eval_loop(batch_norm_class_weighting_dilated_convs_sentence_level_model,\n#                                               train_dataset,\n#                                               test_dataset,\n#                                               nn.CrossEntropyLoss(weight=class_weights_tensor),\n#                                               lr=5e-3,\n#                                               epoch_n=5,\n#                                               batch_size=64,\n#                                               device='cuda',\n#                                               early_stopping_patience=5,\n#                                               max_batches_per_epoch_train=500,\n#                                               max_batches_per_epoch_val=100,\n#                                               lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n#                                                                                                                          factor=0.5,\n#                                                                                                                          verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:35.471313Z","iopub.execute_input":"2024-07-29T16:32:35.471714Z","iopub.status.idle":"2024-07-29T16:48:23.435802Z","shell.execute_reply.started":"2024-07-29T16:32:35.471684Z","shell.execute_reply":"2024-07-29T16:48:23.434821Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"Эпоха 0\nЭпоха: 384 итераций, 179.64 сек\nСреднее значение функции потерь на обучении 2.072048139292747\nСреднее значение функции потерь на валидации 0.863330078006971\nНовая лучшая модель!\n\nЭпоха 1\nЭпоха: 384 итераций, 179.55 сек\nСреднее значение функции потерь на обучении 0.747695760258163\nСреднее значение функции потерь на валидации 0.6841084254850255\nНовая лучшая модель!\n\nЭпоха 2\nЭпоха: 384 итераций, 179.56 сек\nСреднее значение функции потерь на обучении 0.5210403630432362\nСреднее значение функции потерь на валидации 0.41839053680991184\nНовая лучшая модель!\n\nЭпоха 3\nЭпоха: 384 итераций, 179.56 сек\nСреднее значение функции потерь на обучении 0.3901117949280888\nСреднее значение функции потерь на валидации 0.4693059958148711\n\nЭпоха 4\nЭпоха: 384 итераций, 179.55 сек\nСреднее значение функции потерь на обучении 0.3741791774518788\nСреднее значение функции потерь на валидации 0.4333892391607313\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.save(best_batch_norm_class_weighting_dilated_convs_sentence_level_model.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_dilated_convs_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:48:53.727562Z","iopub.execute_input":"2024-07-29T16:48:53.728270Z","iopub.status.idle":"2024-07-29T16:48:53.798401Z","shell.execute_reply.started":"2024-07-29T16:48:53.728236Z","shell.execute_reply":"2024-07-29T16:48:53.797386Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# !git add models\n# !git commit -m 'pos-tagging class_weight_batch_norm_dilated_convs_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:48:57.659053Z","iopub.execute_input":"2024-07-29T16:48:57.659763Z","iopub.status.idle":"2024-07-29T16:49:00.067648Z","shell.execute_reply.started":"2024-07-29T16:48:57.659730Z","shell.execute_reply":"2024-07-29T16:49:00.066443Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"[main c89fbdf] pos-tagging class_weight_batch_norm_dilated_convs_sentence_level_pos added\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 models/task3_cnn_postag/class_weight_batch_norm_dilated_convs_sentence_level_pos.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:49:02.130595Z","iopub.execute_input":"2024-07-29T16:49:02.131115Z","iopub.status.idle":"2024-07-29T16:49:07.254745Z","shell.execute_reply.started":"2024-07-29T16:49:02.131060Z","shell.execute_reply":"2024-07-29T16:49:07.253749Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"Enumerating objects: 8, done.\nCounting objects: 100% (8/8), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 2.20 MiB | 1.40 MiB/s, done.\nTotal 5 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\nTo https://github.com/englishtea21/stepik-dl-nlp.git\n   d3ac6b1..c89fbdf  main -> main\n","output_type":"stream"}]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nbest_batch_norm_class_weighting_dilated_convs_sentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_dilated_convs_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T16:49:44.072236Z","iopub.execute_input":"2024-07-29T16:49:44.073411Z","iopub.status.idle":"2024-07-29T16:49:44.156142Z","shell.execute_reply.started":"2024-07-29T16:49:44.073375Z","shell.execute_reply":"2024-07-29T16:49:44.154958Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_dilated_convs_sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_dilated_convs_sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:50:00.356904Z","iopub.execute_input":"2024-07-29T16:50:00.357361Z","iopub.status.idle":"2024-07-29T16:51:14.338951Z","shell.execute_reply.started":"2024-07-29T16:50:00.357325Z","shell.execute_reply":"2024-07-29T16:51:14.337926Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"767it [00:39, 19.56it/s]                             \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на обучении 0.0372622087597847\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   4330443\n         ADJ       0.77      0.86      0.81     43357\n         ADP       0.95      0.96      0.96     39344\n         ADV       0.72      0.75      0.74     22733\n         AUX       0.64      1.00      0.78      3537\n       CCONJ       0.89      0.97      0.93     15168\n         DET       0.72      0.90      0.80     10781\n        INTJ       0.05      0.96      0.09        50\n        NOUN       0.90      0.85      0.88    103538\n         NUM       0.69      0.94      0.80      5640\n        PART       0.79      0.82      0.81     13556\n        PRON       0.94      0.72      0.81     18734\n       PROPN       0.86      0.91      0.89     14854\n       PUNCT       1.00      1.00      1.00     77972\n       SCONJ       0.84      0.88      0.86      8057\n         SYM       0.97      1.00      0.98       420\n        VERB       0.91      0.77      0.83     47731\n           X       0.26      0.78      0.39       189\n\n    accuracy                           0.99   4756104\n   macro avg       0.77      0.89      0.80   4756104\nweighted avg       0.99      0.99      0.99   4756104\n\n\n","output_type":"stream"},{"name":"stderr","text":"279it [00:14, 19.62it/s]                              \n","output_type":"stream"},{"name":"stdout","text":"Среднее значение функции потерь на валидации 0.03988298028707504\n              precision    recall  f1-score   support\n\n     <NOTAG>       1.00      1.00      1.00   1574439\n         ADJ       0.74      0.85      0.79     15103\n         ADP       0.95      0.96      0.96     13717\n         ADV       0.68      0.73      0.70      7783\n         AUX       0.68      1.00      0.81      1390\n       CCONJ       0.91      0.97      0.94      5672\n         DET       0.72      0.86      0.78      4265\n        INTJ       0.04      0.79      0.08        24\n        NOUN       0.89      0.84      0.86     36238\n         NUM       0.61      0.88      0.72      1734\n        PART       0.78      0.81      0.79      5125\n        PRON       0.94      0.71      0.81      7444\n       PROPN       0.85      0.90      0.87      5473\n       PUNCT       1.00      1.00      1.00     29186\n       SCONJ       0.83      0.88      0.85      2865\n         SYM       0.90      0.92      0.91        62\n        VERB       0.91      0.76      0.83     17110\n           X       0.34      0.78      0.47       134\n\n    accuracy                           0.99   1727764\n   macro avg       0.76      0.87      0.79   1727764\nweighted avg       0.99      0.99      0.99   1727764\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Сравним модели на крайних примерах","metadata":{}},{"cell_type":"code","source":"best_batch_norm_class_weighting_sentence_level_model_pos_tagger = POSTagger(best_batch_norm_class_weighting_sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nbatch_norm_class_weighting_sentence_level_model_more_layers_pos_tagger = POSTagger(batch_norm_class_weighting_sentence_level_model_more_layers, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nbatch_norm_class_weighting_dilated_convs_sentence_level_model_pos_tagger = POSTagger(batch_norm_class_weighting_dilated_convs_sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:33.999141Z","iopub.execute_input":"2024-07-29T16:56:34.000106Z","iopub.status.idle":"2024-07-29T16:56:34.059752Z","shell.execute_reply.started":"2024-07-29T16:56:34.000042Z","shell.execute_reply":"2024-07-29T16:56:34.058593Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.125540Z","start_time":"2019-08-29T13:56:42.106771Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:42.827250Z","iopub.execute_input":"2024-07-29T16:56:42.827639Z","iopub.status.idle":"2024-07-29T16:56:42.881804Z","shell.execute_reply.started":"2024-07-29T16:56:42.827609Z","shell.execute_reply":"2024-07-29T16:56:42.880928Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, best_batch_norm_class_weighting_sentence_level_model_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.126930Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:55.260914Z","iopub.execute_input":"2024-07-29T16:56:55.261777Z","iopub.status.idle":"2024-07-29T16:56:55.329851Z","shell.execute_reply.started":"2024-07-29T16:56:55.261744Z","shell.execute_reply":"2024-07-29T16:56:55.328952Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stderr","text":"1it [00:00, 153.32it/s]                    ","output_type":"stream"},{"name":"stdout","text":"мама-NOUN мыла-VERB раму-PROPN\n\nкосил-NOUN косой-ADJ косой-ADJ косой-ADJ\n\nглокая-ADJ куздра-NOUN штеко-DET будланула-NOUN бокра-NOUN и-PROPN куздрячит-NOUN бокрёнка-NOUN\n\nсяпала-VERB калуша-NOUN с-NOUN калушатами-NOUN по-ADV напушке-NOUN\n\nпирожки-ADV поставлены-VERB в-NOUN печь-ADV мама-NOUN любит-VERB печь-ADV\n\nведро-ADV дало-VERB течь-ADV вода-ADV стала-VERB течь-ADV\n\nтри-NOUN да-NOUN три-NOUN будет-AUX дырка-NOUN\n\nтри-NOUN да-NOUN три-NOUN будет-AUX шесть-NUM\n\nсорок-NOUN сорок-NOUN\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, batch_norm_class_weighting_sentence_level_model_more_layers_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.168810Z","start_time":"2019-08-29T13:56:42.149698Z"},"execution":{"iopub.status.busy":"2024-07-29T16:57:04.171449Z","iopub.execute_input":"2024-07-29T16:57:04.172381Z","iopub.status.idle":"2024-07-29T16:57:04.252847Z","shell.execute_reply.started":"2024-07-29T16:57:04.172346Z","shell.execute_reply":"2024-07-29T16:57:04.251939Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stderr","text":"1it [00:00, 60.47it/s]                     ","output_type":"stream"},{"name":"stdout","text":"мама-VERB мыла-VERB раму-X\n\nкосил-X косой-NOUN косой-NOUN косой-VERB\n\nглокая-VERB куздра-X штеко-X будланула-AUX бокра-VERB и-X куздрячит-X бокрёнка-NOUN\n\nсяпала-VERB калуша-VERB с-X калушатами-X по-X напушке-NOUN\n\nпирожки-X поставлены-VERB в-VERB печь-VERB мама-NOUN любит-VERB печь-VERB\n\nведро-VERB дало-VERB течь-VERB вода-VERB стала-VERB течь-X\n\nтри-X да-X три-X будет-AUX дырка-X\n\nтри-X да-X три-X будет-AUX шесть-X\n\nсорок-VERB сорок-VERB\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, batch_norm_class_weighting_dilated_convs_sentence_level_model_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:58:29.011206Z","iopub.execute_input":"2024-07-29T16:58:29.011599Z","iopub.status.idle":"2024-07-29T16:58:29.094995Z","shell.execute_reply.started":"2024-07-29T16:58:29.011569Z","shell.execute_reply":"2024-07-29T16:58:29.093991Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stderr","text":"1it [00:00, 56.21it/s]                     ","output_type":"stream"},{"name":"stdout","text":"мама-NOUN мыла-NOUN раму-NOUN\n\nкосил-NOUN косой-DET косой-ADJ косой-DET\n\nглокая-NOUN куздра-NOUN штеко-DET будланула-NOUN бокра-NOUN и-CCONJ куздрячит-NOUN бокрёнка-NOUN\n\nсяпала-NOUN калуша-ADV с-ADJ калушатами-ADV по-ADV напушке-ADV\n\nпирожки-ADV поставлены-NOUN в-NOUN печь-ADV мама-NOUN любит-VERB печь-NOUN\n\nведро-ADV дало-NOUN течь-NOUN вода-NOUN стала-NOUN течь-NOUN\n\nтри-NUM да-NOUN три-NUM будет-AUX дырка-NOUN\n\nтри-NUM да-NOUN три-NUM будет-AUX шесть-X\n\nсорок-ADV сорок-ADV\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}