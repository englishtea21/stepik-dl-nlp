{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Свёрточные нейросети и POS-теггинг\n\nPOS-теггинг - определение частей речи (снятие частеречной неоднозначности)","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle,\n# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n\n!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\nimport sys; sys.path.append('./stepik-dl-nlp')","metadata":{"execution":{"iopub.status.busy":"2024-07-24T10:59:40.304793Z","iopub.execute_input":"2024-07-24T10:59:40.305102Z","iopub.status.idle":"2024-07-24T11:00:46.055620Z","shell.execute_reply.started":"2024-07-24T10:59:40.305076Z","shell.execute_reply":"2024-07-24T11:00:46.054567Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'stepik-dl-nlp'...\nremote: Enumerating objects: 296, done.\u001b[K\nremote: Counting objects: 100% (3/3), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 296 (delta 0), reused 1 (delta 0), pack-reused 293\u001b[K\nReceiving objects: 100% (296/296), 42.30 MiB | 18.36 MiB/s, done.\nResolving deltas: 100% (134/134), done.\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.2.2)\nRequirement already satisfied: spacy-udpipe in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\nCollecting pymorphy2 (from -r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: torch>=1.2 in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (2.1.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.7.5)\nCollecting ipymarkup (from -r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (5.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.66.4)\nCollecting youtokentome (from -r stepik-dl-nlp/requirements.txt (line 11))\n  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.12.2)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (6.28.0)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (8.20.0)\nRequirement already satisfied: pyconll in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 15)) (3.2.0)\nCollecting gensim==3.8.1 (from -r stepik-dl-nlp/requirements.txt (line 16))\n  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wget (from -r stepik-dl-nlp/requirements.txt (line 17))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting livelossplot==0.5.3 (from -r stepik-dl-nlp/requirements.txt (line 18))\n  Downloading livelossplot-0.5.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.26.4)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.16.0)\nRequirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (6.4.0)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.10/site-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.5)\nRequirement already satisfied: ufal.udpipe>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.3.1.1)\nCollecting dawg-python>=0.7.1 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2024.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.9.0.post0)\nCollecting intervaltree>=3 (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.4)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.10/site-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (8.1.7)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.8.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.5.8)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (3.11.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.13)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.5.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (69.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (6.0.1)\nRequirement already satisfied: xyzservices>=2021.09.1 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2.1.3)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.4)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.2)\nDownloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\nDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\nDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gensim, youtokentome, wget, intervaltree\n  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.1-cp310-cp310-linux_x86_64.whl size=23986009 sha256=c4efb7560168be9c72f798efef3854ae306dc9faf9c7b9502399c8fe766213aa\n  Stored in directory: /root/.cache/pip/wheels/92/23/5d/b5ce54b3760acfebee170a8fe4d91cb303fafbefd8f93f3723\n  Building wheel for youtokentome (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=187956 sha256=90ccd7a65c388743ae354500c17bc98e24faaee8eb6d97bf8f4daf620f72e105\n  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=fbb1ee219f6cf4ecae9426b61e4ee17a00be96e8cc8a833d5f7bb5c0dbae5efb\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26095 sha256=b485865b489b2643f2de7ecddd8a94b61f2565ed57cffb3665f6bba884d4e5ba\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built gensim youtokentome wget intervaltree\nInstalling collected packages: wget, pymorphy2-dicts-ru, dawg-python, youtokentome, pymorphy2, intervaltree, ipymarkup, gensim, livelossplot\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.2\n    Uninstalling gensim-4.3.2:\n      Successfully uninstalled gensim-4.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.19 requires gensim>=4.0.0, but you have gensim 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 wget-3.2 youtokentome-1.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-07-24T12:07:05.502533Z","iopub.execute_input":"2024-07-24T12:07:05.503276Z","iopub.status.idle":"2024-07-24T12:07:07.700904Z","shell.execute_reply.started":"2024-07-24T12:07:05.503243Z","shell.execute_reply":"2024-07-24T12:07:07.699724Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n/bin/bash: -c: line 0: syntax error near unexpected token `('\n/bin/bash: -c: line 0: `git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git'\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyconll\n!pip install spacy_udpipe","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:42:57.976431Z","start_time":"2019-10-29T19:42:57.959538Z"},"execution":{"iopub.status.busy":"2024-07-24T11:00:46.057567Z","iopub.execute_input":"2024-07-24T11:00:46.058420Z","iopub.status.idle":"2024-07-24T11:01:10.450027Z","shell.execute_reply.started":"2024-07-24T11:00:46.058382Z","shell.execute_reply":"2024-07-24T11:01:10.449060Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyconll in /opt/conda/lib/python3.10/site-packages (3.2.0)\nRequirement already satisfied: spacy_udpipe in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (3.7.5)\nRequirement already satisfied: ufal.udpipe>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (1.3.1.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import classification_report\n\nimport numpy as np\n\nimport pyconll\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\n\nimport dlnlputils\nfrom dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n    character_tokenize, pos_corpus_to_tensor, POSTagger\nfrom dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n\ninit_random_seed()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:34.549739Z","start_time":"2019-10-29T19:49:32.179692Z"},"execution":{"iopub.status.busy":"2024-07-24T11:01:40.042855Z","iopub.execute_input":"2024-07-24T11:01:40.043731Z","iopub.status.idle":"2024-07-24T11:01:46.802037Z","shell.execute_reply.started":"2024-07-24T11:01:40.043701Z","shell.execute_reply":"2024-07-24T11:01:46.801233Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка текстов и разбиение на обучающую и тестовую подвыборки","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n!wget -O ./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:08.433599Z","start_time":"2019-10-29T19:46:05.110693Z"},"execution":{"iopub.status.busy":"2024-07-24T11:02:18.454569Z","iopub.execute_input":"2024-07-24T11:02:18.455477Z","iopub.status.idle":"2024-07-24T11:02:25.004675Z","shell.execute_reply.started":"2024-07-24T11:02:18.455447Z","shell.execute_reply":"2024-07-24T11:02:25.003537Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2024-07-24 11:02:19--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 40736599 (39M) [application/octet-stream]\nSaving to: './stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu'\n\n./stepik-dl-nlp/dat 100%[===================>]  38.85M   234MB/s    in 0.2s    \n\n2024-07-24 11:02:22 (234 MB/s) - './stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu' saved [40736599/40736599]\n\n--2024-07-24 11:02:23--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14704579 (14M) [application/octet-stream]\nSaving to: './stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu'\n\n./stepik-dl-nlp/dat 100%[===================>]  14.02M  --.-KB/s    in 0.07s   \n\n2024-07-24 11:02:24 (203 MB/s) - './stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu' saved [14704579/14704579]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nfull_train = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-train.conllu')\nfull_test = pyconll.load_from_file('./stepik-dl-nlp/datasets/ru_syntagrus-ud-dev.conllu')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.525561Z","start_time":"2019-10-29T19:49:37.315213Z"},"execution":{"iopub.status.busy":"2024-07-24T11:02:36.586794Z","iopub.execute_input":"2024-07-24T11:02:36.587192Z","iopub.status.idle":"2024-07-24T11:02:52.026913Z","shell.execute_reply.started":"2024-07-24T11:02:36.587164Z","shell.execute_reply":"2024-07-24T11:02:52.025938Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for sent in full_train[:2]:\n    for token in sent:\n        print(token.form, token.upos)\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.548127Z","start_time":"2019-10-29T19:49:56.527559Z"},"execution":{"iopub.status.busy":"2024-07-24T11:02:55.377070Z","iopub.execute_input":"2024-07-24T11:02:55.377450Z","iopub.status.idle":"2024-07-24T11:02:55.427909Z","shell.execute_reply.started":"2024-07-24T11:02:55.377421Z","shell.execute_reply":"2024-07-24T11:02:55.426983Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Анкета NOUN\n. PUNCT\n\nНачальник NOUN\nобластного ADJ\nуправления NOUN\nсвязи NOUN\nСемен PROPN\nЕремеевич PROPN\nбыл AUX\nчеловек NOUN\nпростой ADJ\n, PUNCT\nприходил VERB\nна ADP\nработу NOUN\nвсегда ADV\nвовремя ADV\n, PUNCT\nздоровался VERB\nс ADP\nсекретаршей NOUN\nза ADP\nруку NOUN\nи CCONJ\nиногда ADV\nдаже PART\nписал VERB\nв ADP\nстенгазету NOUN\nзаметки NOUN\nпод ADP\nпсевдонимом NOUN\n\" PUNCT\nМуха NOUN\n\" PUNCT\n. PUNCT\n\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_SENT_LEN = max(len(sent) for sent in full_train)\nMAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\nprint('Наибольшая длина предложения', MAX_SENT_LEN)\nprint('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.916262Z","start_time":"2019-10-29T19:49:56.549806Z"},"execution":{"iopub.status.busy":"2024-07-24T11:03:41.726798Z","iopub.execute_input":"2024-07-24T11:03:41.727647Z","iopub.status.idle":"2024-07-24T11:03:41.958460Z","shell.execute_reply.started":"2024-07-24T11:03:41.727617Z","shell.execute_reply":"2024-07-24T11:03:41.957584Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Наибольшая длина предложения 194\nНаибольшая длина токена 31\n","output_type":"stream"}]},{"cell_type":"code","source":"all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\nprint('\\n'.join(all_train_texts[:10]))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:57.251433Z","start_time":"2019-10-29T19:49:56.919818Z"},"execution":{"iopub.status.busy":"2024-07-24T11:04:35.193198Z","iopub.execute_input":"2024-07-24T11:04:35.193622Z","iopub.status.idle":"2024-07-24T11:04:35.430067Z","shell.execute_reply.started":"2024-07-24T11:04:35.193580Z","shell.execute_reply":"2024-07-24T11:04:35.429283Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Анкета .\nНачальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\nВ приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\nОднако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\nПриемная была обставлена просто , но по-деловому .\nУ двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\nВ углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\nКабинет отличался скромностью , присущей Семену Еремеевичу .\nВ глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\nСправа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Для решения задачи pos-теггинга сверточными сетями будем применять посимвольную токенизацию текста. Этот подход оправдан, потому что принадлежность слова к той или иной части речи определяется наличием суффиксов, приставок и вида окончаний. При токенизации по более большим токенам мы бы не смогли уловить структуру слов.","metadata":{}},{"cell_type":"markdown","source":"Здесь нам также нужен фиктивной символ \"отсутсвтие символа\" для уравнивания длины всех предложений","metadata":{}},{"cell_type":"code","source":"train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\nchar_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\nprint(\"Количество уникальных символов\", len(char_vocab))\nprint(list(char_vocab.items())[:10])","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"execution":{"iopub.status.busy":"2024-07-24T11:06:40.484072Z","iopub.execute_input":"2024-07-24T11:06:40.484876Z","iopub.status.idle":"2024-07-24T11:06:41.007903Z","shell.execute_reply.started":"2024-07-24T11:06:40.484845Z","shell.execute_reply":"2024-07-24T11:06:41.007005Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Количество уникальных символов 142\n[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Аналогично назначим id меткам частей речи <br>\nТакже понадобится фиктивная метка части речи отсутствия части речи","metadata":{}},{"cell_type":"code","source":"UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\nlabel2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\nlabel2id","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.524125Z","start_time":"2019-10-29T19:49:58.125577Z"},"execution":{"iopub.status.busy":"2024-07-24T11:08:34.849090Z","iopub.execute_input":"2024-07-24T11:08:34.849755Z","iopub.status.idle":"2024-07-24T11:08:35.054633Z","shell.execute_reply.started":"2024-07-24T11:08:34.849721Z","shell.execute_reply":"2024-07-24T11:08:35.053698Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'<NOTAG>': 0,\n 'ADJ': 1,\n 'ADP': 2,\n 'ADV': 3,\n 'AUX': 4,\n 'CCONJ': 5,\n 'DET': 6,\n 'INTJ': 7,\n 'NOUN': 8,\n 'NUM': 9,\n 'PART': 10,\n 'PRON': 11,\n 'PROPN': 12,\n 'PUNCT': 13,\n 'SCONJ': 14,\n 'SYM': 15,\n 'VERB': 16,\n 'X': 17}"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут pos_corpus_to_tensor в качестве предложений принимает предложения в формате conllu где про каждое слово известна метка части речи. ","metadata":{}},{"cell_type":"markdown","source":"max_token_len + 2 нужно чтобы указать нейросети что определенная n-грамма символов встречается именно в начале токена или именно в конце токена, но не в середине","metadata":{}},{"cell_type":"markdown","source":"Почему может понадобиться отличать начало и конец слова от середины слова?\n<br>\nОдна и та же последовательность символов может быть как частью суффикса, так и частью приставки, при этом неся разную функцию","metadata":{}},{"cell_type":"code","source":"def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            if token.form is None:\n                continue\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            for char_i, char in enumerate(token.form):\n                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n                            \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-07-24T11:19:10.768127Z","iopub.execute_input":"2024-07-24T11:19:10.769128Z","iopub.status.idle":"2024-07-24T11:19:10.829991Z","shell.execute_reply.started":"2024-07-24T11:19:10.769085Z","shell.execute_reply":"2024-07-24T11:19:10.829005Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2024-07-24T11:19:12.677359Z","iopub.execute_input":"2024-07-24T11:19:12.677770Z","iopub.status.idle":"2024-07-24T11:19:43.707392Z","shell.execute_reply.started":"2024-07-24T11:19:12.677738Z","shell.execute_reply":"2024-07-24T11:19:43.706604Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_inputs[1][:5]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.754883Z","start_time":"2019-10-29T19:49:40.582Z"},"scrolled":true,"execution":{"iopub.status.busy":"2024-07-24T11:24:41.260407Z","iopub.execute_input":"2024-07-24T11:24:41.261345Z","iopub.status.idle":"2024-07-24T11:24:41.331604Z","shell.execute_reply.started":"2024-07-24T11:24:41.261313Z","shell.execute_reply":"2024-07-24T11:24:41.330692Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0, 38,  4, 25,  4, 11, 19,  7,  6, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  2, 23, 11,  4,  9,  5,  7,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0, 17, 16, 10,  4, 12, 11,  3,  7,  6, 20,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  9, 12, 20, 21,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0, 40,  3, 15,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"},"metadata":{}}]},{"cell_type":"code","source":"train_labels[1]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.756496Z","start_time":"2019-10-29T19:49:40.711Z"},"execution":{"iopub.status.busy":"2024-07-24T11:24:44.320699Z","iopub.execute_input":"2024-07-24T11:24:44.321300Z","iopub.status.idle":"2024-07-24T11:24:44.374678Z","shell.execute_reply.started":"2024-07-24T11:24:44.321269Z","shell.execute_reply":"2024-07-24T11:24:44.373727Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"tensor([ 8,  1,  8,  8, 12, 12,  4,  8,  1, 13, 16,  2,  8,  3,  3, 13, 16,  2,\n         8,  2,  8,  5,  3, 10, 16,  2,  8,  8,  2,  8, 13,  8, 13, 13,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Вспомогательная свёрточная архитектура","metadata":{}},{"cell_type":"markdown","source":"Своего рода resnet, сумма реализует skip-connection","metadata":{}},{"cell_type":"code","source":"class StackedConv1d(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for _ in range(layers_n):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.316516Z","start_time":"2019-10-29T19:46:17.539Z"},"execution":{"iopub.status.busy":"2024-07-24T11:50:23.497289Z","iopub.execute_input":"2024-07-24T11:50:23.498136Z","iopub.status.idle":"2024-07-24T11:50:23.551031Z","shell.execute_reply.started":"2024-07-24T11:50:23.498104Z","shell.execute_reply":"2024-07-24T11:50:23.550056Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне отдельных токенов","metadata":{}},{"cell_type":"markdown","source":"эта модель никак не учитывает контекст, в котором встречается слово","metadata":{}},{"cell_type":"markdown","source":"потому будет ошибаться в разметке например в следующем предложении: <br>\n\nТри да три будет шесть или три да три будет дырка","metadata":{}},{"cell_type":"code","source":"class SingleTokenPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.backbone = StackedConv1d(embedding_size, **kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Linear(embedding_size, labels_num)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        \n        features = self.backbone(char_embeddings)\n        \n        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n        \n        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.317452Z","start_time":"2019-10-29T19:46:23.135Z"},"execution":{"iopub.status.busy":"2024-07-24T11:50:24.733994Z","iopub.execute_input":"2024-07-24T11:50:24.734692Z","iopub.status.idle":"2024-07-24T11:50:24.788579Z","shell.execute_reply.started":"2024-07-24T11:50:24.734663Z","shell.execute_reply":"2024-07-24T11:50:24.787599Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\nprint('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.318497Z","start_time":"2019-10-29T19:46:23.764Z"},"execution":{"iopub.status.busy":"2024-07-24T11:50:28.020102Z","iopub.execute_input":"2024-07-24T11:50:28.020837Z","iopub.status.idle":"2024-07-24T11:50:28.078364Z","shell.execute_reply.started":"2024-07-24T11:50:28.020804Z","shell.execute_reply":"2024-07-24T11:50:28.077380Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Количество параметров 47314\n","output_type":"stream"}]},{"cell_type":"code","source":"(best_val_loss,\n best_single_token_model) = train_eval_loop(single_token_model,\n                                            train_dataset,\n                                            test_dataset,\n                                            F.cross_entropy,\n                                            lr=5e-3,\n                                            epoch_n=10,\n                                            batch_size=64,\n                                            device='cuda',\n                                            early_stopping_patience=5,\n                                            max_batches_per_epoch_train=500,\n                                            max_batches_per_epoch_val=100,\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                       factor=0.5,\n                                                                                                                       verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.319470Z","start_time":"2019-10-29T19:46:25.552Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntorch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.320568Z","start_time":"2019-10-29T19:46:47.579Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsingle_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(single_token_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(single_token_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне предложений (с учётом контекста)","metadata":{}},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.326925Z","start_time":"2019-10-29T19:46:50.310Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model) = train_eval_loop(sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              F.cross_entropy,\n                                              lr=5e-3,\n                                              epoch_n=10,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\ntorch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.564926Z","start_time":"2019-08-29T13:56:16.544481Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.092139Z","start_time":"2019-08-29T13:56:16.567242Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Применение полученных теггеров и сравнение","metadata":{}},{"cell_type":"code","source":"single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nsentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.125540Z","start_time":"2019-08-29T13:56:42.106771Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.126930Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.168810Z","start_time":"2019-08-29T13:56:42.149698Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Свёрточный модуль своими руками","metadata":{}},{"cell_type":"code","source":"class MyConv1d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n                                   requires_grad=True)\n        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n\n        batch_size, src_channels, sequence_len = x.shape        \n        if self.padding > 0:\n            pad = x.new_zeros(batch_size, src_channels, self.padding)\n            x = torch.cat((pad, x, pad), dim=-1)\n            sequence_len = x.shape[-1]\n\n        chunks = []\n        chunk_size = sequence_len - self.kernel_size + 1\n        for offset in range(self.kernel_size):\n            chunks.append(x[:, :, offset:offset + chunk_size])\n\n        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n        return out_features","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.193140Z","start_time":"2019-08-29T13:56:42.170233Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.210013Z","start_time":"2019-08-29T13:56:42.194620Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n                                                      train_dataset,\n                                                      test_dataset,\n                                                      F.cross_entropy,\n                                                      lr=5e-3,\n                                                      epoch_n=10,\n                                                      batch_size=64,\n                                                      device='cuda',\n                                                      early_stopping_patience=5,\n                                                      max_batches_per_epoch_train=500,\n                                                      max_batches_per_epoch_val=100,\n                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                                 factor=0.5,\n                                                                                                                                 verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:00.233326Z","start_time":"2019-08-29T13:56:42.211456Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:39.145214Z","start_time":"2019-08-29T14:06:00.234936Z"}},"execution_count":null,"outputs":[]}]}