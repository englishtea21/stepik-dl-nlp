{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Свёрточные нейросети и POS-теггинг\n\nPOS-теггинг - определение частей речи (снятие частеречной неоднозначности)","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/englishtea21/stepik-dl-nlp.git\n!pip install -r stepik-dl-nlp/requirements.txt\nimport sys;","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:02:37.515849Z","iopub.execute_input":"2024-07-30T19:02:37.516222Z","iopub.status.idle":"2024-07-30T19:03:59.050449Z","shell.execute_reply.started":"2024-07-30T19:02:37.516192Z","shell.execute_reply":"2024-07-30T19:03:59.049290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'stepik-dl-nlp'...\nremote: Enumerating objects: 398, done.\u001b[K\nremote: Counting objects: 100% (104/104), done.\u001b[K\nremote: Compressing objects: 100% (57/57), done.\u001b[K\nremote: Total 398 (delta 54), reused 94 (delta 46), pack-reused 294\u001b[K\nReceiving objects: 100% (398/398), 146.63 MiB | 29.55 MiB/s, done.\nResolving deltas: 100% (188/188), done.\nUpdating files: 100% (64/64), done.\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.2.2)\nCollecting spacy-udpipe (from -r stepik-dl-nlp/requirements.txt (line 2))\n  Downloading spacy_udpipe-1.0.0-py3-none-any.whl.metadata (5.5 kB)\nCollecting pymorphy2 (from -r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: torch>=1.2 in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (2.1.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.7.5)\nCollecting ipymarkup (from -r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (5.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.11.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.66.4)\nCollecting youtokentome (from -r stepik-dl-nlp/requirements.txt (line 11))\n  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.12.2)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (6.28.0)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (8.20.0)\nCollecting pyconll (from -r stepik-dl-nlp/requirements.txt (line 15))\n  Downloading pyconll-3.2.0-py3-none-any.whl.metadata (8.0 kB)\nCollecting gensim==3.8.1 (from -r stepik-dl-nlp/requirements.txt (line 16))\n  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting wget (from -r stepik-dl-nlp/requirements.txt (line 17))\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting livelossplot==0.5.3 (from -r stepik-dl-nlp/requirements.txt (line 18))\n  Downloading livelossplot-0.5.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.26.4)\nRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.16.0)\nRequirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (6.4.0)\nRequirement already satisfied: bokeh in /opt/conda/lib/python3.10/site-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.2.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.5)\nCollecting ufal.udpipe>=1.2.0 (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2))\n  Downloading ufal.udpipe-1.3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting dawg-python>=0.7.1 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\nCollecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3))\n  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: docopt>=0.6 in /opt/conda/lib/python3.10/site-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2024.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.9.0.post0)\nCollecting intervaltree>=3 (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6))\n  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2023.4)\nRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.10/site-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (8.1.7)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.8.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.7.1)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (1.5.8)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (24.0.1)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (6.3.3)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.9.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\nRequirement already satisfied: sortedcontainers<3.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.3)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (3.11.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.13)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.5.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (69.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\nRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (6.0.1)\nRequirement already satisfied: xyzservices>=2021.09.1 in /opt/conda/lib/python3.10/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (2.1.3)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.4)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.18.1)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.1.2)\nDownloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\nDownloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\nDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\nDownloading pyconll-3.2.0-py3-none-any.whl (27 kB)\nDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\nDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ufal.udpipe-1.3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (936 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m936.8/936.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: gensim, youtokentome, wget, intervaltree\n  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.1-cp310-cp310-linux_x86_64.whl size=23986009 sha256=46aad1badf80894faf46545f361e0a355f6f3a17d6cc7c72c87c5d04e984c6a9\n  Stored in directory: /root/.cache/pip/wheels/92/23/5d/b5ce54b3760acfebee170a8fe4d91cb303fafbefd8f93f3723\n  Building wheel for youtokentome (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=187956 sha256=5559ef22d9ec43575b09d5596a4f0b5232336406d428ec4766e92b7bc00ae565\n  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=56f67cd1bd519264bf6f37cb4454a2e02f540a2a6a286d4fccf4d4c4692aeff5\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26095 sha256=5bcb7871fc85004c66781f8c4ce2ac7c5a13404194579bf49081aefa0eaa9008\n  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\nSuccessfully built gensim youtokentome wget intervaltree\nInstalling collected packages: wget, ufal.udpipe, pymorphy2-dicts-ru, dawg-python, youtokentome, pymorphy2, pyconll, intervaltree, ipymarkup, gensim, livelossplot, spacy-udpipe\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.2\n    Uninstalling gensim-4.3.2:\n      Successfully uninstalled gensim-4.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.19 requires gensim>=4.0.0, but you have gensim 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pyconll-3.2.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-udpipe-1.0.0 ufal.udpipe-1.3.1.1 wget-3.2 youtokentome-1.0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/stepik-dl-nlp","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:03:59.054632Z","iopub.execute_input":"2024-07-30T19:03:59.055473Z","iopub.status.idle":"2024-07-30T19:03:59.066390Z","shell.execute_reply.started":"2024-07-30T19:03:59.055439Z","shell.execute_reply":"2024-07-30T19:03:59.065527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/stepik-dl-nlp\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:03:59.067784Z","iopub.execute_input":"2024-07-30T19:03:59.068162Z","iopub.status.idle":"2024-07-30T19:03:59.085818Z","shell.execute_reply.started":"2024-07-30T19:03:59.068132Z","shell.execute_reply":"2024-07-30T19:03:59.084853Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:03:59.088533Z","iopub.execute_input":"2024-07-30T19:03:59.089615Z","iopub.status.idle":"2024-07-30T19:04:01.518465Z","shell.execute_reply.started":"2024-07-30T19:03:59.089585Z","shell.execute_reply":"2024-07-30T19:04:01.517127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"englishtea21@mail.ru\"\n!git config --global user.name \"englishtea21\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:04:01.520212Z","iopub.execute_input":"2024-07-30T19:04:01.520574Z","iopub.status.idle":"2024-07-30T19:04:03.742091Z","shell.execute_reply.started":"2024-07-30T19:04:01.520540Z","shell.execute_reply":"2024-07-30T19:04:03.740645Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install pyconll\n!pip install spacy_udpipe","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:42:57.976431Z","start_time":"2019-10-29T19:42:57.959538Z"},"execution":{"iopub.status.busy":"2024-07-30T19:04:03.743834Z","iopub.execute_input":"2024-07-30T19:04:03.744188Z","iopub.status.idle":"2024-07-30T19:04:31.112757Z","shell.execute_reply.started":"2024-07-30T19:04:03.744156Z","shell.execute_reply":"2024-07-30T19:04:31.111671Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyconll in /opt/conda/lib/python3.10/site-packages (3.2.0)\nRequirement already satisfied: spacy_udpipe in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (3.7.5)\nRequirement already satisfied: ufal.udpipe>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy_udpipe) (1.3.1.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.12.3)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.66.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2024.7.4)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.4)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (13.7.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.18.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (6.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.1.3)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import classification_report\n\nimport numpy as np\n\nimport pyconll\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset\n\nimport dlnlputils\nfrom dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n    character_tokenize, pos_corpus_to_tensor, POSTagger\nfrom dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n\ninit_random_seed()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:34.549739Z","start_time":"2019-10-29T19:49:32.179692Z"},"execution":{"iopub.status.busy":"2024-07-30T19:04:31.114435Z","iopub.execute_input":"2024-07-30T19:04:31.114794Z","iopub.status.idle":"2024-07-30T19:04:38.558582Z","shell.execute_reply.started":"2024-07-30T19:04:31.114762Z","shell.execute_reply":"2024-07-30T19:04:38.557598Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка текстов и разбиение на обучающую и тестовую подвыборки","metadata":{}},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# !wget -O datasets/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n# !wget -O ./datasets/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n# !wget -O ./datasets/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n# !wget -O datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:08.433599Z","start_time":"2019-10-29T19:46:05.110693Z"},"execution":{"iopub.status.busy":"2024-07-30T19:04:38.560026Z","iopub.execute_input":"2024-07-30T19:04:38.560525Z","iopub.status.idle":"2024-07-30T19:04:45.985800Z","shell.execute_reply.started":"2024-07-30T19:04:38.560494Z","shell.execute_reply":"2024-07-30T19:04:45.984740Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"--2024-07-30 19:04:39--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 40736599 (39M) [application/octet-stream]\nSaving to: 'datasets/ru_syntagrus-ud-train-a.conllu'\n\ndatasets/ru_syntagr 100%[===================>]  38.85M   191MB/s    in 0.2s    \n\n2024-07-30 19:04:40 (191 MB/s) - 'datasets/ru_syntagrus-ud-train-a.conllu' saved [40736599/40736599]\n\n--2024-07-30 19:04:41--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42819975 (41M) [text/plain]\nSaving to: './datasets/ru_syntagrus-ud-train-b.conllu'\n\n./datasets/ru_synta 100%[===================>]  40.84M   143MB/s    in 0.3s    \n\n2024-07-30 19:04:43 (143 MB/s) - './datasets/ru_syntagrus-ud-train-b.conllu' saved [42819975/42819975]\n\n--2024-07-30 19:04:44--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 32367510 (31M) [text/plain]\nSaving to: './datasets/ru_syntagrus-ud-train-c.conllu'\n\n./datasets/ru_synta 100%[===================>]  30.87M   176MB/s    in 0.2s    \n\n2024-07-30 19:04:45 (176 MB/s) - './datasets/ru_syntagrus-ud-train-c.conllu' saved [32367510/32367510]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git status","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:04:57.115503Z","iopub.execute_input":"2024-07-30T19:04:57.115937Z","iopub.status.idle":"2024-07-30T19:04:58.278053Z","shell.execute_reply.started":"2024-07-30T19:04:57.115906Z","shell.execute_reply":"2024-07-30T19:04:58.276951Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"On branch main\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31mdatasets/ru_syntagrus-ud-train-a.conllu\u001b[m\n\t\u001b[31mdatasets/ru_syntagrus-ud-train-b.conllu\u001b[m\n\t\u001b[31mdatasets/ru_syntagrus-ud-train-c.conllu\u001b[m\n\t\u001b[31mdlnlputils/__pycache__/\u001b[m\n\t\u001b[31mdlnlputils/data/__pycache__/\u001b[m\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git add datasets/ru_syntagrus-ud-train-a.conllu datasets/ru_syntagrus-ud-train-b.conllu datasets/ru_syntagrus-ud-train-c.conllu","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:05:03.917367Z","iopub.execute_input":"2024-07-30T19:05:03.917801Z","iopub.status.idle":"2024-07-30T19:05:06.659750Z","shell.execute_reply.started":"2024-07-30T19:05:03.917766Z","shell.execute_reply":"2024-07-30T19:05:06.658433Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# !git commit -m 'pos-tagging datasets altered'","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:05:09.157272Z","iopub.execute_input":"2024-07-30T19:05:09.158147Z","iopub.status.idle":"2024-07-30T19:05:10.887834Z","shell.execute_reply.started":"2024-07-30T19:05:09.158109Z","shell.execute_reply":"2024-07-30T19:05:10.886765Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[main 5846d4c] pos-tagging datasets altered\n 3 files changed, 1456916 insertions(+)\n create mode 100644 datasets/ru_syntagrus-ud-train-a.conllu\n create mode 100644 datasets/ru_syntagrus-ud-train-b.conllu\n create mode 100644 datasets/ru_syntagrus-ud-train-c.conllu\n","output_type":"stream"}]},{"cell_type":"code","source":"# !git push -u origin main","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:05:13.915103Z","iopub.execute_input":"2024-07-30T19:05:13.915990Z","iopub.status.idle":"2024-07-30T19:05:27.799067Z","shell.execute_reply.started":"2024-07-30T19:05:13.915953Z","shell.execute_reply":"2024-07-30T19:05:27.797700Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Enumerating objects: 7, done.\nCounting objects: 100% (7/7), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (5/5), done.\nWriting objects: 100% (5/5), 11.85 MiB | 2.36 MiB/s, done.\nTotal 5 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 2 local objects.\u001b[K\nTo https://github.com/englishtea21/stepik-dl-nlp.git\n   ff01157..5846d4c  main -> main\nBranch 'main' set up to track remote branch 'main' from 'origin'.\n","output_type":"stream"}]},{"cell_type":"code","source":"full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train-a.conllu')\nfull_train.extend(pyconll.load_from_file('./datasets/ru_syntagrus-ud-train-b.conllu'))\nfull_train.extend(pyconll.load_from_file('./datasets/ru_syntagrus-ud-train-c.conllu'))\nfull_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.525561Z","start_time":"2019-10-29T19:49:37.315213Z"},"execution":{"iopub.status.busy":"2024-07-30T19:05:35.493177Z","iopub.execute_input":"2024-07-30T19:05:35.494169Z","iopub.status.idle":"2024-07-30T19:06:14.280706Z","shell.execute_reply.started":"2024-07-30T19:05:35.494133Z","shell.execute_reply":"2024-07-30T19:06:14.279865Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"В датасете имеются отсутствующие подразумеваемые слова, которые не имеют типа и вызывают ошибку при их разложении на буквы - для этого задания их нужно исключить.\n\nПример предложения: Если кратко - мы научились хакерские взломы эффективно отражать\n\nПосле токена \"если\" есть токен с описанием        _   говорить    _   _   _   _   _   5:advcl _\n\nВ некоторых предложениях такие токены имеют вид   _   _   _   _   _   _   _   0:root|5:conj   _","metadata":{}},{"cell_type":"code","source":"for sent in full_train:\n    for token in reversed(sent):\n        if token.upos is None:\n            sent._tokens.remove(token)\n\nfor sent in full_test:\n    for token in reversed(sent):\n        if token.upos is None:\n            sent._tokens.remove(token)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:07:10.075412Z","iopub.execute_input":"2024-07-30T19:07:10.075784Z","iopub.status.idle":"2024-07-30T19:07:11.299162Z","shell.execute_reply.started":"2024-07-30T19:07:10.075754Z","shell.execute_reply":"2024-07-30T19:07:11.298133Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for sent in full_train[:2]:\n    for token in sent:\n        print(token.form, token.upos)\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.548127Z","start_time":"2019-10-29T19:49:56.527559Z"},"execution":{"iopub.status.busy":"2024-07-30T19:07:14.408355Z","iopub.execute_input":"2024-07-30T19:07:14.409402Z","iopub.status.idle":"2024-07-30T19:07:14.466993Z","shell.execute_reply.started":"2024-07-30T19:07:14.409366Z","shell.execute_reply":"2024-07-30T19:07:14.465868Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Анкета NOUN\n. PUNCT\n\nНачальник NOUN\nобластного ADJ\nуправления NOUN\nсвязи NOUN\nСемен PROPN\nЕремеевич PROPN\nбыл AUX\nчеловек NOUN\nпростой ADJ\n, PUNCT\nприходил VERB\nна ADP\nработу NOUN\nвсегда ADV\nвовремя ADV\n, PUNCT\nздоровался VERB\nс ADP\nсекретаршей NOUN\nза ADP\nруку NOUN\nи CCONJ\nиногда ADV\nдаже PART\nписал VERB\nв ADP\nстенгазету NOUN\nзаметки NOUN\nпод ADP\nпсевдонимом NOUN\n\" PUNCT\nМуха NOUN\n\" PUNCT\n. PUNCT\n\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_SENT_LEN = max(len(sent) for sent in full_train)\nMAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\nprint('Наибольшая длина предложения', MAX_SENT_LEN)\nprint('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:56.916262Z","start_time":"2019-10-29T19:49:56.549806Z"},"execution":{"iopub.status.busy":"2024-07-30T19:07:18.447449Z","iopub.execute_input":"2024-07-30T19:07:18.448167Z","iopub.status.idle":"2024-07-30T19:07:19.083451Z","shell.execute_reply.started":"2024-07-30T19:07:18.448129Z","shell.execute_reply":"2024-07-30T19:07:19.082411Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Наибольшая длина предложения 205\nНаибольшая длина токена 47\n","output_type":"stream"}]},{"cell_type":"code","source":"all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\nprint('\\n'.join(all_train_texts[:10]))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:57.251433Z","start_time":"2019-10-29T19:49:56.919818Z"},"execution":{"iopub.status.busy":"2024-07-30T19:07:21.655826Z","iopub.execute_input":"2024-07-30T19:07:21.656201Z","iopub.status.idle":"2024-07-30T19:07:22.281755Z","shell.execute_reply.started":"2024-07-30T19:07:21.656173Z","shell.execute_reply":"2024-07-30T19:07:22.280761Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Анкета .\nНачальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\nВ приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\nОднако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\nПриемная была обставлена просто , но по-деловому .\nУ двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\nВ углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\nКабинет отличался скромностью , присущей Семену Еремеевичу .\nВ глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\nСправа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Для решения задачи pos-теггинга сверточными сетями будем применять посимвольную токенизацию текста. Этот подход оправдан, потому что принадлежность слова к той или иной части речи определяется наличием суффиксов, приставок и вида окончаний. При токенизации по более большим токенам мы бы не смогли уловить структуру слов.","metadata":{}},{"cell_type":"markdown","source":"Здесь нам также нужен фиктивной символ \"отсутсвтие символа\" для уравнивания длины всех предложений","metadata":{}},{"cell_type":"code","source":"train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\nchar_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\nprint(\"Количество уникальных символов\", len(char_vocab))\nprint(list(char_vocab.items())[:10])","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.124148Z","start_time":"2019-10-29T19:49:57.254191Z"},"execution":{"iopub.status.busy":"2024-07-30T19:07:30.875930Z","iopub.execute_input":"2024-07-30T19:07:30.876310Z","iopub.status.idle":"2024-07-30T19:07:32.259994Z","shell.execute_reply.started":"2024-07-30T19:07:30.876282Z","shell.execute_reply":"2024-07-30T19:07:32.258929Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Количество уникальных символов 153\n[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Аналогично назначим id меткам частей речи <br>\nТакже понадобится фиктивная метка части речи отсутствия части речи (уже есть в исходных данных)","metadata":{}},{"cell_type":"code","source":"UNIQUE_TAGS = sorted({token.upos for sent in full_train for token in sent if token.upos})\nlabel2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\nlabel2id, UNIQUE_TAGS","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.524125Z","start_time":"2019-10-29T19:49:58.125577Z"},"execution":{"iopub.status.busy":"2024-07-30T19:08:37.893941Z","iopub.execute_input":"2024-07-30T19:08:37.894759Z","iopub.status.idle":"2024-07-30T19:08:38.352804Z","shell.execute_reply.started":"2024-07-30T19:08:37.894720Z","shell.execute_reply":"2024-07-30T19:08:38.351855Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"({'ADJ': 0,\n  'ADP': 1,\n  'ADV': 2,\n  'AUX': 3,\n  'CCONJ': 4,\n  'DET': 5,\n  'INTJ': 6,\n  'NOUN': 7,\n  'NUM': 8,\n  'PART': 9,\n  'PRON': 10,\n  'PROPN': 11,\n  'PUNCT': 12,\n  'SCONJ': 13,\n  'SYM': 14,\n  'VERB': 15,\n  'X': 16},\n ['ADJ',\n  'ADP',\n  'ADV',\n  'AUX',\n  'CCONJ',\n  'DET',\n  'INTJ',\n  'NOUN',\n  'NUM',\n  'PART',\n  'PRON',\n  'PROPN',\n  'PUNCT',\n  'SCONJ',\n  'SYM',\n  'VERB',\n  'X'])"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут pos_corpus_to_tensor в качестве предложений принимает предложения в формате conllu где про каждое слово известна метка части речи. ","metadata":{}},{"cell_type":"markdown","source":"max_token_len + 2 нужно чтобы указать нейросети что определенная n-грамма символов встречается именно в начале токена или именно в конце токена, но не в середине\nэто нужно т.к. в процессе обучения свертки создают n-граммы символов и важно понимать отвечают эти символы началу слова или концу","metadata":{}},{"cell_type":"markdown","source":"Почему может понадобиться отличать начало и конец слова от середины слова?\n<br>\nОдна и та же последовательность символов может быть как частью суффикса, так и частью приставки, при этом неся разную функцию","metadata":{}},{"cell_type":"code","source":"def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            for char_i, char in enumerate(token.form):\n                # тут мы сдвигаем заполнение символами на один, чтобы были незначащие нули в конце и в начале слова\n                # - показывают нейросети границы слова при разбиении его на n-граммы с помощью сверток\n                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n                            \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-07-30T19:09:12.470658Z","iopub.execute_input":"2024-07-30T19:09:12.471068Z","iopub.status.idle":"2024-07-30T19:09:12.549783Z","shell.execute_reply.started":"2024-07-30T19:09:12.471037Z","shell.execute_reply":"2024-07-30T19:09:12.548199Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2024-07-30T19:09:15.793817Z","iopub.execute_input":"2024-07-30T19:09:15.794816Z","iopub.status.idle":"2024-07-30T19:10:36.972565Z","shell.execute_reply.started":"2024-07-30T19:09:15.794779Z","shell.execute_reply":"2024-07-30T19:10:36.971590Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_inputs[1][:5]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.754883Z","start_time":"2019-10-29T19:49:40.582Z"},"scrolled":true,"execution":{"iopub.status.busy":"2024-07-29T07:12:44.801087Z","iopub.execute_input":"2024-07-29T07:12:44.801818Z","iopub.status.idle":"2024-07-29T07:12:44.878444Z","shell.execute_reply.started":"2024-07-29T07:12:44.801779Z","shell.execute_reply":"2024-07-29T07:12:44.877480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels[1]","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.756496Z","start_time":"2019-10-29T19:49:40.711Z"},"execution":{"iopub.status.busy":"2024-07-30T19:10:36.974196Z","iopub.execute_input":"2024-07-30T19:10:36.974541Z","iopub.status.idle":"2024-07-30T19:10:37.048680Z","shell.execute_reply.started":"2024-07-30T19:10:36.974511Z","shell.execute_reply":"2024-07-30T19:10:37.047602Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([ 7,  0,  7,  7, 11, 11,  3,  7,  0, 12, 15,  1,  7,  2,  2, 12, 15,  1,\n         7,  1,  7,  4,  2,  9, 15,  1,  7,  7,  1,  7, 12,  7, 12, 12,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Вспомогательная свёрточная архитектура","metadata":{}},{"cell_type":"markdown","source":"Своего рода resnet, сумма реализует skip-connection","metadata":{}},{"cell_type":"code","source":"class StackedConv1d(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for _ in range(layers_n):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n                nn.Dropout(dropout),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.316516Z","start_time":"2019-10-29T19:46:17.539Z"},"execution":{"iopub.status.busy":"2024-07-30T19:11:10.185614Z","iopub.execute_input":"2024-07-30T19:11:10.186025Z","iopub.status.idle":"2024-07-30T19:11:10.246132Z","shell.execute_reply.started":"2024-07-30T19:11:10.185994Z","shell.execute_reply":"2024-07-30T19:11:10.244954Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне отдельных токенов","metadata":{}},{"cell_type":"markdown","source":"эта модель никак не учитывает контекст, в котором встречается слово","metadata":{}},{"cell_type":"markdown","source":"потому будет ошибаться в разметке например в следующем предложении: <br>\n\nТри да три будет шесть или три да три будет дырка","metadata":{}},{"cell_type":"markdown","source":"Физический смысл модели - рассмотреть все возможные n-грамы и по ним определить часть речи для токена <br>\nТ.к. backbone использует skip-connection то учитываются n-грамы разной размерности, получаемые засчет различных сверток <br>\nНапример, если мы используем размер ядра свёртки, равный 3, то первый блок учитывает трёхграммы, второй блок уже учитывает пятиграммы, а третий — семиграммы, соответственно. При этом, благодаря тому, что есть \"skip connection\", информация о трёхграммах не теряется, она пробрасывать до самого конца.","metadata":{}},{"cell_type":"code","source":"class SingleTokenPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n        super().__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.backbone = StackedConv1d(embedding_size, **kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Linear(embedding_size, labels_num)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        \n        features = self.backbone(char_embeddings)\n        \n        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n        \n        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.317452Z","start_time":"2019-10-29T19:46:23.135Z"},"execution":{"iopub.status.busy":"2024-07-30T19:11:12.245190Z","iopub.execute_input":"2024-07-30T19:11:12.245965Z","iopub.status.idle":"2024-07-30T19:11:12.307169Z","shell.execute_reply.started":"2024-07-30T19:11:12.245902Z","shell.execute_reply":"2024-07-30T19:11:12.305970Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\nprint('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.318497Z","start_time":"2019-10-29T19:46:23.764Z"},"execution":{"iopub.status.busy":"2024-07-30T19:11:14.081813Z","iopub.execute_input":"2024-07-30T19:11:14.082534Z","iopub.status.idle":"2024-07-30T19:11:14.156965Z","shell.execute_reply.started":"2024-07-30T19:11:14.082501Z","shell.execute_reply":"2024-07-30T19:11:14.156045Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Количество параметров 47953\n","output_type":"stream"}]},{"cell_type":"code","source":"(best_val_loss,\n best_single_token_model) = train_eval_loop(single_token_model,\n                                            train_dataset,\n                                            test_dataset,\n                                            F.cross_entropy,\n                                            lr=5e-3,\n                                            epoch_n=10,\n                                            batch_size=64,\n                                            device='cuda',\n                                            early_stopping_patience=5,\n                                            max_batches_per_epoch_train=500,\n                                            max_batches_per_epoch_val=100,\n                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                       factor=0.5,\n                                                                                                                       verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.319470Z","start_time":"2019-10-29T19:46:25.552Z"},"execution":{"iopub.status.busy":"2024-07-30T19:12:14.592201Z","iopub.execute_input":"2024-07-30T19:12:14.592821Z","iopub.status.idle":"2024-07-30T19:17:04.613862Z","shell.execute_reply.started":"2024-07-30T19:12:14.592773Z","shell.execute_reply":"2024-07-30T19:17:04.612786Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Эпоха 0\nЭпоха: 501 итераций, 251.87 сек\nСреднее значение функции потерь на обучении 0.038502571345892495\nСреднее значение функции потерь на валидации 0.03797047465375745\nНовая лучшая модель!\n\nЭпоха 1\nДосрочно остановлено пользователем\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n\n# newpath = r'models/task3_cnn_postag' \n# if not os.path.exists(newpath):\n#     os.makedirs(newpath)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:25:41.166786Z","iopub.execute_input":"2024-07-29T07:25:41.167692Z","iopub.status.idle":"2024-07-29T07:25:41.222517Z","shell.execute_reply.started":"2024-07-29T07:25:41.167658Z","shell.execute_reply":"2024-07-29T07:25:41.221641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_single_token_model.state_dict(), 'models/task3_cnn_postag/baseline_single_token_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.320568Z","start_time":"2019-10-29T19:46:47.579Z"},"execution":{"iopub.status.busy":"2024-07-29T07:25:43.745381Z","iopub.execute_input":"2024-07-29T07:25:43.745762Z","iopub.status.idle":"2024-07-29T07:25:43.804783Z","shell.execute_reply.started":"2024-07-29T07:25:43.745732Z","shell.execute_reply":"2024-07-29T07:25:43.803685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add models\n!git commit -m 'pos-tagging baseline model added'\n!git push -u origin main","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:26:27.051945Z","iopub.execute_input":"2024-07-29T07:26:27.052363Z","iopub.status.idle":"2024-07-29T07:26:31.161254Z","shell.execute_reply.started":"2024-07-29T07:26:27.052331Z","shell.execute_reply":"2024-07-29T07:26:31.160029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsingle_token_model.load_state_dict(torch.load('models/task3_cnn_postag/baseline_single_token_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:32.930199Z","iopub.execute_input":"2024-07-29T15:02:32.930640Z","iopub.status.idle":"2024-07-29T15:02:33.115460Z","shell.execute_reply.started":"2024-07-29T15:02:32.930607Z","shell.execute_reply":"2024-07-29T15:02:33.114256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(single_token_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(single_token_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T15:02:33.116603Z","iopub.execute_input":"2024-07-29T15:02:33.116916Z","iopub.status.idle":"2024-07-29T15:03:10.242600Z","shell.execute_reply.started":"2024-07-29T15:02:33.116891Z","shell.execute_reply":"2024-07-29T15:03:10.241509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Несмотря на простоту сверточной модели, результат на валидации не так далек от теста и при этом высок - 85%","metadata":{}},{"cell_type":"markdown","source":"Напомним, что наша модель никак не учитывает контекст токена при его pos-tagging'е, потому модель не справляется с задачей при частеречной омонимии","metadata":{}},{"cell_type":"markdown","source":"## Предсказание частей речи на уровне предложений (с учётом контекста)","metadata":{}},{"cell_type":"markdown","source":"Тут используется 2 resnet'а - первый для построения векторного представления токена с учетом того из каких символов он составлен, второй же уже учитывает контекст токенов в предложении","metadata":{}},{"cell_type":"code","source":"class SentenceLevelPOSTagger(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.325744Z","start_time":"2019-10-29T19:46:50.139Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.246289Z","iopub.execute_input":"2024-07-29T15:03:10.246577Z","iopub.status.idle":"2024-07-29T15:03:10.303292Z","shell.execute_reply.started":"2024-07-29T15:03:10.246554Z","shell.execute_reply":"2024-07-29T15:03:10.302450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.326925Z","start_time":"2019-10-29T19:46:50.310Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.304513Z","iopub.execute_input":"2024-07-29T15:03:10.304812Z","iopub.status.idle":"2024-07-29T15:03:10.359627Z","shell.execute_reply.started":"2024-07-29T15:03:10.304786Z","shell.execute_reply":"2024-07-29T15:03:10.358754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_sentence_level_model) = train_eval_loop(sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              F.cross_entropy,\n                                              lr=5e-3,\n                                              epoch_n=10,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.327888Z","start_time":"2019-10-29T19:46:50.737Z"},"execution":{"iopub.status.busy":"2024-07-29T07:29:09.744280Z","iopub.execute_input":"2024-07-29T07:29:09.744955Z","iopub.status.idle":"2024-07-29T07:41:57.613976Z","shell.execute_reply.started":"2024-07-29T07:29:09.744923Z","shell.execute_reply":"2024-07-29T07:41:57.613009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_sentence_level_model.state_dict(), 'models/task3_cnn_postag/baseline_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T07:42:23.045376Z","iopub.execute_input":"2024-07-29T07:42:23.046288Z","iopub.status.idle":"2024-07-29T07:42:23.103738Z","shell.execute_reply.started":"2024-07-29T07:42:23.046252Z","shell.execute_reply":"2024-07-29T07:42:23.102934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add models\n!git commit -m 'pos-tagging sentence_level baseline model added'\n!git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T07:42:46.099246Z","iopub.execute_input":"2024-07-29T07:42:46.099640Z","iopub.status.idle":"2024-07-29T07:42:50.571955Z","shell.execute_reply.started":"2024-07-29T07:42:46.099609Z","shell.execute_reply":"2024-07-29T07:42:50.571031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nsentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/baseline_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.564926Z","start_time":"2019-08-29T13:56:16.544481Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.360812Z","iopub.execute_input":"2024-07-29T15:03:10.361172Z","iopub.status.idle":"2024-07-29T15:03:10.415168Z","shell.execute_reply.started":"2024-07-29T15:03:10.361145Z","shell.execute_reply":"2024-07-29T15:03:10.414228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.092139Z","start_time":"2019-08-29T13:56:16.567242Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:10.416419Z","iopub.execute_input":"2024-07-29T15:03:10.416835Z","iopub.status.idle":"2024-07-29T15:03:45.807254Z","shell.execute_reply.started":"2024-07-29T15:03:10.416803Z","shell.execute_reply":"2024-07-29T15:03:45.806290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Применение полученных теггеров и сравнение","metadata":{}},{"cell_type":"code","source":"single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nsentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:58.350633Z","iopub.execute_input":"2024-07-29T15:03:58.351409Z","iopub.status.idle":"2024-07-29T15:03:58.400950Z","shell.execute_reply.started":"2024-07-29T15:03:58.351357Z","shell.execute_reply":"2024-07-29T15:03:58.399935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.125540Z","start_time":"2019-08-29T13:56:42.106771Z"},"execution":{"iopub.status.busy":"2024-07-29T15:03:59.058225Z","iopub.execute_input":"2024-07-29T15:03:59.058910Z","iopub.status.idle":"2024-07-29T15:03:59.109927Z","shell.execute_reply.started":"2024-07-29T15:03:59.058868Z","shell.execute_reply":"2024-07-29T15:03:59.108902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.126930Z"},"execution":{"iopub.status.busy":"2024-07-29T15:04:00.221060Z","iopub.execute_input":"2024-07-29T15:04:00.221475Z","iopub.status.idle":"2024-07-29T15:04:00.286352Z","shell.execute_reply.started":"2024-07-29T15:04:00.221446Z","shell.execute_reply":"2024-07-29T15:04:00.285429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.168810Z","start_time":"2019-08-29T13:56:42.149698Z"},"execution":{"iopub.status.busy":"2024-07-29T15:04:00.706884Z","iopub.execute_input":"2024-07-29T15:04:00.707282Z","iopub.status.idle":"2024-07-29T15:04:00.770287Z","shell.execute_reply.started":"2024-07-29T15:04:00.707250Z","shell.execute_reply":"2024-07-29T15:04:00.769335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как-то наша sentence based модель не очень справляется, показывает себя хуже, чем предыдущая более простая. Скорее всего она переобучается. Далее возьмем dilated conv вместо обычный и увеличим dropout.","metadata":{}},{"cell_type":"markdown","source":"## Свёрточный модуль своими руками","metadata":{}},{"cell_type":"code","source":"# class MyConv1d(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n#         super().__init__()\n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.kernel_size = kernel_size\n#         self.padding = padding\n#         self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n#                                    requires_grad=True)\n#         self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n    \n#     def forward(self, x):\n#         \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n\n#         batch_size, src_channels, sequence_len = x.shape        \n#         if self.padding > 0:\n#             pad = x.new_zeros(batch_size, src_channels, self.padding)\n#             x = torch.cat((pad, x, pad), dim=-1)\n#             sequence_len = x.shape[-1]\n\n#         chunks = []\n#         chunk_size = sequence_len - self.kernel_size + 1\n#         for offset in range(self.kernel_size):\n#             chunks.append(x[:, :, offset:offset + chunk_size])\n\n#         in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n#         in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n#         out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n#         out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n#         return out_features","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.193140Z","start_time":"2019-08-29T13:56:42.170233Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n#                                                       single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n#                                                       context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n# print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.210013Z","start_time":"2019-08-29T13:56:42.194620Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (best_val_loss,\n#  best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n#                                                       train_dataset,\n#                                                       test_dataset,\n#                                                       F.cross_entropy,\n#                                                       lr=5e-3,\n#                                                       epoch_n=10,\n#                                                       batch_size=64,\n#                                                       device='cuda',\n#                                                       early_stopping_patience=5,\n#                                                       max_batches_per_epoch_train=500,\n#                                                       max_batches_per_epoch_val=100,\n#                                                       lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n#                                                                                                                                  factor=0.5,\n#                                                                                                                                  verbose=True))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:00.233326Z","start_time":"2019-08-29T13:56:42.211456Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n# train_loss = F.cross_entropy(torch.tensor(train_pred),\n#                              torch.tensor(train_labels))\n# print('Среднее значение функции потерь на обучении', float(train_loss))\n# print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n# print()\n\n# test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n# test_loss = F.cross_entropy(torch.tensor(test_pred),\n#                             torch.tensor(test_labels))\n# print('Среднее значение функции потерь на валидации', float(test_loss))\n# print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-08-29T14:06:39.145214Z","start_time":"2019-08-29T14:06:00.234936Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Улучшим качество pos-tagging'а, учитывающего контекст","metadata":{}},{"cell_type":"markdown","source":"Идеи:\n- Взвешивание классов\n- Батч нормализация в архитектуре сети\n- Подключение прореженных сверток\n- использовать в качестве обозначения начала и конца слова не 0, а какой-нибудь другой токен (для 0 nn.Embedding всегда выдаёт нулевой вектор, а в этом случае для начала а конца слова будут учиться специальные вектора)\n- Поиграться с численными параметрами модели - с размерностью скрытого представления, числом stacked conv, силой dropout","metadata":{}},{"cell_type":"markdown","source":"Т.к. учить долго лень сравнивать будем при обучении на 5 эпохах","metadata":{}},{"cell_type":"markdown","source":"##  Взвесим классы","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import class_weight","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:27.599776Z","iopub.execute_input":"2024-07-29T15:04:27.600466Z","iopub.status.idle":"2024-07-29T15:04:27.650035Z","shell.execute_reply.started":"2024-07-29T15:04:27.600432Z","shell.execute_reply":"2024-07-29T15:04:27.649140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:28.569237Z","iopub.execute_input":"2024-07-29T15:04:28.570109Z","iopub.status.idle":"2024-07-29T15:04:28.623166Z","shell.execute_reply.started":"2024-07-29T15:04:28.570063Z","shell.execute_reply":"2024-07-29T15:04:28.622104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels.flatten()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:04:29.244905Z","iopub.execute_input":"2024-07-29T15:04:29.245572Z","iopub.status.idle":"2024-07-29T15:04:29.297975Z","shell.execute_reply.started":"2024-07-29T15:04:29.245539Z","shell.execute_reply":"2024-07-29T15:04:29.296998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute class weights using sklearn\nclass_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_labels.flatten().numpy()), y=train_labels.flatten().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:10.699533Z","iopub.execute_input":"2024-07-29T15:05:10.699921Z","iopub.status.idle":"2024-07-29T15:05:12.394601Z","shell.execute_reply.started":"2024-07-29T15:05:10.699892Z","shell.execute_reply":"2024-07-29T15:05:12.393730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:13.692222Z","iopub.execute_input":"2024-07-29T15:05:13.693172Z","iopub.status.idle":"2024-07-29T15:05:13.745169Z","shell.execute_reply.started":"2024-07-29T15:05:13.693130Z","shell.execute_reply":"2024-07-29T15:05:13.744142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:15.025177Z","iopub.execute_input":"2024-07-29T15:05:15.026004Z","iopub.status.idle":"2024-07-29T15:05:15.077355Z","shell.execute_reply.started":"2024-07-29T15:05:15.025972Z","shell.execute_reply":"2024-07-29T15:05:15.076439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights_tensor = class_weights_tensor.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-07-29T15:05:16.185699Z","iopub.execute_input":"2024-07-29T15:05:16.186057Z","iopub.status.idle":"2024-07-29T15:05:16.236861Z","shell.execute_reply.started":"2024-07-29T15:05:16.186027Z","shell.execute_reply":"2024-07-29T15:05:16.235953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Для эмбеддинга начала и конца токена используем ненулевые значения","metadata":{}},{"cell_type":"code","source":"def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n    inputs = torch.ones((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long)\n    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n\n    for sent_i, sent in enumerate(sentences):\n        for token_i, token in enumerate(sent):\n            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n            for char_i, char in enumerate(token.form):\n                # тут мы сдвигаем заполнение символами на один, чтобы были незначащие нули в конце и в начале слова\n                # - показывают нейросети границы слова при разбиении его на n-граммы с помощью сверток\n                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0)                \n                            \n    return inputs, targets","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:01:48.071544Z","iopub.execute_input":"2024-07-29T16:01:48.071961Z","iopub.status.idle":"2024-07-29T16:01:48.132355Z","shell.execute_reply.started":"2024-07-29T16:01:48.071935Z","shell.execute_reply":"2024-07-29T16:01:48.131231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\n\ntest_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\ntest_dataset = TensorDataset(test_inputs, test_labels)","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:49:58.752672Z","start_time":"2019-10-29T19:49:58.526431Z"},"execution":{"iopub.status.busy":"2024-07-29T16:01:51.247183Z","iopub.execute_input":"2024-07-29T16:01:51.247871Z","iopub.status.idle":"2024-07-29T16:02:20.226463Z","shell.execute_reply.started":"2024-07-29T16:01:51.247840Z","shell.execute_reply":"2024-07-29T16:02:20.225590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Включим взвешивание и batch norm","metadata":{}},{"cell_type":"code","source":"class StackedConv1dBatchNorm(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for i in range(1, layers_n+1):\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2, dilation=1),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(features_num),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:24.745405Z","iopub.execute_input":"2024-07-29T16:02:24.746392Z","iopub.status.idle":"2024-07-29T16:02:24.805472Z","shell.execute_reply.started":"2024-07-29T16:02:24.746348Z","shell.execute_reply":"2024-07-29T16:02:24.804606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTaggerBatchNorm(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1dBatchNorm(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1dBatchNorm(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:26.144696Z","iopub.execute_input":"2024-07-29T16:02:26.145058Z","iopub.status.idle":"2024-07-29T16:02:26.205816Z","shell.execute_reply.started":"2024-07-29T16:02:26.145030Z","shell.execute_reply":"2024-07-29T16:02:26.204945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_sentence_level_model = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=64,\n                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:31.983460Z","iopub.execute_input":"2024-07-29T16:02:31.984302Z","iopub.status.idle":"2024-07-29T16:02:32.046109Z","shell.execute_reply.started":"2024-07-29T16:02:31.984268Z","shell.execute_reply":"2024-07-29T16:02:32.045062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_batch_norm_class_weighting_sentence_level_model) = train_eval_loop(batch_norm_class_weighting_sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              nn.CrossEntropyLoss(weight=class_weights_tensor),\n                                              lr=5e-3,\n                                              epoch_n=5,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:02:37.028329Z","iopub.execute_input":"2024-07-29T16:02:37.028717Z","iopub.status.idle":"2024-07-29T16:13:44.930595Z","shell.execute_reply.started":"2024-07-29T16:02:37.028684Z","shell.execute_reply":"2024-07-29T16:13:44.929535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_batch_norm_class_weighting_sentence_level_model.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:13:49.609341Z","iopub.execute_input":"2024-07-29T16:13:49.609732Z","iopub.status.idle":"2024-07-29T16:13:49.670369Z","shell.execute_reply.started":"2024-07-29T16:13:49.609701Z","shell.execute_reply":"2024-07-29T16:13:49.669378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add models\n!git commit -m 'pos-tagging class_weight_batch_norm_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:13:56.953766Z","iopub.execute_input":"2024-07-29T16:13:56.954215Z","iopub.status.idle":"2024-07-29T16:13:59.251277Z","shell.execute_reply.started":"2024-07-29T16:13:56.954182Z","shell.execute_reply":"2024-07-29T16:13:59.250125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:14:04.098952Z","iopub.execute_input":"2024-07-29T16:14:04.099369Z","iopub.status.idle":"2024-07-29T16:14:07.493403Z","shell.execute_reply.started":"2024-07-29T16:14:04.099337Z","shell.execute_reply":"2024-07-29T16:14:07.492212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_sentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T16:14:34.984332Z","iopub.execute_input":"2024-07-29T16:14:34.985186Z","iopub.status.idle":"2024-07-29T16:14:35.059982Z","shell.execute_reply.started":"2024-07-29T16:14:34.985150Z","shell.execute_reply":"2024-07-29T16:14:35.059103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:14:41.962750Z","iopub.execute_input":"2024-07-29T16:14:41.963596Z","iopub.status.idle":"2024-07-29T16:15:19.377694Z","shell.execute_reply.started":"2024-07-29T16:14:41.963560Z","shell.execute_reply":"2024-07-29T16:15:19.376689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Увеличим число слоев и размер эмбеддинга","metadata":{}},{"cell_type":"code","source":"batch_norm_class_weighting_sentence_level_model_more_layers = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=128,\n                                              single_backbone_kwargs=dict(layers_n=5, kernel_size=3, dropout=0.1),\n                                              context_backbone_kwargs=dict(layers_n=5, kernel_size=3, dropout=0.1))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:15:31.877788Z","iopub.execute_input":"2024-07-29T16:15:31.878881Z","iopub.status.idle":"2024-07-29T16:15:31.945120Z","shell.execute_reply.started":"2024-07-29T16:15:31.878827Z","shell.execute_reply":"2024-07-29T16:15:31.944243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_batch_norm_class_weighting_sentence_level_model_more_layers) = train_eval_loop(batch_norm_class_weighting_sentence_level_model_more_layers,\n                                              train_dataset,\n                                              test_dataset,\n                                              nn.CrossEntropyLoss(weight=class_weights_tensor),\n                                              lr=5e-3,\n                                              epoch_n=5,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:15:50.036664Z","iopub.execute_input":"2024-07-29T16:15:50.037569Z","iopub.status.idle":"2024-07-29T16:29:10.819165Z","shell.execute_reply.started":"2024-07-29T16:15:50.037536Z","shell.execute_reply":"2024-07-29T16:29:10.818100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_batch_norm_class_weighting_sentence_level_model_more_layers.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_more_layers_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:29:24.311858Z","iopub.execute_input":"2024-07-29T16:29:24.312623Z","iopub.status.idle":"2024-07-29T16:29:24.379454Z","shell.execute_reply.started":"2024-07-29T16:29:24.312588Z","shell.execute_reply":"2024-07-29T16:29:24.378584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add models\n!git commit -m 'pos-tagging class_weight_batch_norm_more_layers_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:28.273918Z","iopub.execute_input":"2024-07-29T16:29:28.274674Z","iopub.status.idle":"2024-07-29T16:29:30.662390Z","shell.execute_reply.started":"2024-07-29T16:29:28.274641Z","shell.execute_reply":"2024-07-29T16:29:30.661274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:29:33.381617Z","iopub.execute_input":"2024-07-29T16:29:33.381996Z","iopub.status.idle":"2024-07-29T16:29:37.873244Z","shell.execute_reply.started":"2024-07-29T16:29:33.381968Z","shell.execute_reply":"2024-07-29T16:29:37.872125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nbatch_norm_class_weighting_sentence_level_model_more_layers.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_more_layers_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T15:57:12.832951Z","iopub.execute_input":"2024-07-29T15:57:12.833803Z","iopub.status.idle":"2024-07-29T15:57:12.913392Z","shell.execute_reply.started":"2024-07-29T15:57:12.833768Z","shell.execute_reply":"2024-07-29T15:57:12.912452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model_more_layers, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_sentence_level_model_more_layers, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:29:55.663989Z","iopub.execute_input":"2024-07-29T16:29:55.664407Z","iopub.status.idle":"2024-07-29T16:31:03.678840Z","shell.execute_reply.started":"2024-07-29T16:29:55.664375Z","shell.execute_reply":"2024-07-29T16:31:03.677809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Добавим dilated convs и увеличим число слоев","metadata":{}},{"cell_type":"code","source":"class StackedConv1dBatchNormDilatedConvs(nn.Module):\n    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n        super().__init__()\n        layers = []\n        for i in range(0, layers_n):\n            dilation_rate = 2 ** i\n            padding=(kernel_size // 2) * dilation_rate\n            layers.append(nn.Sequential(\n                conv_layer(features_num, features_num, kernel_size, padding=padding, dilation=dilation_rate),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(features_num),\n                nn.LeakyReLU()))\n        self.layers = nn.ModuleList(layers)\n    \n    def forward(self, x):\n        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n        for layer in self.layers:\n            x = x + layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:31:38.283450Z","iopub.execute_input":"2024-07-29T16:31:38.284114Z","iopub.status.idle":"2024-07-29T16:31:38.344105Z","shell.execute_reply.started":"2024-07-29T16:31:38.284063Z","shell.execute_reply":"2024-07-29T16:31:38.343206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceLevelPOSTaggerBatchNormDilatedConvs(nn.Module):\n    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n        super().__init__()\n        self.embedding_size = embedding_size\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n        self.single_token_backbone = StackedConv1dBatchNormDilatedConvs(embedding_size, **single_backbone_kwargs)\n        self.context_backbone = StackedConv1dBatchNormDilatedConvs(embedding_size, **context_backbone_kwargs)\n        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n        self.labels_num = labels_num\n    \n    def forward(self, tokens):\n        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n        batch_size, max_sent_len, max_token_len = tokens.shape\n        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n        \n        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n        char_features = self.single_token_backbone(char_embeddings)\n        \n        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n\n        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n\n        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:31:46.094807Z","iopub.execute_input":"2024-07-29T16:31:46.095190Z","iopub.status.idle":"2024-07-29T16:31:46.155379Z","shell.execute_reply.started":"2024-07-29T16:31:46.095159Z","shell.execute_reply":"2024-07-29T16:31:46.154286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_dilated_convs_sentence_level_model = SentenceLevelPOSTaggerBatchNorm(len(char_vocab), len(label2id), embedding_size=128,\n                                              single_backbone_kwargs=dict(layers_n=6, kernel_size=3, dropout=0.2),\n                                              context_backbone_kwargs=dict(layers_n=6, kernel_size=3, dropout=0.2))\nprint('Количество параметров', sum(np.product(t.shape) for t in batch_norm_class_weighting_sentence_level_model.parameters()))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:12.765241Z","iopub.execute_input":"2024-07-29T16:32:12.766183Z","iopub.status.idle":"2024-07-29T16:32:12.834648Z","shell.execute_reply.started":"2024-07-29T16:32:12.766148Z","shell.execute_reply":"2024-07-29T16:32:12.833721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(best_val_loss,\n best_batch_norm_class_weighting_dilated_convs_sentence_level_model) = train_eval_loop(batch_norm_class_weighting_dilated_convs_sentence_level_model,\n                                              train_dataset,\n                                              test_dataset,\n                                              nn.CrossEntropyLoss(weight=class_weights_tensor),\n                                              lr=5e-3,\n                                              epoch_n=5,\n                                              batch_size=64,\n                                              device='cuda',\n                                              early_stopping_patience=5,\n                                              max_batches_per_epoch_train=500,\n                                              max_batches_per_epoch_val=100,\n                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n                                                                                                                         factor=0.5,\n                                                                                                                         verbose=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:32:35.471313Z","iopub.execute_input":"2024-07-29T16:32:35.471714Z","iopub.status.idle":"2024-07-29T16:48:23.435802Z","shell.execute_reply.started":"2024-07-29T16:32:35.471684Z","shell.execute_reply":"2024-07-29T16:48:23.434821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(best_batch_norm_class_weighting_dilated_convs_sentence_level_model.state_dict(), 'models/task3_cnn_postag/class_weight_batch_norm_dilated_convs_sentence_level_pos.pth')","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:16.542052Z","start_time":"2019-08-29T13:56:16.529110Z"},"execution":{"iopub.status.busy":"2024-07-29T16:48:53.727562Z","iopub.execute_input":"2024-07-29T16:48:53.728270Z","iopub.status.idle":"2024-07-29T16:48:53.798401Z","shell.execute_reply.started":"2024-07-29T16:48:53.728236Z","shell.execute_reply":"2024-07-29T16:48:53.797386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git add models\n!git commit -m 'pos-tagging class_weight_batch_norm_dilated_convs_sentence_level_pos added'","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:48:57.659053Z","iopub.execute_input":"2024-07-29T16:48:57.659763Z","iopub.status.idle":"2024-07-29T16:49:00.067648Z","shell.execute_reply.started":"2024-07-29T16:48:57.659730Z","shell.execute_reply":"2024-07-29T16:49:00.066443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git push","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:49:02.130595Z","iopub.execute_input":"2024-07-29T16:49:02.131115Z","iopub.status.idle":"2024-07-29T16:49:07.254745Z","shell.execute_reply.started":"2024-07-29T16:49:02.131060Z","shell.execute_reply":"2024-07-29T16:49:07.253749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_norm_class_weighting_dilated_convs_sentence_level_model.load_state_dict(torch.load('models/task3_cnn_postag/class_weight_batch_norm_dilated_convs_sentence_level_pos.pth'))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.321566Z","start_time":"2019-10-29T19:46:47.731Z"},"execution":{"iopub.status.busy":"2024-07-29T16:49:44.072236Z","iopub.execute_input":"2024-07-29T16:49:44.073411Z","iopub.status.idle":"2024-07-29T16:49:44.156142Z","shell.execute_reply.started":"2024-07-29T16:49:44.073375Z","shell.execute_reply":"2024-07-29T16:49:44.154958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = predict_with_model(best_batch_norm_class_weighting_dilated_convs_sentence_level_model, train_dataset)\ntrain_loss = F.cross_entropy(torch.tensor(train_pred),\n                             torch.tensor(train_labels))\nprint('Среднее значение функции потерь на обучении', float(train_loss))\nprint(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\nprint()\n\ntest_pred = predict_with_model(best_batch_norm_class_weighting_dilated_convs_sentence_level_model, test_dataset)\ntest_loss = F.cross_entropy(torch.tensor(test_pred),\n                            torch.tensor(test_labels))\nprint('Среднее значение функции потерь на валидации', float(test_loss))\nprint(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))","metadata":{"ExecuteTime":{"end_time":"2019-10-29T19:47:48.324276Z","start_time":"2019-10-29T19:46:48.445Z"},"execution":{"iopub.status.busy":"2024-07-29T16:50:00.356904Z","iopub.execute_input":"2024-07-29T16:50:00.357361Z","iopub.status.idle":"2024-07-29T16:51:14.338951Z","shell.execute_reply.started":"2024-07-29T16:50:00.357325Z","shell.execute_reply":"2024-07-29T16:51:14.337926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Сравним модели на крайних примерах","metadata":{}},{"cell_type":"code","source":"best_batch_norm_class_weighting_sentence_level_model_pos_tagger = POSTagger(best_batch_norm_class_weighting_sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nbatch_norm_class_weighting_sentence_level_model_more_layers_pos_tagger = POSTagger(batch_norm_class_weighting_sentence_level_model_more_layers, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\nbatch_norm_class_weighting_dilated_convs_sentence_level_model_pos_tagger = POSTagger(batch_norm_class_weighting_dilated_convs_sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.105418Z","start_time":"2019-08-29T13:56:42.093744Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:33.999141Z","iopub.execute_input":"2024-07-29T16:56:34.000106Z","iopub.status.idle":"2024-07-29T16:56:34.059752Z","shell.execute_reply.started":"2024-07-29T16:56:34.000042Z","shell.execute_reply":"2024-07-29T16:56:34.058593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentences = [\n    'Мама мыла раму.',\n    'Косил косой косой косой.',\n    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n    'Сяпала Калуша с Калушатами по напушке.',\n    'Пирожки поставлены в печь, мама любит печь.',\n    'Ведро дало течь, вода стала течь.',\n    'Три да три, будет дырка.',\n    'Три да три, будет шесть.',\n    'Сорок сорок'\n]\ntest_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.125540Z","start_time":"2019-08-29T13:56:42.106771Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:42.827250Z","iopub.execute_input":"2024-07-29T16:56:42.827639Z","iopub.status.idle":"2024-07-29T16:56:42.881804Z","shell.execute_reply.started":"2024-07-29T16:56:42.827609Z","shell.execute_reply":"2024-07-29T16:56:42.880928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, best_batch_norm_class_weighting_sentence_level_model_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.148124Z","start_time":"2019-08-29T13:56:42.126930Z"},"execution":{"iopub.status.busy":"2024-07-29T16:56:55.260914Z","iopub.execute_input":"2024-07-29T16:56:55.261777Z","iopub.status.idle":"2024-07-29T16:56:55.329851Z","shell.execute_reply.started":"2024-07-29T16:56:55.261744Z","shell.execute_reply":"2024-07-29T16:56:55.328952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, batch_norm_class_weighting_sentence_level_model_more_layers_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"ExecuteTime":{"end_time":"2019-08-29T13:56:42.168810Z","start_time":"2019-08-29T13:56:42.149698Z"},"execution":{"iopub.status.busy":"2024-07-29T16:57:04.171449Z","iopub.execute_input":"2024-07-29T16:57:04.172381Z","iopub.status.idle":"2024-07-29T16:57:04.252847Z","shell.execute_reply.started":"2024-07-29T16:57:04.172346Z","shell.execute_reply":"2024-07-29T16:57:04.251939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sent_tokens, sent_tags in zip(test_sentences_tokenized, batch_norm_class_weighting_dilated_convs_sentence_level_model_pos_tagger(test_sentences)):\n    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:58:29.011206Z","iopub.execute_input":"2024-07-29T16:58:29.011599Z","iopub.status.idle":"2024-07-29T16:58:29.094995Z","shell.execute_reply.started":"2024-07-29T16:58:29.011569Z","shell.execute_reply":"2024-07-29T16:58:29.093991Z"},"trusted":true},"execution_count":null,"outputs":[]}]}