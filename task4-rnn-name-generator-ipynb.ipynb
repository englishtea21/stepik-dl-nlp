{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Генерация текста с помощью RNN\n","metadata":{}},{"cell_type":"markdown","source":"(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/englishtea21/stepik-dl-nlp.git\n# !pip install -r stepik-dl-nlp/requirements.txt\nimport sys;","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T09:57:59.341119Z","iopub.execute_input":"2024-08-10T09:57:59.341496Z","iopub.status.idle":"2024-08-10T09:58:13.206535Z","shell.execute_reply.started":"2024-08-10T09:57:59.341466Z","shell.execute_reply":"2024-08-10T09:58:13.205309Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'stepik-dl-nlp'...\nremote: Enumerating objects: 461, done.\u001b[K\nremote: Counting objects: 100% (167/167), done.\u001b[K\nremote: Compressing objects: 100% (108/108), done.\u001b[K\nremote: Total 461 (delta 96), reused 121 (delta 58), pack-reused 294\u001b[K\nReceiving objects: 100% (461/461), 166.56 MiB | 31.89 MiB/s, done.\nResolving deltas: 100% (230/230), done.\nUpdating files: 100% (69/69), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/stepik-dl-nlp","metadata":{"execution":{"iopub.status.busy":"2024-08-10T09:58:39.967800Z","iopub.execute_input":"2024-08-10T09:58:39.968625Z","iopub.status.idle":"2024-08-10T09:58:39.975674Z","shell.execute_reply.started":"2024-08-10T09:58:39.968584Z","shell.execute_reply":"2024-08-10T09:58:39.974700Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/stepik-dl-nlp\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T09:58:41.509370Z","iopub.execute_input":"2024-08-10T09:58:41.510043Z","iopub.status.idle":"2024-08-10T09:58:41.520189Z","shell.execute_reply.started":"2024-08-10T09:58:41.510010Z","shell.execute_reply":"2024-08-10T09:58:41.519154Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-08-10T09:58:43.294202Z","iopub.execute_input":"2024-08-10T09:58:43.295050Z","iopub.status.idle":"2024-08-10T09:58:45.580166Z","shell.execute_reply.started":"2024-08-10T09:58:43.295019Z","shell.execute_reply":"2024-08-10T09:58:45.579027Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"englishtea21@mail.ru\"\n!git config --global user.name \"englishtea21\"","metadata":{"execution":{"iopub.status.busy":"2024-08-10T09:58:45.582185Z","iopub.execute_input":"2024-08-10T09:58:45.582509Z","iopub.status.idle":"2024-08-10T09:58:47.665550Z","shell.execute_reply.started":"2024-08-10T09:58:45.582481Z","shell.execute_reply":"2024-08-10T09:58:47.664410Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:20:34.854793Z","start_time":"2019-11-05T18:20:34.372865Z"},"execution":{"iopub.status.busy":"2024-08-10T09:58:49.870082Z","iopub.execute_input":"2024-08-10T09:58:49.870455Z","iopub.status.idle":"2024-08-10T09:58:49.877182Z","shell.execute_reply.started":"2024-08-10T09:58:49.870423Z","shell.execute_reply":"2024-08-10T09:58:49.876251Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Данные\nДатасет содержит ~9k имен, все написаны латиницей.","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/russian_names.txt') as input_file:\n    names = input_file.read()[:-1].split('\\n')","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.509714Z","start_time":"2019-11-05T18:21:03.491489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.946758Z","start_time":"2019-11-05T18:21:03.938432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на распределение длин имен:","metadata":{}},{"cell_type":"markdown","source":"# Препроцессинг","metadata":{}},{"cell_type":"code","source":"len(list(filter(str.isalpha, names))), len(names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nonly_latin_letters=re.compile(r'[A-Za-z]+')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_tmp = list(filter(only_latin_letters.fullmatch, names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(names_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names=names_tmp\ndel names_tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Name length distribution')\nplt.hist(list(map(len, names)), bins=25);","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:05.420060Z","start_time":"2019-11-05T18:21:05.179513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [' ' + line for line in names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all unique characters go here\ntokens = list(set(''.join(names)))\n\nnum_tokens = len(tokens)\nprint ('num_tokens = ', num_tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.335188Z","start_time":"2019-11-05T18:21:07.320148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Мы специально добавляем пробел к нашим именам в тренировочных данных. Зачем? <br>\nEсли не будет специального символа, с которого начинается генерация, то мы лишим нашу модель способности выбирать первый символ последовательности","metadata":{}},{"cell_type":"markdown","source":"После того, как мы обучим нашу нейронную сеть, мы сможем генерировать имена, которые соответствуют некоторым условиям — например, имена, которые начинаются на букву \"a\" или на буквы \"abc\", или какие-либо другие условия. Если же мы захотим генерировать любые имена, начинающиеся с любой буквы, мы просто передадим нашей функции пробел в качестве первого символа. Таким образом, сможем сгенерировать имена, начинающиеся на любую букву. Отлично! С этой небольшой хитростью в коде разобрались.","metadata":{}},{"cell_type":"markdown","source":"### Символы -> id\n\nСоздадим словарь < символ > -> < id >","metadata":{}},{"cell_type":"code","source":"token_to_id = {token: idx for idx, token in enumerate(tokens)}","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.674548Z","start_time":"2019-11-05T18:21:07.671129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_to_id.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n\nfor i in range(num_tokens):\n    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n\nprint(\"Seems alright!\")","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.838814Z","start_time":"2019-11-05T18:21:07.833611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отлично! И теперь мы хотим преобразовать наши входные данные, а именно — наши 9 с небольшим хвостиком тысяч имён в некоторое численное представление, то есть вместо имени мы хотим получить численный вектор. Сделать это мы можем с помощью функций \"to_matrix\", которая будет преобразовывать наше имя из буквенного, человеко-читаемого формата в формат \"вектор с числами\".","metadata":{}},{"cell_type":"code","source":"def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n    max_len = max_len or max(map(len, data))\n    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n    for i in range(len(data)):\n        line_ix = [token_to_id[c] for c in data[i]]\n        data_ix[i, :len(line_ix)] = line_ix\n        \n    if not batch_first: # convert [batch, time] into [time, batch]\n        data_ix = np.transpose(data_ix)\n\n    return data_ix","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.988093Z","start_time":"2019-11-05T18:21:07.977722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Example: cast 4 names to matrices, pad with zeros\nprint('\\n'.join(names[::2000]))\nprint(to_matrix(names[::2000], token_to_id))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:08.136936Z","start_time":"2019-11-05T18:21:08.131609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"22 - id пробела, этот токен встречается вначале (по умолчанию как знак начала), так и может быть в конце - паддинг до равной длины строк в матрице","metadata":{}},{"cell_type":"markdown","source":"# Рекуррентные нейронные сети\n\n<img src=\"img/rnn.png\" width=480>","metadata":{}},{"cell_type":"code","source":"import torch, torch.nn as nn\nimport torch.nn.functional as F\n# from torch.autograd import Variable","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.739438Z","start_time":"2019-11-05T18:21:09.661222Z"},"execution":{"iopub.status.busy":"2024-08-10T05:19:49.192368Z","iopub.execute_input":"2024-08-10T05:19:49.193080Z","iopub.status.idle":"2024-08-10T05:19:55.665579Z","shell.execute_reply.started":"2024-08-10T05:19:49.193041Z","shell.execute_reply":"2024-08-10T05:19:55.664618Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CharRNNCell(nn.Module):\n    \"\"\"\n    Implement the scheme above as torch module\n    \"\"\"\n    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n        super(self.__class__,self).__init__()\n        self.num_units = rnn_num_units\n        \n        self.embedding = nn.Embedding(num_tokens, embedding_size)\n        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward(self, x, h_prev):\n        \"\"\"\n        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n        We'll call it repeatedly to produce the whole sequence.\n        \n        :param x: batch of character ids, variable containing vector of int64\n        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n        \"\"\"\n        # get vector embedding of x\n        x_emb = self.embedding(x)\n        \n        # compute next hidden state using self.rnn_update\n        x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n        h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n        \n        h_next = F.tanh(h_next)\n        \n        assert h_next.size() == h_prev.size()\n        \n        #compute logits for next character probs\n        logits = self.rnn_to_logits(h_next)\n        \n        return h_next, F.log_softmax(logits, -1)\n    \n    def initial_state(self, batch_size):\n        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n        return torch.zeros(batch_size, self.num_units)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.751862Z","start_time":"2019-11-05T18:21:10.741772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_rnn = CharRNNCell()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.071002Z","start_time":"2019-11-05T18:21:11.052377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети, RNN loop","metadata":{}},{"cell_type":"code","source":"def rnn_loop(rnn, batch_index):\n    \"\"\"\n    Computes log P(next_character) for all time-steps in names_ix\n    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n    \"\"\"\n    batch_size, max_length = batch_index.size()\n    hid_state = rnn.initial_state(batch_size)\n    logprobs = []\n\n    for x_t in batch_index.transpose(0,1):\n        hid_state, logp_next = rnn(x_t, hid_state)  \n        logprobs.append(logp_next)\n        \n    return torch.stack(logprobs, dim=1)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.521078Z","start_time":"2019-11-05T18:21:11.510175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom random import sample\n\nchar_rnn = CharRNNCell()\nopt = torch.optim.Adam(char_rnn.parameters())\ncriterion = nn.NLLLoss()\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:12.120106Z","start_time":"2019-11-05T18:21:12.109585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\nfor i in range(1000):\n\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = rnn_loop(char_rnn, batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    # visualizing training process\n    history.append(loss.data.numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history,label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.521061Z","start_time":"2019-11-05T18:21:12.302892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN: генерация имен","metadata":{}},{"cell_type":"code","source":"def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    hid_state = char_rnn.initial_state(batch_size=1)\n    \n    #feed the seed phrase, if any\n    for i in range(len(seed_phrase) - 1):\n        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n    \n    #start generating\n    for _ in range(max_length - len(seed_phrase)):\n        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n        \n        # sample next token and push it back into x_sequence\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n        \n    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.540765Z","start_time":"2019-11-05T18:21:23.524503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.625562Z","start_time":"2019-11-05T18:21:23.544968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_stripped = [name.strip() for name in names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_stripped[::2000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[matched for el in sampled if el.strip() in names_stripped]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Ar'))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.702249Z","start_time":"2019-11-05T18:21:23.629226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[matched for el in sampled if el.strip() in names_stripped]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видно, наша модель именно генерирует новые имена, а не вспоминает запоменнные","metadata":{}},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Koval'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"При маленькой температуре сеть генерирует фамилии, в которых она наиболее уверена <br>\nПри большой - очень разнообразные фамилии","metadata":{}},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Podo', temperature=1.2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Более простое решение\n\n* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n\nКроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n\nПерепишем наш пример с генерацией имен с помощью средств PyTorch.","metadata":{}},{"cell_type":"code","source":"class CharRNNLoop(nn.Module):\n    def __init__(self, num_tokens=num_tokens, emb_size=32, rnn_num_units=64):\n        super(self.__class__, self).__init__()\n        self.num_units = rnn_num_units\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        return torch.zeros(1, batch_size, self.num_units) \n    \nmodel = CharRNNLoop()\nopt = torch.optim.Adam(model.parameters())\ncriterion = nn.NLLLoss()\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.713285Z","start_time":"2019-11-05T18:21:23.704755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=model.to('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the model applies over the whole sequence\nbatch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n# batch_ix = torch.LongTensor(batch_ix).to('cuda')\nbatch_ix = torch.LongTensor(batch_ix)\n\nlogp_seq = model(batch_ix)\n\n# compute loss\nloss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\n\nfor i in range(1000):\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n#     batch_ix = torch.tensor(batch_ix, dtype=torch.int64).to('cuda')\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = model(batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    history.append(loss.data.cpu().numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history, label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    if seed_phrase!=' ':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=model.cpu()\nfor _ in range(10):\n    print(generate_sample(model,seed_phrase='Stryk', temperature=1))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.526436Z","start_time":"2019-11-05T18:21:31.469965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Домашнее задание: мотивационные лозунги","metadata":{}},{"cell_type":"markdown","source":"Возможно стоит учить эмбеддинги n-грамм","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/author_quotes.txt') as input_file:\n    quotes = input_file.read()[:-1].split('\\n')\n    quotes = [' ' + line for line in quotes]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.570320Z","start_time":"2019-11-05T18:21:31.528673Z"},"execution":{"iopub.status.busy":"2024-08-10T09:59:14.927679Z","iopub.execute_input":"2024-08-10T09:59:14.928051Z","iopub.status.idle":"2024-08-10T09:59:14.971534Z","shell.execute_reply.started":"2024-08-10T09:59:14.928021Z","shell.execute_reply":"2024-08-10T09:59:14.970727Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"quotes[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.575286Z","start_time":"2019-11-05T18:21:31.571798Z"},"execution":{"iopub.status.busy":"2024-08-10T09:59:17.556280Z","iopub.execute_input":"2024-08-10T09:59:17.557179Z","iopub.status.idle":"2024-08-10T09:59:17.564077Z","shell.execute_reply.started":"2024-08-10T09:59:17.557144Z","shell.execute_reply":"2024-08-10T09:59:17.563181Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[' If you live to be a hundred, I want to live to be a hundred minus one day so I never have to live without you.',\n \" Promise me you'll always remember: You're braver than you believe, and stronger than you seem, and smarter than you think.\",\n ' Did you ever stop to think, and forget to start again?',\n ' Organizing is what you do before you do something, so that when you do it, it is not all mixed up.',\n ' Weeds are flowers too, once you get to know them.']"},"metadata":{}}]},{"cell_type":"code","source":"tokens = list(set(''.join(quotes)))\ntoken_to_id = {token: idx for idx, token in enumerate(tokens)}\nnum_tokens = len(tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.653673Z","start_time":"2019-11-05T18:21:31.578424Z"},"execution":{"iopub.status.busy":"2024-08-08T13:36:28.263267Z","iopub.execute_input":"2024-08-08T13:36:28.263925Z","iopub.status.idle":"2024-08-08T13:36:28.336508Z","shell.execute_reply.started":"2024-08-08T13:36:28.263890Z","shell.execute_reply":"2024-08-08T13:36:28.335644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, quotes))\nMAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:36:29.301620Z","iopub.execute_input":"2024-08-08T13:36:29.302015Z","iopub.status.idle":"2024-08-08T13:36:29.310273Z","shell.execute_reply.started":"2024-08-08T13:36:29.301983Z","shell.execute_reply":"2024-08-08T13:36:29.309273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens, \nnum_tokens","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:10.090559Z","iopub.execute_input":"2024-08-08T12:34:10.090962Z","iopub.status.idle":"2024-08-08T12:34:10.097270Z","shell.execute_reply.started":"2024-08-08T12:34:10.090931Z","shell.execute_reply":"2024-08-08T12:34:10.096258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:10.939322Z","iopub.execute_input":"2024-08-08T12:34:10.939680Z","iopub.status.idle":"2024-08-08T12:34:10.999455Z","shell.execute_reply.started":"2024-08-08T12:34:10.939652Z","shell.execute_reply":"2024-08-08T12:34:10.998273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharRNNLoop(nn.Module):\n    def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n        super(self.__class__, self).__init__()\n        self.device=device\n        self.num_layers=num_layers\n        self.hidden_size = hidden_size\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n        self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n#         return torch.zeros(1, batch_size, self.hidden_size, device=self.device) \n        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n    \nquotes_baseline_model = CharRNNLoop(num_layers=4)\nopt = torch.optim.Adam(quotes_baseline_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=1)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:16.485102Z","iopub.execute_input":"2024-08-08T12:34:16.485797Z","iopub.status.idle":"2024-08-08T12:34:17.928856Z","shell.execute_reply.started":"2024-08-08T12:34:16.485764Z","shell.execute_reply":"2024-08-08T12:34:17.927459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nquotes_baseline_model = quotes_baseline_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    quotes_baseline_model = nn.DataParallel(quotes_baseline_model)\n    \nquotes_baseline_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:18.847974Z","iopub.execute_input":"2024-08-08T12:34:18.848506Z","iopub.status.idle":"2024-08-08T12:34:19.164620Z","shell.execute_reply.started":"2024-08-08T12:34:18.848473Z","shell.execute_reply":"2024-08-08T12:34:19.163603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport copy","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:21.664325Z","iopub.execute_input":"2024-08-08T12:34:21.665033Z","iopub.status.idle":"2024-08-08T12:34:21.669710Z","shell.execute_reply.started":"2024-08-08T12:34:21.664998Z","shell.execute_reply":"2024-08-08T12:34:21.668605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n    max_len = max_len or max(map(len, data))\n    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n    for i in range(len(data)):\n        line_ix = [token_to_id[c] for c in data[i]]\n        data_ix[i, :len(line_ix)] = line_ix\n        \n    if not batch_first: # convert [batch, time] into [time, batch]\n        data_ix = np.transpose(data_ix)\n\n    return data_ix","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:22.522131Z","iopub.execute_input":"2024-08-08T12:34:22.522508Z","iopub.status.idle":"2024-08-08T12:34:22.530354Z","shell.execute_reply.started":"2024-08-08T12:34:22.522479Z","shell.execute_reply":"2024-08-08T12:34:22.529213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QuotesDataset(Dataset):\n    def __init__(self, quotes_list: list[str], token_to_id: dict, max_len: int):\n        self.quotes_list=copy.deepcopy(quotes_list)\n        self.token_to_id=copy.deepcopy(token_to_id)\n        self.max_len=max_len\n        self.quotes_tensors=torch.LongTensor(to_matrix(self.quotes_list, self.token_to_id, self.max_len)) \n\n    def __len__(self):\n        return len(self.quotes_list)\n\n    def __getitem__(self, idx):\n        return self.quotes_tensors[idx, :]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:23.649784Z","iopub.execute_input":"2024-08-08T12:34:23.650567Z","iopub.status.idle":"2024-08-08T12:34:23.657867Z","shell.execute_reply.started":"2024-08-08T12:34:23.650531Z","shell.execute_reply":"2024-08-08T12:34:23.656659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train = QuotesDataset(quotes, token_to_id, MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:24.583396Z","iopub.execute_input":"2024-08-08T12:34:24.583792Z","iopub.status.idle":"2024-08-08T12:34:25.552001Z","shell.execute_reply.started":"2024-08-08T12:34:24.583755Z","shell.execute_reply":"2024-08-08T12:34:25.550940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(quotes)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:09.808431Z","iopub.execute_input":"2024-08-08T10:48:09.808753Z","iopub.status.idle":"2024-08-08T10:48:09.817348Z","shell.execute_reply.started":"2024-08-08T10:48:09.808726Z","shell.execute_reply":"2024-08-08T10:48:09.816352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:10.242266Z","iopub.execute_input":"2024-08-08T10:48:10.242613Z","iopub.status.idle":"2024-08-08T10:48:10.247010Z","shell.execute_reply.started":"2024-08-08T10:48:10.242585Z","shell.execute_reply":"2024-08-08T10:48:10.246105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataloader = DataLoader(\n    quotes_train, \n    batch_size=128, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:11.216910Z","iopub.execute_input":"2024-08-08T10:48:11.217293Z","iopub.status.idle":"2024-08-08T10:48:11.222631Z","shell.execute_reply.started":"2024-08-08T10:48:11.217265Z","shell.execute_reply":"2024-08-08T10:48:11.221497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix=batch_ix.to(device)\nlogp_seq = quotes_baseline_model(batch_ix)\n\n# compute loss\nloss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:30.068634Z","iopub.execute_input":"2024-08-08T12:34:30.069288Z","iopub.status.idle":"2024-08-08T12:34:30.073897Z","shell.execute_reply.started":"2024-08-08T12:34:30.069255Z","shell.execute_reply":"2024-08-08T12:34:30.072810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:30.646491Z","iopub.execute_input":"2024-08-08T12:34:30.647300Z","iopub.status.idle":"2024-08-08T12:34:30.651955Z","shell.execute_reply.started":"2024-08-08T12:34:30.647266Z","shell.execute_reply":"2024-08-08T12:34:30.650714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=20\nfor epoch in range(num_epochs):\n    quotes_baseline_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = quotes_baseline_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 25 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n\n     # Validate the model and calculate the metric\n    quotes_baseline_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch_ix.to(device)\n\n            logp_seq = quotes_baseline_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    print(f'Val loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n    \n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\ntrained_model = quotes_baseline_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    if seed_phrase!=' ':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:21.958473Z","iopub.execute_input":"2024-08-08T10:48:21.959217Z","iopub.status.idle":"2024-08-08T10:48:21.969597Z","shell.execute_reply.started":"2024-08-08T10:48:21.959184Z","shell.execute_reply":"2024-08-08T10:48:21.968545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_sample(trained_model, seed_phrase='Life is ', temperature=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наша посимвольная модель страдает фигней... <br>\nПопробуем вместо эмбеддинга символов использовать эмбеддинги FastText <br>\nТакже вместо rnn воспользуемся lstm или gru","metadata":{}},{"cell_type":"markdown","source":"## LSTM-based посимвольная модель","metadata":{}},{"cell_type":"code","source":"class CharLSTMLoop(nn.Module):\n    def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n        super(self.__class__, self).__init__()\n        self.device=device\n        self.num_layers=num_layers\n        self.hidden_size = hidden_size\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        return next_logits\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        # Initialize both hidden state (h_0) and cell state (c_0)\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        return (h_0, c_0)\n    \nquotes_char_lstm_model = CharLSTMLoop(emb_size=128, hidden_size=128, num_layers=2)\nopt = torch.optim.Adam(quotes_char_lstm_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:35.392888Z","iopub.execute_input":"2024-08-08T12:34:35.393614Z","iopub.status.idle":"2024-08-08T12:34:35.413283Z","shell.execute_reply.started":"2024-08-08T12:34:35.393571Z","shell.execute_reply":"2024-08-08T12:34:35.412320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nquotes_char_lstm_model = quotes_char_lstm_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    quotes_char_lstm_model = nn.DataParallel(quotes_char_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:36.526797Z","iopub.execute_input":"2024-08-08T12:34:36.527740Z","iopub.status.idle":"2024-08-08T12:34:36.536427Z","shell.execute_reply.started":"2024-08-08T12:34:36.527703Z","shell.execute_reply":"2024-08-08T12:34:36.535250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataloader = DataLoader(\n    quotes_train, \n    batch_size=256, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:37.356553Z","iopub.execute_input":"2024-08-08T12:34:37.357255Z","iopub.status.idle":"2024-08-08T12:34:37.780171Z","shell.execute_reply.started":"2024-08-08T12:34:37.357224Z","shell.execute_reply":"2024-08-08T12:34:37.778871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix=batch_ix.to(device)\nlogp_seq = quotes_char_lstm_model(batch_ix)\n\n# compute loss\nloss = criterion(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:33.778697Z","iopub.execute_input":"2024-08-08T10:48:33.779413Z","iopub.status.idle":"2024-08-08T10:48:33.783736Z","shell.execute_reply.started":"2024-08-08T10:48:33.779379Z","shell.execute_reply":"2024-08-08T10:48:33.782634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:34.293087Z","iopub.execute_input":"2024-08-08T10:48:34.293875Z","iopub.status.idle":"2024-08-08T10:48:34.297778Z","shell.execute_reply.started":"2024-08-08T10:48:34.293846Z","shell.execute_reply":"2024-08-08T10:48:34.296758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbest_model=None\nbest_loss=float('inf')\nfor epoch in range(num_epochs):\n    quotes_char_lstm_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = quotes_char_lstm_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 25 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n    \n    # Validate the model and calculate the metric\n    quotes_char_lstm_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch.to(device)\n\n            logp_seq = quotes_char_lstm_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    \n    if val_loss<best_loss:\n        print(f'Новый лучший лосс: {val_loss}')\n        best_loss=val_loss\n        best_model=copy.deepcopy(quotes_char_lstm_model)\n    \n    print(f'Текущий loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n\n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\ntrained_model = best_model\n# train_model = quotes_char_lstm_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n    if seed_phrase.strip() != '':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, hid_state = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(token_to_id), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(char_torch_rnn.device)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:38.423069Z","iopub.execute_input":"2024-08-08T10:48:38.423730Z","iopub.status.idle":"2024-08-08T10:48:38.434648Z","shell.execute_reply.started":"2024-08-08T10:48:38.423678Z","shell.execute_reply":"2024-08-08T10:48:38.433723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in np.linspace(1, 1.5, 10):\n    print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Получается туфта, возьмем эмбеддинги fasttext","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:00:17.604779Z","iopub.execute_input":"2024-08-10T10:00:17.605318Z","iopub.status.idle":"2024-08-10T10:00:17.610280Z","shell.execute_reply.started":"2024-08-10T10:00:17.605284Z","shell.execute_reply":"2024-08-10T10:00:17.609306Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:00:22.293476Z","iopub.execute_input":"2024-08-10T10:00:22.293851Z","iopub.status.idle":"2024-08-10T10:00:39.285632Z","shell.execute_reply.started":"2024-08-10T10:00:22.293822Z","shell.execute_reply":"2024-08-10T10:00:39.284217Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"--2024-08-10 10:00:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.96, 3.163.189.51, 3.163.189.108, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.96|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: 'cc.en.300.bin.gz'\n\ncc.en.300.bin.gz    100%[===================>]   4.19G   264MB/s    in 16s     \n\n2024-08-10 10:00:39 (271 MB/s) - 'cc.en.300.bin.gz' saved [4503593528/4503593528]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!gunzip cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:00:57.981055Z","iopub.execute_input":"2024-08-10T10:00:57.981770Z","iopub.status.idle":"2024-08-10T10:01:56.419973Z","shell.execute_reply.started":"2024-08-10T10:00:57.981740Z","shell.execute_reply":"2024-08-10T10:01:56.418689Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"ft = fasttext.load_model('cc.en.300.bin')\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:01:59.868549Z","iopub.execute_input":"2024-08-10T10:01:59.868972Z","iopub.status.idle":"2024-08-10T10:02:05.974494Z","shell.execute_reply.started":"2024-08-10T10:01:59.868912Z","shell.execute_reply":"2024-08-10T10:02:05.973566Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"markdown","source":"Понизим размерность","metadata":{}},{"cell_type":"code","source":"EMB_SIZE=100","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:02:09.771833Z","iopub.execute_input":"2024-08-10T10:02:09.772197Z","iopub.status.idle":"2024-08-10T10:02:09.776577Z","shell.execute_reply.started":"2024-08-10T10:02:09.772169Z","shell.execute_reply":"2024-08-10T10:02:09.775687Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"fasttext.util.reduce_model(ft, EMB_SIZE)\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:02:11.684313Z","iopub.execute_input":"2024-08-10T10:02:11.684987Z","iopub.status.idle":"2024-08-10T10:02:25.333303Z","shell.execute_reply.started":"2024-08-10T10:02:11.684957Z","shell.execute_reply":"2024-08-10T10:02:25.332400Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}]},{"cell_type":"markdown","source":"Будем нашей моделью предсказывать следующий токен: n-грамму fasttext","metadata":{}},{"cell_type":"markdown","source":"Весь текст токенизируем на слова и пунктуацию","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:25:45.716629Z","iopub.execute_input":"2024-08-10T10:25:45.717016Z","iopub.status.idle":"2024-08-10T10:25:45.721311Z","shell.execute_reply.started":"2024-08-10T10:25:45.716980Z","shell.execute_reply":"2024-08-10T10:25:45.720444Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"quotes_tokenized = [word_tokenize(quote) for quote in quotes]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:25:47.204678Z","iopub.execute_input":"2024-08-10T10:25:47.206124Z","iopub.status.idle":"2024-08-10T10:25:59.192002Z","shell.execute_reply.started":"2024-08-10T10:25:47.206080Z","shell.execute_reply":"2024-08-10T10:25:59.190989Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"quotes_tokenized[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:25:59.193849Z","iopub.execute_input":"2024-08-10T10:25:59.194618Z","iopub.status.idle":"2024-08-10T10:25:59.201768Z","shell.execute_reply.started":"2024-08-10T10:25:59.194581Z","shell.execute_reply":"2024-08-10T10:25:59.200970Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"['If',\n 'you',\n 'live',\n 'to',\n 'be',\n 'a',\n 'hundred',\n ',',\n 'I',\n 'want',\n 'to',\n 'live',\n 'to',\n 'be',\n 'a',\n 'hundred',\n 'minus',\n 'one',\n 'day',\n 'so',\n 'I',\n 'never',\n 'have',\n 'to',\n 'live',\n 'without',\n 'you',\n '.']"},"metadata":{}}]},{"cell_type":"markdown","source":"Будем использовать n-граммы различных размеров","metadata":{}},{"cell_type":"code","source":"import random\nfrom typing import List, Dict\n\nutils_tokens={'<PAD>': 0, '<UNK>': 1}\n\ndef extract_ngrams(word, n):\n    # Function to extract n-grams from text\n    \n    # артикли и пунктуацию добавляем как есть\n    if len(word)==1:\n        return [word]\n    ngrams = [word[i:i+n] for i in range(len(word) - n + 1)]\n    return ngrams\n\ndef build_ngram_vocab(texts_tokenized, ns: List[int], utils_tokens=utils_tokens):\n    ngram_to_index = utils_tokens.copy()\n    ngram_to_index[' '] = len(ngram_to_index)\n    index = len(ngram_to_index)\n    for n in ns:\n        for text in texts_tokenized:\n            for word in text:\n                ngrams = extract_ngrams(word, n)\n                for ngram in ngrams:\n                    if ngram not in ngram_to_index:\n                        ngram_to_index[ngram] = index\n                        index += 1\n    return ngram_to_index","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:28:56.368290Z","iopub.execute_input":"2024-08-10T10:28:56.368667Z","iopub.status.idle":"2024-08-10T10:28:56.377181Z","shell.execute_reply.started":"2024-08-10T10:28:56.368640Z","shell.execute_reply":"2024-08-10T10:28:56.376224Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Sample text data\n# texts = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\n\n# Build vocabulary for bigrams (2-grams)\nNGRAMS=[2, 3]\nNGRAM_VOCAB = build_ngram_vocab(quotes_tokenized, NGRAMS)\nNUM_NGRAMS = len(NGRAM_VOCAB)\nNUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:29:02.753985Z","iopub.execute_input":"2024-08-10T10:29:02.754347Z","iopub.status.idle":"2024-08-10T10:29:06.631601Z","shell.execute_reply.started":"2024-08-10T10:29:02.754320Z","shell.execute_reply":"2024-08-10T10:29:06.630526Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"10986"},"metadata":{}}]},{"cell_type":"code","source":"# for key, val in NGRAM_VOCAB.items():\n#     if key==' ':\n#         print(val)\n#         break","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:26:50.645814Z","iopub.execute_input":"2024-08-10T10:26:50.646665Z","iopub.status.idle":"2024-08-10T10:26:50.651659Z","shell.execute_reply.started":"2024-08-10T10:26:50.646629Z","shell.execute_reply":"2024-08-10T10:26:50.650768Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create an embedding matrix for n-grams\ndef create_ngram_embedding_matrix(ngram_vocab, ft):\n    emb_size = ft.get_dimension()\n    embedding_matrix = torch.zeros((len(ngram_vocab), emb_size))\n    for ngram, idx in ngram_vocab.items():\n        embedding_matrix[idx] = torch.tensor(ft.get_word_vector(ngram))\n    return embedding_matrix\n\n# Generate embedding matrix\nEMBEDDING_MATRIX = create_ngram_embedding_matrix(NGRAM_VOCAB, ft)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:29:11.212840Z","iopub.execute_input":"2024-08-10T10:29:11.213486Z","iopub.status.idle":"2024-08-10T10:29:11.490969Z","shell.execute_reply.started":"2024-08-10T10:29:11.213434Z","shell.execute_reply":"2024-08-10T10:29:11.490168Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:26:57.188402Z","iopub.execute_input":"2024-08-10T10:26:57.188846Z","iopub.status.idle":"2024-08-10T10:26:57.196050Z","shell.execute_reply.started":"2024-08-10T10:26:57.188812Z","shell.execute_reply":"2024-08-10T10:26:57.195097Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"torch.Size([10985, 100])"},"metadata":{}}]},{"cell_type":"markdown","source":"Каждое предложение предсатвляется всеми возможными n-граммами, разные по размеру n-граммы отделяются специальным токеном \\<NEW_NGRAM\\>","metadata":{}},{"cell_type":"markdown","source":"Мы будем каждый раз разбивать предложение на n-граммы рандомно","metadata":{}},{"cell_type":"code","source":"def extract_ngrams_of_random_size(word: str, ns: List[int]) -> List[str]:\n    \"\"\"\n    Extract n-grams of randomly chosen sizes from the word.\n    For each n-gram, the size is sampled from the list of possible sizes.\n    \n    :param text: The input text.\n    :param ns: List of possible n-gram sizes to consider.\n    :return: A list of n-grams of randomly chosen sizes.\n    \"\"\"\n    ngrams = []\n    \n    # Iterate over each possible start position of n-grams in the text\n    for start in range(len(word)):\n        # Randomly choose an n-gram size for this start position\n        n = random.choice(ns)\n        \n        # Ensure the size does not exceed the remaining length of the text\n        if start + n <= len(word):\n            ngram = word[start:start + n]\n            ngrams.append(ngram)\n    \n    return ngrams","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:29:16.948353Z","iopub.execute_input":"2024-08-10T10:29:16.949085Z","iopub.status.idle":"2024-08-10T10:29:16.955168Z","shell.execute_reply.started":"2024-08-10T10:29:16.949055Z","shell.execute_reply":"2024-08-10T10:29:16.954272Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def prepare_sent(sentence_tokenized, ngram_vocab, ns, utils_tokens=utils_tokens):\n    \"\"\"Prepare a sentence as embeddings.\"\"\"\n    ngrams = []\n    for word in sentence_tokenized:\n        ngrams.extend(extract_ngrams_of_random_size(word, ns))\n        ngrams.append(ngram_vocab[' '])\n    ngrams.pop()\n    indices=[ngram_vocab.get(ngram, utils_tokens.get('<UNK>')) for ngram in ngrams]\n\n    return torch.LongTensor(indices)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:30:39.211634Z","iopub.execute_input":"2024-08-10T10:30:39.212025Z","iopub.status.idle":"2024-08-10T10:30:39.219762Z","shell.execute_reply.started":"2024-08-10T10:30:39.211993Z","shell.execute_reply":"2024-08-10T10:30:39.218675Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"sentences = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\nprepare_sent(word_tokenize(sentences[0]), NGRAM_VOCAB, NGRAMS)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:30:40.988322Z","iopub.execute_input":"2024-08-10T10:30:40.988687Z","iopub.status.idle":"2024-08-10T10:30:40.996886Z","shell.execute_reply.started":"2024-08-10T10:30:40.988658Z","shell.execute_reply":"2024-08-10T10:30:40.996011Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"tensor([ 165, 3164, 2087,  189,  187,    1, 2144,   95,   22, 2635, 2448,  109,\n           1,   77,    1, 2358, 4625, 3630, 3012,    1, 2143, 3585,   63, 2603,\n        2604,  163,  120,  257,   27])"},"metadata":{}}]},{"cell_type":"code","source":"NUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:30:43.051777Z","iopub.execute_input":"2024-08-10T10:30:43.052485Z","iopub.status.idle":"2024-08-10T10:30:43.058067Z","shell.execute_reply.started":"2024-08-10T10:30:43.052452Z","shell.execute_reply":"2024-08-10T10:30:43.057199Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"10986"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:43:59.955169Z","iopub.execute_input":"2024-08-10T10:43:59.955548Z","iopub.status.idle":"2024-08-10T10:43:59.960186Z","shell.execute_reply.started":"2024-08-10T10:43:59.955519Z","shell.execute_reply":"2024-08-10T10:43:59.959246Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"class FastTextEmbLSTMLoop(nn.Module):\n    def __init__(self, device='cpu', num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=128, num_layers=2, embedding_matrix=None):\n        super(FastTextEmbLSTMLoop, self).__init__()\n        self.device = device\n        self.num_tokens = num_tokens\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        \n        # Initialize n-gram embedding layer\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        if embedding_matrix is not None:\n            assert emb_size==embedding_matrix.shape[1]\n            assert num_tokens==embedding_matrix.shape[0]\n            \n            self.emb.weight.data.copy_(embedding_matrix)\n            self.emb.weight.requires_grad = False  # Optional: set to True if you want to fine-tune embeddings\n        \n        # RNN and Linear layers\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.hid_to_logits = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),  # First hidden layer\n            nn.ReLU(),  # Activation function\n            nn.Dropout(0.5),  # Dropout for regularization\n            nn.Linear(hidden_size, hidden_size),  # Second hidden layer\n            nn.ReLU(),  # Activation function\n            nn.Dropout(0.5),  # Dropout for regularization\n            nn.Linear(hidden_size, hidden_size),  # Third hidden layer\n        )\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state = self.initial_state(x.shape[0])\n        \n        # Apply embedding layer\n        x_embedded = self.emb(x)\n        \n        # RNN forward pass\n        h_seq, hidden_state = self.rnn(x_embedded, hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        return next_logits\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        return (h_0, c_0)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:00.827967Z","iopub.execute_input":"2024-08-10T10:44:00.828954Z","iopub.status.idle":"2024-08-10T10:44:00.842132Z","shell.execute_reply.started":"2024-08-10T10:44:00.828907Z","shell.execute_reply":"2024-08-10T10:44:00.841134Z"},"trusted":true},"execution_count":146,"outputs":[]},{"cell_type":"markdown","source":"Т.к. изменили слой эмбеддинга, изменин класс датасета","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:02.347816Z","iopub.execute_input":"2024-08-10T10:44:02.348197Z","iopub.status.idle":"2024-08-10T10:44:02.352609Z","shell.execute_reply.started":"2024-08-10T10:44:02.348167Z","shell.execute_reply":"2024-08-10T10:44:02.351610Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass NGramDataset(Dataset):\n    def __init__(self, sentences, ngram_vocab=NGRAM_VOCAB, ns=NGRAMS):\n        \"\"\"\n        Args:\n            sentences (list of str): List of sentences (each sentence is a string).\n            ngram_vocab (dict): Dictionary mapping n-grams to indices.\n            n (int): Size of the n-grams (e.g., 2 for bigrams).\n        \"\"\"\n        self.sentences = sentences\n        self.ngram_vocab = ngram_vocab\n        self.ns = ns\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        # Prepare the batch for the single sentence\n        return prepare_sent(sentence, self.ngram_vocab, self.ns)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:03.612232Z","iopub.execute_input":"2024-08-10T10:44:03.612636Z","iopub.status.idle":"2024-08-10T10:44:03.619477Z","shell.execute_reply.started":"2024-08-10T10:44:03.612612Z","shell.execute_reply":"2024-08-10T10:44:03.618391Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:04.652246Z","iopub.execute_input":"2024-08-10T10:44:04.652584Z","iopub.status.idle":"2024-08-10T10:44:04.656974Z","shell.execute_reply.started":"2024-08-10T10:44:04.652559Z","shell.execute_reply":"2024-08-10T10:44:04.655913Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataset = NGramDataset(quotes_tokenized, NGRAM_VOCAB, NGRAMS)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:05.437089Z","iopub.execute_input":"2024-08-10T10:44:05.437446Z","iopub.status.idle":"2024-08-10T10:44:05.441907Z","shell.execute_reply.started":"2024-08-10T10:44:05.437414Z","shell.execute_reply":"2024-08-10T10:44:05.440991Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:06.446306Z","iopub.execute_input":"2024-08-10T10:44:06.447147Z","iopub.status.idle":"2024-08-10T10:44:06.451562Z","shell.execute_reply.started":"2024-08-10T10:44:06.447115Z","shell.execute_reply":"2024-08-10T10:44:06.450586Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-length sequences.\"\"\"\n    # `batch` is a list of tensors, each representing a sentence\n    # Pad sequences to the maximum length in this batch\n    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n\nquotes_train_dataloader = DataLoader(\n    quotes_train_dataset, \n    batch_size=32, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:07.372406Z","iopub.execute_input":"2024-08-10T10:44:07.372888Z","iopub.status.idle":"2024-08-10T10:44:07.380718Z","shell.execute_reply.started":"2024-08-10T10:44:07.372846Z","shell.execute_reply":"2024-08-10T10:44:07.379539Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:08.579698Z","iopub.execute_input":"2024-08-10T10:44:08.580096Z","iopub.status.idle":"2024-08-10T10:44:08.585859Z","shell.execute_reply.started":"2024-08-10T10:44:08.580065Z","shell.execute_reply":"2024-08-10T10:44:08.585035Z"},"trusted":true},"execution_count":153,"outputs":[{"execution_count":153,"output_type":"execute_result","data":{"text/plain":"torch.Size([10986, 100])"},"metadata":{}}]},{"cell_type":"code","source":"ft_emb_lstm_model = FastTextEmbLSTMLoop(device, num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=256, num_layers=4, embedding_matrix=EMBEDDING_MATRIX)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:11.500697Z","iopub.execute_input":"2024-08-10T10:44:11.501074Z","iopub.status.idle":"2024-08-10T10:44:11.541145Z","shell.execute_reply.started":"2024-08-10T10:44:11.501045Z","shell.execute_reply":"2024-08-10T10:44:11.540337Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam(ft_emb_lstm_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:13.433745Z","iopub.execute_input":"2024-08-10T10:44:13.434616Z","iopub.status.idle":"2024-08-10T10:44:13.440032Z","shell.execute_reply.started":"2024-08-10T10:44:13.434580Z","shell.execute_reply":"2024-08-10T10:44:13.439192Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nft_emb_lstm_model = ft_emb_lstm_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ft_emb_lstm_model = nn.DataParallel(ft_emb_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:44:14.916212Z","iopub.execute_input":"2024-08-10T10:44:14.916609Z","iopub.status.idle":"2024-08-10T10:44:15.282162Z","shell.execute_reply.started":"2024-08-10T10:44:14.916581Z","shell.execute_reply":"2024-08-10T10:44:15.280944Z"},"trusted":true},"execution_count":156,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[156], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Move model to the first GPU device\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ft_emb_lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mft_emb_lstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use DataParallel to utilize multiple GPUs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"Теперь для подсчета лосса мне нужно разобрать каждое предложение на n-граммы","metadata":{}},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T10:43:39.008032Z","iopub.execute_input":"2024-08-10T10:43:39.008762Z","iopub.status.idle":"2024-08-10T10:43:39.358958Z","shell.execute_reply.started":"2024-08-10T10:43:39.008728Z","shell.execute_reply":"2024-08-10T10:43:39.357724Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:43:39.933243Z","iopub.execute_input":"2024-08-10T10:43:39.933623Z","iopub.status.idle":"2024-08-10T10:43:39.940337Z","shell.execute_reply.started":"2024-08-10T10:43:39.933590Z","shell.execute_reply":"2024-08-10T10:43:39.939418Z"},"trusted":true},"execution_count":141,"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"curr_batch=batch_ix.to(device)\n\nlogp_seq = ft_emb_lstm_model(curr_batch)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:43:40.988607Z","iopub.execute_input":"2024-08-10T10:43:40.989530Z","iopub.status.idle":"2024-08-10T10:43:41.009475Z","shell.execute_reply.started":"2024-08-10T10:43:40.989494Z","shell.execute_reply":"2024-08-10T10:43:41.008684Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"# compute loss\npredictions_logp = logp_seq[:, :-1]\nactual_next_tokens = curr_batch[:, 1:]\n\nloss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\nloss.backward()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:43:41.683875Z","iopub.execute_input":"2024-08-10T10:43:41.684679Z","iopub.status.idle":"2024-08-10T10:43:41.962714Z","shell.execute_reply.started":"2024-08-10T10:43:41.684645Z","shell.execute_reply":"2024-08-10T10:43:41.961504Z"},"trusted":true},"execution_count":143,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[143], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m actual_next_tokens \u001b[38;5;241m=\u001b[39m curr_batch[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions_logp\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m), actual_next_tokens)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"],"ename":"RuntimeError","evalue":"cuDNN error: CUDNN_STATUS_INTERNAL_ERROR","output_type":"error"}]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:43:42.524023Z","iopub.execute_input":"2024-08-10T10:43:42.524957Z","iopub.status.idle":"2024-08-10T10:43:42.532398Z","shell.execute_reply.started":"2024-08-10T10:43:42.524906Z","shell.execute_reply":"2024-08-10T10:43:42.531425Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:40:54.587851Z","iopub.execute_input":"2024-08-10T10:40:54.588786Z","iopub.status.idle":"2024-08-10T10:40:54.592875Z","shell.execute_reply.started":"2024-08-10T10:40:54.588751Z","shell.execute_reply":"2024-08-10T10:40:54.591831Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"import copy","metadata":{"execution":{"iopub.status.busy":"2024-08-10T10:40:55.467514Z","iopub.execute_input":"2024-08-10T10:40:55.468341Z","iopub.status.idle":"2024-08-10T10:40:55.472211Z","shell.execute_reply.started":"2024-08-10T10:40:55.468308Z","shell.execute_reply":"2024-08-10T10:40:55.471285Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbest_model=None\nbest_loss=float('inf')\nfor epoch in range(num_epochs):\n    ft_emb_lstm_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = ft_emb_lstm_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 100 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n    \n    # Validate the model and calculate the metric\n    ft_emb_lstm_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch.to(device)\n\n            logp_seq = ft_emb_lstm_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    \n    if val_loss<best_loss:\n        print(f'Новый лучший лосс: {val_loss}')\n        best_loss=val_loss\n        best_model=copy.deepcopy(ft_emb_lstm_model)\n    \n    print(f'Текущий loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n\n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T10:40:56.622718Z","iopub.execute_input":"2024-08-10T10:40:56.623102Z","iopub.status.idle":"2024-08-10T10:42:46.509669Z","shell.execute_reply.started":"2024-08-10T10:40:56.623071Z","shell.execute_reply":"2024-08-10T10:42:46.508346Z"},"trusted":true},"execution_count":128,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAhwAAAGdCAYAAABZ+qqcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABu30lEQVR4nO3dd5wU5f0H8M/22+tHOTjg6L2KogjYwYJo0PhTYzCiRo2KUdSokMRe0CQaSyKJxqCxETWIDUVAAUFAOgdIk16Pdv1ub8v8/tibvWdmZ3Znttxe+bxfr3txtzu7+8zuMvOd7/N9nsciSZIEIiIioiSyproBRERE1Pwx4CAiIqKkY8BBRERESceAg4iIiJKOAQcRERElHQMOIiIiSjoGHERERJR0DDiIiIgo6ewN/YKBQAAHDx5EVlYWLBZLQ788ERERxUCSJJSXl6NDhw6wWs3nKxo84Dh48CAKCwsb+mWJiIgoAfbt24dOnTqZflyDBxxZWVkAgg3Ozs5u6JcnIiKiGJSVlaGwsDB0HjerwQMOuRslOzubAQcREVETE2s5BItGiYiIKOkYcBAREVHSMeAgIiKipGvwGg4iIqKGJEkSfD4f/H5/qpvSqNlsNtjt9qRNWcGAg4iImq3a2locOnQIVVVVqW5Kk5Ceno6CggI4nc6EPzcDDiIiapYCgQB27doFm82GDh06wOl0csJJHZIkoba2FkePHsWuXbvQq1evmCb3ioQBBxERNUu1tbUIBAIoLCxEenp6qpvT6LndbjgcDuzZswe1tbVIS0tL6POzaJSIiJq1RF+pN2fJfK/4KRAREVHSMeAgIiKipGPAQURE1Micd955mDx5cqqbkVAMOIiIiCjpms0olb/M3QqLBRjZow0GdMxGdpoj1U0iIiKiOs0iw+EPSPjPst145ZsduO715Rj82NcY+9J3eHPpLny18RAOlVanuolERNQISJKEqlpfg/9IkhRzm0+ePIkbbrgBeXl5SE9Px9ixY7F9+/bQ/Xv27MHll1+OvLw8ZGRkYMCAAZgzZ07osRMmTEDbtm3hdrvRq1cvzJgxI+73MRbNIsPhCwTwx3H98d2OY1i1+wQOldbgx0NleOyzzQAAp92K287ujkGdctAx142BHXNS3GIiIkqFaq8f/R+Z2+Cvu/mJi5HujO2Ue+ONN2L79u349NNPkZ2djYceegiXXnopNm/eDIfDgUmTJqG2thaLFy9GRkYGNm/ejMzMTADAww8/jM2bN+PLL79EmzZtsGPHDlRXp+YivFkEHC67DdecXohrTi8EAByv8ODjtQcwb/MRHK3wYOfRSvzt2x2h7a87ozOeumIgbFbOOEdERI2XHGgsXboUI0eOBAC8++67KCwsxOzZs3H11Vdj7969uOqqqzBo0CAAQPfu3UOP37t3L4YOHYphw4YBALp27drg+yBrFgGHWutMF245uztuObs7JEnCp+sPYk7RIew5XoUth8vx/g97sXbvSVzUvx1uO7cHMl3N8m0gIiIVt8OGzU9cnJLXjcWPP/4Iu92O4cOHh25r3bo1+vTpgx9//BEAcPfdd+OOO+7A119/jTFjxuCqq67C4MGDAQB33HEHrrrqKqxZswYXXXQRrrjiilDg0tCaRQ1HJBaLBeNP6Yh//moYvpp8Dv5x/anIcNqw5XA5Xv5mBwY+Ohd7jlemuplERNQALBYL0p32Bv9J5hout9xyC3bu3Ilf/epXKCoqwrBhw/DKK68AAMaOHYs9e/bg3nvvxcGDBzF69Gj87ne/S1pbImn2AYfaJQMLsHTKBbjq1E6h26589Xus2Xsyha0iIiIK169fP/h8PqxYsSJ02/Hjx7F161b0798/dFthYSFuv/12zJo1C/fffz9ef/310H1t27bFxIkT8c477+DFF1/Ea6+91qD7IGtxAQcA5KY78ZerB+OBi/sAAE5U1uL3s4pS3CoiIiKlXr16Yfz48bj11luxZMkSrF+/Htdffz06duyI8ePHAwAmT56MuXPnYteuXVizZg2+/fZb9OvXDwDwyCOP4JNPPsGOHTuwadMmfP7556H7GlqLDDiAYFpt0vk98eU9ZwMAthwux8YDpSluFRERkdKMGTNw2mmn4bLLLsOIESMgSRLmzJkDhyM435Tf78ekSZPQr18/XHLJJejduzdeffVVAIDT6cTUqVMxePBgnHPOObDZbJg5c2ZK9sMixTM4OAZlZWXIyclBaWkpsrOzG/Kldd044wcs3HoUha3cmDv5nJiHLhERUeNRU1ODXbt2oVu3bglfar25ivSexXv+Np3hKC8vx+TJk9GlSxe43W6MHDkSK1euNP3CjclzVw1Gx1w39p2oxgtfb0t1c4iIiJod0wHHLbfcgnnz5uHtt99GUVERLrroIowZMwYHDhxIRvsaRLvsNDx15UAAwIzvd+NEZW2KW0RERNS8mAo4qqur8b///Q9/+tOfcM4556Bnz5547LHH0LNnT0yfPj1ZbWwQ5/fJR8/8TPgDEtbs4YgVIiKiRDIVcPh8Pvj9/rB+HbfbjSVLlmg+xuPxoKysTPHTWJ3WOQ8AsJpDZImIiBLKVMCRlZWFESNG4Mknn8TBgwfh9/vxzjvvYNmyZTh06JDmY6ZNm4acnJzQT2FhYUIangyndakLOJjhICJqNhp4bESTlsz3ynQNx9tvvw1JktCxY0e4XC68/PLLuO6662C1aj/V1KlTUVpaGvrZt29f3I1OllPrAo71+0rg9QdS3BoiIoqHPGy0qqoqxS1pOuT3Sn7vEsn0+M8ePXpg0aJFqKysRFlZGQoKCnDttdcqFosRuVwuuFyuuBvaELq3yUBuugMlVV5s2F+C07q0SnWTiIgoRjabDbm5uSguLgYApKenJ3WK8aZMkiRUVVWhuLgYubm5sNliW/slkpgnnMjIyEBGRgZOnjyJuXPn4k9/+lMi25USVqsFZ/dqi8/WH8SXRYcZcBARNXHt27cHgFDQQZHl5uaG3rNEMx1wzJ07F5IkoU+fPtixYwceeOAB9O3bFzfddFMy2tfgLh3YHp+tP4jvth9LdVOIiChOFosFBQUFyM/Ph9frTXVzGjWHw5GUzIbMdMBRWlqKqVOnYv/+/WjVqhWuuuoqPP3000np70mFHvmZAIAj5TUpbgkRESWKzWZL6smUojMdcFxzzTW45pprktGWRqFtZrDepKTKC4/PD5edX1AiIqJ4tdjF2/TkuB1w2IJFRccrOOMoERFRIjDgULFaLWhTl+U4Wu5JcWuIiIiaBwYcGvKzggHHodLqFLeEiIioeWDAoaFtVnDq9ue+2prilhARETUPDDg0XD2sEwBg17FKzjhKRESUAAw4NJzfJz/0e1WtP4UtISIiah4YcGhw2q2wW4MjVapqfSluDRERUdPHgENHujM4/0alhxkOIiKieDHg0JHhCs6JxgwHERFR/Bhw6GCGg4iIKHEYcOhghoOIiChxGHDoCGU4OEqFiIgobgw4dGQ46zIcHmY4iIiI4sWAQ0d6XZcKMxxERETxY8ChI6OuS4UZDiIiovgx4NCR7mSGg4iIKFEYcOjIcMnDYpnhICIiihcDDh05bgcA4GRVbYpbQkRE1PQx4NDRKsMJACip8qa4JURERE0fAw4deenBgIMZDiIiovgx4NCRmx7sUmGGg4iIKH4MOHQww0FERJQ4DDh0yAFHVa0fHh+HxhIREcWDAYeOrDQ7rJbg7+xWISIiig8DDh1WqwXZdUNjS6sZcBAREcWDAUcE6Y7g5F/VnG2UiIgoLgw4InDXradS7WXAQUREFA8GHBEw4CAiIkoMBhwRuNmlQkRElBAMOCJIY8BBRESUEAw4IkhnlwoREVFCMOCIQO5SqWHAQUREFBcGHBHIRaNV7FIhIiKKi6mAw+/34+GHH0a3bt3gdrvRo0cPPPnkk5AkKVntS6lQDQczHERERHGxm9n4ueeew/Tp0/HWW29hwIABWLVqFW666Sbk5OTg7rvvTlYbUyZUw8EMBxERUVxMBRzff/89xo8fj3HjxgEAunbtivfffx8//PBDUhqXaqzhICIiSgxTXSojR47EggULsG3bNgDA+vXrsWTJEowdOzYpjUs1uUuFNRxERETxMZXhmDJlCsrKytC3b1/YbDb4/X48/fTTmDBhgu5jPB4PPB5P6O+ysrLYW9vAONMoERFRYpjKcHzwwQd499138d5772HNmjV466238Je//AVvvfWW7mOmTZuGnJyc0E9hYWHcjW4orOEgIiJKDItkYohJYWEhpkyZgkmTJoVue+qpp/DOO+9gy5Ytmo/RynAUFhaitLQU2dnZcTQ9+b7edBi3vb0apxTmYvakUaluDhERUcqUlZUhJycn5vO3qS6VqqoqWK3KpIjNZkMgENB9jMvlgsvlMt2wxiArzQEAKKvxprglRERETZupgOPyyy/H008/jc6dO2PAgAFYu3YtXnjhBdx8883Jal9KZbuDb095jS/FLSEiImraTAUcr7zyCh5++GHceeedKC4uRocOHfCb3/wGjzzySLLal1LZdRmOcmY4iIiI4mIq4MjKysKLL76IF198MUnNaVyy0oJvT403AK8/AIeNM8ETERHFgmfQCDJd9fEYu1WIiIhix4AjArvNioy6obFl1exWISIiihUDjiiyQnUczHAQERHFigFHFHIdBwtHiYiIYseAIwo54ChjhoOIiChmDDiiyHZz8i8iIqJ4MeCIgjUcRERE8WPAEQVrOIiIiOLHgCOK+oCDGQ4iIqJYMeCIgtObExERxY8BRxTZ8iiVamY4iIiIYsWAI4pQ0aiHGQ4iIqJYMeCIgjUcRERE8WPAEYU8DwcDDiIiotgx4IgiLz0YcBwt90CSpBS3hoiIqGliwBFFl9YZcNqsqPD4sP9kdaqbQ0RE1CQx4IjCYbOiV7tMAMCmg2Upbg0REVHTxIDDgD7tswAAPx2tSHFLiIiImiYGHAZkuYIjVapr/SluCRERUdPEgMMApz34Nnn9gRS3hIiIqGliwGGAwxZ8mzw+BhxERESxYMBhADMcRERE8WHAYYAccNQyw0FERBQTBhwGOOu6VGqZ4SAiIooJAw4D2KVCREQUHwYcBshFo+xSISIiig0DDgPqu1S4lgoREVEsGHAYUF80yom/iIiIYsGAwwB2qRAREcWHAYcBrlDRKLtUiIiIYsGAwwDOw0FERBQfBhwGyF0qHBZLREQUGwYcBsgZDq6lQkREFBsGHAZwplEiIqL4mAo4unbtCovFEvYzadKkZLWvUXDaLQCAo+UeVHh8KW4NERFR02Mq4Fi5ciUOHToU+pk3bx4A4Oqrr05K4xoLp80W+v2e99emsCVERERNk93Mxm3btlX8/eyzz6JHjx4499xzE9qoxsZRl+EAgAVbilPYEiIioqbJVMAhqq2txTvvvIP77rsPFotFdzuPxwOPxxP6u6ysLNaXTBm5hoOIiIhiE/OZdPbs2SgpKcGNN94Ycbtp06YhJycn9FNYWBjrS6aMPEqFiIiIYhPzmfSNN97A2LFj0aFDh4jbTZ06FaWlpaGfffv2xfqSKeMQMhyZrpiTQkRERC1WTGfPPXv2YP78+Zg1a1bUbV0uF1wuVywv02i4hAxHz/zMFLaEiIioaYopwzFjxgzk5+dj3LhxiW5Po2SxWPDPX50GAOBqKkREROaZDjgCgQBmzJiBiRMnwm5vOd0LaY7g0FgvZxslIiIyzXTAMX/+fOzduxc333xzMtrTaDlswZE4XE+FiIjIPNMpiosuugiS1PI6FuShsb5Ay9t3IiKieHG8p0HySBUuUU9ERGQeAw6DuEQ9ERFR7BhwGMQaDiIiotgx4DCoPsPBGg4iIiKzGHAY5Kib/KuWGQ4iIiLTGHAYJHep+BhwEBERmcaAwyB5WGxAAvwcGktERGQKAw6DxAXcPt9wMIUtISIianoYcBgkBhz3zFyXuoYQERE1QQw4DJJrOIiIiMg8BhwGWSwMOIiIiGLFgIOIiIiSjgEHERERJR0DDiIiIko6BhxERESUdAw4iIiIKOkYcJjQo21G6HdJ4myjRERERjHgMOHD20eGfvdxenMiIiLDGHCY4HbYQr97uYgbERGRYQw4TLALs416fcxwEBERGcWAwwS7tT7gqGWGg4iIyDAGHCZYLJbQMvW+AAMOIiIioxhwmCR3q7BLhYiIyDgGHCbJy9SzS4WIiMg4BhwmOdilQkREZBoDDpOc7FIhIiIyjQGHSXZ2qRAREZnGgMMkh5zhYMBBRERkGAMOk+QajuMVtSluCRERUdPBgMOkqlo/AGDSe2tS3BIiIqKmgwGHSXtPVKW6CURERE0OA444BLhiLBERkSEMOOLAJeqJiIiMYcARBz8DDiIiIkNMBxwHDhzA9ddfj9atW8PtdmPQoEFYtWpVMtrWKA3ulBP6nbONEhERGWMq4Dh58iRGjRoFh8OBL7/8Eps3b8bzzz+PvLy8ZLWv0Zlx4+mh35nhICIiMsZuZuPnnnsOhYWFmDFjRui2bt26JbxRjVmrDGfod9ZwEBERGWMqw/Hpp59i2LBhuPrqq5Gfn4+hQ4fi9ddfj/gYj8eDsrIyxU9TZrFYYA1ONsoMBxERkUGmAo6dO3di+vTp6NWrF+bOnYs77rgDd999N9566y3dx0ybNg05OTmhn8LCwrgbnWp2q7xiLAMOIiIiIyySJBk+azqdTgwbNgzff/996La7774bK1euxLJlyzQf4/F44PF4Qn+XlZWhsLAQpaWlyM7OjqPpqdPv4a9Q7fXjuwfPR2Gr9FQ3h4iIKOnKysqQk5MT8/nbVIajoKAA/fv3V9zWr18/7N27V/cxLpcL2dnZip+mzl7Xp8IMBxERkTGmAo5Ro0Zh69atitu2bduGLl26JLRRjZ2tbsVYP4fFEhERGWIq4Lj33nuxfPlyPPPMM9ixYwfee+89vPbaa5g0aVKy2tcoMcNBRERkjqmA4/TTT8fHH3+M999/HwMHDsSTTz6JF198ERMmTEhW+xolmxxw+BlwEBERGWFqHg4AuOyyy3DZZZcloy1NhjxKhcNiiYiIjOFaKjGwsUuFiIjIFAYcMZADDmY4iIiIjGHAEYP6DAdHqRARERnBgCMG8igVxhtERETGMOCIATMcRERE5jDgiIGdNRxERESmMOCIgZzh+Gj1/hS3hIiIqGlgwBEDeR6OLzcextbD5SluDRERUePHgCMGcoYDAI6U1aSwJURERE0DA4442W2W6BsRERG1cAw4YiCOTnHagm/hm0t34Za3VsLj86eqWURERI0WA44Y1AqLtsndK499thnzfyzG/1YfSFWziIiIGi0GHDHw+uozHOqhsZUeX0M3h4iIqNFjwBEDr78+4OACbkRERNEx4IiBGGRw8i8iIqLoGHDEoNbHDAcREZEZDDhiII5S8avWU7FwlCwREVEYBhwx8AqjVMTfAUBiwoOIiCgMA44YiEWjrOEgIiKKjgFHDCKNUmGXChERUTgGHDEQu1HUNRxEREQUjgFHDMRuFJ+fXSpERETRMOCIE4fFEhERRceAIwavTjg19DsDDiIiougYcMTg0kEFuKh/OwCA388aDiIiomgYcMQo3WkDwAwHERGREQw4YmSzBt86zsNBREQUHQOOGNmtwQk3mOEgIiKKjgFHjGy2uoCDw2KJiIiiYsARIznDwYm/iIiIomPAESN7XQ0Hu1SIiIiiY8ARI7tNznBIkLhELBERUUQMOGJkq+tS8folxZL0Fq7eRkREFMZUwPHYY4/BYrEofvr27ZustjVqYg1HgBkOIiKiiOxmHzBgwADMnz+//gnspp+iWbAJw2IZbhAREUVmOlqw2+1o3759MtrSpDhs9RN/McNBREQUmekaju3bt6NDhw7o3r07JkyYgL1790bc3uPxoKysTPHTHCgyHIw3iIiIIjIVcAwfPhxvvvkmvvrqK0yfPh27du3C2WefjfLyct3HTJs2DTk5OaGfwsLCuBvdGNTXcDDDQUREFI2pgGPs2LG4+uqrMXjwYFx88cWYM2cOSkpK8MEHH+g+ZurUqSgtLQ397Nu3L+5GNwb1o1QC4FQcREREkcVV8Zmbm4vevXtjx44dutu4XC64XK54XqZREjMcnIeDiIgosrjm4aioqMBPP/2EgoKCRLWnybAJM42KGQ7OwkFERBTOVMDxu9/9DosWLcLu3bvx/fff48orr4TNZsN1112XrPY1WpxplIiIyDhTXSr79+/Hddddh+PHj6Nt27Y466yzsHz5crRt2zZZ7Wu0xOXpI9VwFJfVQALQLjutYRpGRETUCJkKOGbOnJmsdjQ5ma7gW3ei0qOb4fD6AzjjmQUAgK1PXQKX3dZg7SMiImpMuJZKjLq2yQAA7DleBb8QcIihR1m1V/jd11BNIyIianQYcMSoc6t0AEB5jQ8nKmtDt4vZDjEQsbKalIiIWjAGHDFKc9hQkBOsy9h1tDJ0u9i74vULwQcn6yAiohaMAUcc5IDjcFlN6DZx1lGP1x/63ceAg4iIWjAGHHGQi0A9vkDoNjGsEG/3+RlwEBFRy8WAIw4Oe/Dt83jrAwtFhkMMOAL1vxMREbU0DDji4Kyb/Mvjq+86EWs42KVCREQUxIAjDs66DEet2KWil+FglwoREbVgDDji4LAF374aIcMhJjLEgIOjVIiIqCVjwBEHOeAQazgUXSpCIOJlDQcREbVgDDjiEAo4fDpFo15mOIiIiAAGHHFxmajh8PqZ4SAiopaLAUccHHWjVMQajh1HK0K/i10qzHAQEVFLxoAjDlo1HHOKDuOTdQeCt3OUChEREQAGHHGpr+HwK25/8/vdAJRdLZyHg4iIWjIGHHEIzcOhU5+h7FJhDQcREbVcDDji4JTn4fAqgwl5JXqxq8XLLhUiImrBGHDEwaExtbmIE38REREFMeCIg9bibSLFxF+qbpedRytQWu1NXuOIiIgaEQYccZC7VNQ1HBaLnPnQznDsKK7ABc8vwulPzW+AVhIREaUeA444OKNlOLzao1SW/XQMgH6xKRERUXPDgCMOWou3icQuFZ8YXFgsGlsTERE1Xww44iAHHJJOPahHZx4OhhtERNTSMOCIg9ylohYaFqsXcDDiICKiFoYBRxzkYbFqcmiht5aKlREHERG1MAw44iCPUtGjnPir/neGG0RE1NIw4IiDQyfgkAMKcRSKX6dLRdIrACEiImpGGHDEoXOrdKQ7bbr36w2LtQg5Ds5ASkRELQEDjjjkZTixbMpo3fv1hsWKGQ6uIktERC0BA4445aQ7wm6TAwr9USqWsNtLq7z478q9nO6ciIiaJQYcSaQIOPza83DImY/fzlyLh/5XhMkz1xp67lpfAC8v2I71+0oS0VQiIqKkYsCRBCt3n8R/V+5V1GfozcMh375421EAwLdbjxp6jRlLd+GFedsw/u9LE9BiIiKi5GLAkSQP/a9I8bdPZ8SKmPkwo+hAaWwNIyIiSgEGHA1EzHCIAceUWRtier5aX+oWfvMHJKzafQLVtdpryBAREanFFXA8++yzsFgsmDx5coKa03yJAYJfmHtj4dajKK0yXyjqbaCVZncUl6PC41Pc9q/vduL//rEMt729qkHaQERETV/MAcfKlSvxz3/+E4MHD05ke5otsYA0oB4KG8PUow2xtP3avScx5oXFGP38QsXt/1m2BwDw3fZjSW8DERE1DzEFHBUVFZgwYQJef/115OXlJbpNzZJiTg5VwBEWgBjg9SV//o6vNh0GABwp8yhu5+yoRERkVkwBx6RJkzBu3DiMGTMm6rYejwdlZWWKn5ZIzHCoZxf1x3ACb4gMBxhXEBFRgpgOOGbOnIk1a9Zg2rRphrafNm0acnJyQj+FhYWmG9mU/e6i3gCUNRwBVYARy/TmZopGJUnCJ+sOYNuRclOvoW5n6PlMPUvsPD4/bnlrFWYs3dVAr0hERMliKuDYt28f7rnnHrz77rtIS0sz9JipU6eitLQ09LNv376YGtoUuexWDO6UC0B/1lEgtoAjWtHovhNVmLVmP3z+ABZuPYp7Zq7DRX9dbOo19JqlF4gk2uy1BzD/xyN4/LPNDfJ6RESUPHYzG69evRrFxcU49dRTQ7f5/X4sXrwYf/vb3+DxeGCzKRczc7lccLlciWltE5PpssNlD8Z0Hp8fR8pqkOawhdVsmAk4JEnC9W+swPbiiojbnfPnbyFJQHmNDyeras03HhEyHA2U4iiv8UXfiIiImgRTGY7Ro0ejqKgI69atC/0MGzYMEyZMwLp168KCjZYu3WWDsy7gOFruwfBnFmDI419DnZwwE3AUl3uwdMfxqNvJQcGyn47rZiqMPkfY7bE9XVTT5vyIy175DjXe5M/vUVrtxX+W7caxCk/0jYmIKG6mMhxZWVkYOHCg4raMjAy0bt067HYCMpx2uOzBIEy8WvcHlBGHuotFkiTFAm8im9XcGFqb1RJzSkJvNEqyMhz/XLwTAPDw7I3YdqQcnfLSk/NCAB74cD2+3nwEs9YcwOxJo5L2OkREFMSZRpMow2WHyxH+FocNi1WdwaeopkWPtG00Fot+LUY0+o9Lbp/Kh6v3Y/3+UnxRdChpr/H15iMAgHVc/I6IqEHEHXAsXLgQL774YgKa0nTpJR3SnTY4beFvsUc1wkS9nsp/V+3DyGkLsGF/CQCgaH8pJv77B/x4qAwBk6NhbVZLzEWeiazh2H6kHI99ugnF5TUxtYWIiJo2ZjgSQK+bI81h08xwqGsUtE7sB0trcO9/1wEArnx1KRZtO4qbZqyEz2TEYbNYEp7hiOXpLvzrYrz5/W4888WPsTWGiIiaNAYcCRAx4LCHF9LWeJVBw+FS7at++YQvd8EcLqvRzHBEmqnUYrHEPDOoT2fobTwzje4+XhXzYxOFM6USETU8U0WjpM2mU+DpsltDw2JFNT5lhuOW/2gvgqZ+rMNm0ZyV1BeQ4NQJemzW2ObNWL3nBD5cvV/zPrPPJgYuXVsnrxDUiCc/34zPNxxMaRuIiFoiZjgSwKpzsnfZrdo1HAaHfaY5bGF/q0e4AJGH1dqsFkXNhdGr+wc+3KB7n9G1X45VeFBe48W+k9Wh29plG5swTsvxCg++3VIc09ozsjeW7ApbG4aIiJKPAUcCROpS0QpG1EWjetQZjmDAEb6dN0Jdh0VVw2F0zg+9IAowluEorfZi2FPzMeixr3Gisv4Erx6hY8bP/rYUN725Ev9dlZzZamu8fhwqrY6+IRERmcaAIwHsETIcWoxObKXOcLjsVs2Awe+PkOGwKEepGD3hq/dJkVUw8BTiui3HK+pnOo21LkSSJBwoCQYDs9bsx1/nbTO9NowWMQN16cvfYcS0b7A9Ac9LRERKDDgSwKpTw6EOGGRVtUYDDq0Mh3YNhx519sVowKF+nFg7YuQZxPjhtrdXh36v1QmOojVL3O+Vu0/ipQXbTa8No8Vhq9/PnUcrAQA3zljZILOdEhG1JAw4EuB3F/fRvF0vw7HpYJmh51WPcElzWDWLRv0BSZEhELMRwYm/hAyHwWXtwwIO4TmN1IHobaP3+tG6erT2OxEcGp/RgZJqvLRge1Jej4iopWLAkQDXDCvElUM7ht0uZzguHdQ+pudVZzjcOhmOX72xAuNeXhJaQVas6bBZLIrHiBmOYxUePPjReqzdezLsOSMGHKptdxRXYOXuE4rb9MIDvVVuowUcZic8M8qhUdQLAAu3HjX9XEfLPfj7tztQXMbJzYiI1BhwJEhBTvjoCznD8ef/GxLTc9qsymBBr0tle3EFNh8qw466FWS9QreFzWpRnOQXbzuKd1fsARBcs+SDVftx5avfhz2nuoZD0aWiasKYFxbh6n8sC9VYaG0j8+p0qUTLYBjNcGzYX4LvfzpmaFsAmqOIAOBgSTVe+HorivaXGn6uSe+twZ/nbsWv39Ie5kxE1JJxHo4E0TodylfPelfR0fj8Eipr6xd9c9m1Aw5xewDwCqNgLBaL4iR/3wfrAQD9CrKxPsI6IuoMR0CR4aj/vVqoRzlwshodc91h24h0MxwRCl8B/QyIeqG7n/1tKQBg5R/GoG2WK+JzAsoaDlFptRcvf7MDL3+zA7ufHRf1eQDgh13BLE/RAeNBChFRS8EMR4JoTa5lrzuZ6Z3UovEHJFR56k/odmFdlL7ts9A6w6nYvrzGCyB8mGytxjDc/SerURuhnsNuVX41fAHtDMdBYRipkbjK6w9g44FSPPvlFlR6hBV0o2Qw9ObeWLy9Ppsh1ocYXbMl1mCQiIjMYYYjQbTOl3KWQG+p+Wi8AWWGwy9JoRO/1WIJy0KUyQGHkC0ISJJuYOHx6gcc6nk4Xv9uJ0qrvDivT75iHpEDwqRe4ugbvfjBF5Bw2StLAADVtT48Pn5gcN+i1HDo7cPEf/8QykCI7dKbG0XN6HZERBQfBhwJojUqQ2/Kc6NKqmpx/b9WhP72+QOhK32b1RJWZ1FWEwxOxC6V1xbvRKYr/GOWJCniBGTq5/7nop0AgJkrlZNuHSzRDjj0uk7EbMvaui6djQa6IIY/s0D3PrlbRRFwGHzvY11Jd8GPRzCn6DCeGD8AGRrvLxERKfFImSAThnfB69/tUtwWabZOI77brix+/Hbr0dCVvs1qgU3VVVNWLWc4lCf7CqHrQhSpS8Xolf8BRcBR/zo+nZoMsWvG4w1g25HyUMYjVuUeH7LTHPAIa9RoddFodcvEOvOpXBjaKc+Ney/sHdNzxGrjgVLsPVGFSwcVNOjrEhHFgx3YCdK1TQbeu3W44ja9GUjjsXTHcQByhkP58YUyHFEKMLWo6zyMtr1YWJdEzHD4dMaxisFQjc+P5TuPm2mmThuC9RpiF5FWwPOBxpTocleO0Snfw17bYK1IIl32yhLc+e4arItQ9EtE1Ngw4Eggt2pm0WTWB9i0ajiqvdhyuAxbDkefWOxkZa3ib7ng1CwxqyCPWFm95wRmLN2tub0Y2NR4/Ql5j+TF2MSMjTrL4w9ImDKrKOyxcmCiFyBFow76tBhdMM8sTsFORE0JA44EUl8kD+yYE9PzPHPloKjbaNVwHCqtxiUvfhca+hrJY59tVvw9p+hQaFjn/pNV+HLjYUNtFesmKutG1Fw1fRlW7Dqhub3YhVFe48M7y/caep1I5CyDIsMRkLCjuAI3zvgBa/ae1A2o4s1w2KOMQDpSVoMznlmAaV/+GNPzR5KkOIaIKCkYcCTQ4E456NE2A0M65eD7KRegTWb0eSDUvrznbGS7o5fW2KzhGY41e0tMv57s4U824Zp/LoPPH8AvX18R/QF1xDVHqry+qFfzYuahqtaPHw8Zm+ZdS37dPBsbD5ShaH+pItvi9Qdw05s/YOHWo7juteUoq9auY5EDoFi6oQD9YbX/XrIL3+84hndX7MXRck+o6DaR9OY6ISJqjFg0mkAOmxVf33surJbYh8JmuuyG6iesGhmOo+Uena2N6//I3IjFpDK3w4Zqr1+R4aiu9aOkKnLXjF4xaSzys10oLvfgjSW78MaSXXjqioGh+xb8WIx9J4IFrR5fIDRkWM1f15ViJsMhBlV6n9UTnwczSH8c10/xuFi/F9rtSNhTERElHTMcCWazWgydVHq0zUC3Nhlht1s1ikG12DUyHIlgJNgAALczWK8iBhwb9pfi12+t1Nz+N+d2N/X8RrRVZZDWCGvCvLFEOWJIHsGjJmc4jC5qBwCPC91R0YJDMcsVLRgzi/EGETUlDDhS5NrTCzVvt1nCh7tqsVqUgYnWXBvJ4rJbQydasUtl3b4S3W6dTGewfWZO7Gq98jMVf+dnKdev0Zt7I91pQ6lOwCFnNswMj33z+92h3+02K3z+AGb+oF2LIgZYh0oTO6KFGQ4iakoYcKSI3WrVnHTKajU2JNVutUBMhOS4HYlsXkQf/GZEqI2RJg8TpdcFRLHWSgDALWd3U/zdJks5tXu1EPyIct0O3S6V+gyHsXapAyZ/QMJjn23SHAEDAMcr6kcDHS6r1twmVqzhIKKmhAFHijhsFu3p0DWGu2qxWS3ITqsPMnLTYws40hzmvwLZbkcoCyMWakaSUdcFE0+XinoxtoIct+JvvSxGTrpTt2i0PsOh364dxRW4/JUl+GLDIVzw/CLFfV5/IOJIm+MV9XU1iaixEcU4sKbJiic7RkSpx4CjgXx211mKv+027QyH1oReWqxWCwYJw27TnbYIW+s7q2cbvPSLU0w9Js1hRV56MLsgF2aKRvfND7tNnv7b5w8gK8bun7aZyi6Ughzl3yerlHOLyH48VIavN2sP8/UHJJRWeyMWjU753wYUHSjFpPfWYO+JKsV9ry78KWKbjwvznYgr65I5L83fjoGPzcXWw5x7hKipYsDRQAZ1ysGUsX1Df9utOhkOg8WgdqsFPzulA4Dg8FAjQYoWl8OG8ad0RHeNAlbdx9htGNIpV/O+DKcNZ/VqE367KxgQaZ3X8wxmZ9RZnNx0ZZdKpKLMlbtP6t535d+X6nb15KXrd8cYcUzIcFRHWCxPz/p9JdgnBDmKYcctqIjjr/O3ocYbwDNzEj+fCRE1DAYcDUicidQRIcNhZDl7q8WCLq0zMP++c/DZb8+KecTKiboaA5fDeIbEZbdiaOdczfvaZLmQlRYeQGQ467Ma6lqLvAynenNNblUWJ0c1X0lpjKNAdh6r1M1w+AMSnPbY/5uINRw1OjUmev4ydyvG/30pJggL+InNbDnhBhE1Bww4GpAYcNh1aji0lp3XIs831TM/C+2y02IOOOTF18zUcrjsVlzYv53mfVlpds0RM9luB+QmqkeEtEo3GHCogiJ1YFOus0idEV6dGo6ABDh1JvcyQsxwRAs4PD4/fvXGCvxzUbCb5rMNBwFA0Y0jBkZai9FR07TxQClmLN0V84y3TcWRshr8dd42HClr+DWIKPUYcDSgNOEKXW+UirqG467ze+KsnuFdFDZVF0q0kS2nFOZq3t4uO1iImWY3nuGw26zISnNgfF2XjijL5dCcKTXdaQsr8pSpu0b0qAOONpku3f0yS+9A7wsE4DLx3qgVl4tdKpEDjo/XHMB3249h2pdbACh7TOSuFPE707xPTS3LZa8sweOfbcbHaw+kuilJ9eu3VuKlBdt15+uh5o0BRwNSdqlYNE8Y6lEqZ3ZvjYcv6x++nVX9d7QJqMJP6hf2b4dpPx8MILbRKlp1I26nDa00ukjcDhs6t0rXfJ5WGcZqOKyqfbRZLZg9aRTuHt1Lc/sZN55u6HkB/WGxgQDgiuG90RKtaLRKdb8YXMg1JoqAgxFHs7O9uHkXxW48UKb4tzlI1uKMzREDjgak7FKxan5R1VOWu51WzWBCPclVtEXEWmeEr+vy+g3D0LNuMq00EzUcMqc9/DWtFotmF0ma04YurbUDjjyDGQ49eiN0erXL1Lxdi96wWL8kxdWlIqr2+nG4tAbfbik2dJASN5ELV8VMTCIPc0t3HMP7OpOXUcNRZ/GocZuxdBdOf3q+oRW6iQFHg3I7699uh84oFUCZrXDZbdoBhyq7oP5brb1qCKlaLAGHVobDbrVodpG4HTYU6mQ4jHap6NELOMx0hejNNOoPSLoLtJlV4/Vj3Mvf4aY3V+IrjdV41ROlisHFsKfmo7rWDzEuSuSV1YR/rcDUWUXYsL8kYc/ZmFR6fDEXFTekWP4fUuo8/tlmHKuoxQtfb0t1U5oEBhwNSDyY2KwWPHfVYM3txBOc22nTrM9QnwOj1XCoJ81Sc8UwEkPrRGyzWjRHdThsVs2uluB98a0Jo3eQFrtC/u+0ThGfI9JMo0YnN4um2usPzcshF4SK1O+C+nW3HC7TrPvRsulgKe54ZzV+Olphqo0HSxI7G2pjIEkSBj42F0Oe+BpVtbEXFieLGDimxTEiilInMy1Yt1ZcXoN3lu9BRRwF7M2ZqW/39OnTMXjwYGRnZyM7OxsjRozAl19+may2NTvpwtBQu82KMf3b4ccnLsET4wcothMzGmkOW1jtAqBdzyA6VTVstXWUoaexXFlpBQpabZXpTb/eu12W4deUA4eJI7qEbtPPcNR/vQd1zAkVyGrZc7xS9z51bUWsxBqOOUWHw2bOVC/6V+MNv98vnJwiBR9X/v17fLnxMH7z9mpTbWzo7uij5Z6EBXR6fAEptF/qidsawvc/HcOmg6W694uz7zLD0XSI/3/bZwczyBNeX4E/zt6Ixz7dlKpmNWqmAo5OnTrh2WefxerVq7Fq1SpccMEFGD9+PDZt4ptrhLpoFAhmMK4Y2hFts1wY3q0VAGVq3WW3ai5KFlbDIZzoX75uKH6rKqRsrVpZtbCVcsRILIWRWnUjkTItuaqAY9nUCzD/vnPRMU979IqWp64YiHd+PRy/F5Z91ws4xNoLu80Ca4RVfJ/6Qn9CqWijS4xSL2z3/sp9uttKkoQajROxOBTWHwh207y7Yk9oeLNMPontNJnhaMh441BpNU5/ej7GvvhdxO3izbqI6/1YwvJIQZIkJWUm2IMl1fjl6ysw7uUlutuIr8uAo+k4IoxAk7O324uD/9/mbtKe2bilMzXH9OWXX674++mnn8b06dOxfPlyDBgwQOdRJNMrCMtOc+D7KReEDoXiIdFpt0KSwgsa1Ve/YmYhx+0IO6wO7pSj+Pvzu85W/G1mWKxMq0sl0miXbCHgsFktoWGy6pNlJGmO8JlM9Q7SYsbAbg0PONIc1rD3UUuypiT/3+r9+NWZ9ZkasXmVtf6wbIM/EFBkOPyBAF5asB3TF/6EvHQH1j5yUdhrmJ20rCEzHEu2HwMQnHhNkqSwDA8AvLtiD/7w8ca4XscjBIx6Meet/1mN+T8ewfdTLkCHXOMBcDT7DGRUKmujt48aHzEQVmcbOXBFW8wdhn6/HzNnzkRlZSVGjBihu53H40FZWZnip6VKE4pG1VNpO2xW2OtO4K0zXZg4ogtuHtUN2WkOzaJR9XTbYmbBoZoefemUC5DmsOG+C3sDAG4Y0QU5qmnCtU5MC+4/N+z+AR2yFW2Wnd2rDdpnp+G3FwQzK19NVgY0gLJLRWxfvJX5YleVHrvVGnYw/+9t+t9bUaXJfv/3bz0Tt53TPep2+RHqaso1plP3eAOKmUZ9AQnfbikGAJzUKYhMVMFrMrQRsm5lNdrv8ZOfb477dcQuC6/OAnDzfzwCAPhw1f64X0+P3kRt1cL3iyeqpkOs01B/rThUVpvpVbSKioowYsQI1NTUIDMzEx9//DH69w+fJ0I2bdo0PP7443E1srkQU/zRVr58fPzA0O9aXSplqpVRxRO4zWpRBDTySq13nd8TY/q1Q5/24TUTWvUYnVulY/akUfho9T7cf2EfHCipRve2GZqPuffC3hhamBu6Su3bPhs5bodiBVdxLRRx/+V1VmJlJGDpW5AVFrgZvfo3m+EY0aO1odEeh0qVsy2KravQOAF7fAHFSWvxtqPYEmUxMyPFwAHFUNuGO1CKn8fh0hrNGp8Ii/ga5hGyWLW+1K046w0E4LKGf1crPfXfL6NFwZR6yu5N5fcqFZ/ix2v3470Ve/H3CaciPyvyqMRUMX3506dPH6xbtw4rVqzAHXfcgYkTJ2LzZv2rkKlTp6K0tDT0s2+ffr91cyemjLu1Nb5Ymk0jGIiU4bDbrIoTh5w5sVot6N8hWzNjojXXhN1qwSmFuXjqikHIy3BiYMccZeGrMCzWbrWEpcTVB3dxynPxYs/I8NVIg3AirZT71eSz8e8bh2FAh5ywLpVoI3tk0YpG/3vbmaHf5ffWyFTzu49XKpavFw9SWlf8NV6/YqisuiZEi/i5ViquyCSs2HkcZTVe/N8/vq9vQwMeKcVsw6FS7W41fwwNOlbhUTy3WMORyoBDbySU+P1qSlObl1Z78bsP12PpjmOpbkrCnKisxY0zfsCcokNRtxWH0quH1acibrz3v+uxcvdJPFs3U3FjZDrgcDqd6NmzJ0477TRMmzYNQ4YMwUsvvaS7vcvlCo1qkX9asmVTL8C8e88xFYFqZThKq5UnJHEeDofNojihGzmxOjSuhLX61PUeo1WQqU5fR3s+2XcPnh92W6SgJFKhXd/22bigb7u611feF2lEjSha0ejw7q1Dv8tX6Ua6MsprfDjtqfmhwk4xK6U1rM7jC5i+ApY/o7mbDmPAo3PxxpJdAIA3v9+Na19bjtOenKcIXGI9TvoDEnbV1WIYJX4/Dpdqr61h9gS861glhj01H+P/tjR0mzgK5ouiQ9h1TH9EUqKJrdfrzqn2pr5LJZYugL/O24aPVu9XLC6o5VBpNe77YB2K9uuP1Gksnv96KxZuPYo7310TddtAhHWNGjJTqHaysjb6RikSdwdvIBCAx+OJviEBAApy3OhlYhgooH213EOVIbGrulTEE5ORk18sff0ORVYlvI16k2lpefbngwAAD1zcR3Pxt0ijaNQZjjH92uHtX58Rtp08dA0IDhMW37NOeW7cf2FvTJ9watjj9I7FH90+Iiw4kgMO9Wf2syH1686o7/vr/O0AlN1MR8vD/095fH7TAYec4bjvv+sA1NdEzKybVVRdSxRr3/Mjn2zE+X9ZiPdMzFYqvrZeDYpZn68Pzm+y+VB9rZiY4fjPsj04/y8LE/JaRognIvV7LUt1l8ruY5U49cl5+Ns32009bv9JY0OM7/9gPWatOYDL/6Y/UqexOFll/GQdKcPREImq4rIaLPvpeNjtZo67Dc3UWWbq1KlYvHgxdu/ejaKiIkydOhULFy7EhAkTktU+gvIE9cT4AbhhRBc8ern+3B0O1bTpRtL7sUzfLQYpRrsn9EaxXHt6IZZNvQB3ntdDM/PQo63+NOVihiPTZce/Jg7D2b3ahm33p/8bjLN6tsEfx/XDwgfOU2RlctwO/HZ0L3RpXR/IRUvIDOvaKmz21BE9gtkO9fuRJ9SvqAOqhVuDhZ/igeJ3H64Pez2PLxBWnKZFvKKX61RaqdbSiXV1YbXyGi+OlNXg3RXBQOPF+cqTVo3Xj5/9bQmmfRk+7Fi84o9W0yQrqfbioY82YPWeE4bbaKYbRe/KtGh/aUxXjl5FwKGX4RC6VFIQcEz78kecrPLiLyZny9SaaVjLtiPmhmanktF9ApTBYdjnFsPH+Nn6g/hiQ7Ar53iFBzOW7or4nTtz2gJc9/pyfK/q0mrM3XKmikaLi4txww034NChQ8jJycHgwYMxd+5cXHjhhclqH0FZv9C/IBs3jOgato2ihsNqMV1spy6gvEJjJdiw17SJWRVj/1Fz3A7UeMOv3i2W+mGy4snwmmGdcLLKi0cv1y9MVk+UpqdTXjreuWV46G+x20Kuc+nWJhhwuOxWxZVxNHPuPhufbziIO8/vGdYmAMgTJl5zO2yorvWHRk+U1/hQXFaje0KS1XiNZTjEglN5v1pluLDvRH2dRKICjlOfnKe4cs9OUx5SPt9wCBv2l2LD/lJMHdtPcZ+4v16DB8n1+0qwfl8J/rtqH3Y/Oy7sfvWzVNX6UBPnPCord5/A1f9Yhhy3A+sfDR96HInXJwZV2vsoBkSJOldU1foMjd6K5zW1asu0RPqqlVTVxr20QSJFW5NKJJ7Y/epMocmIo7Tai9++vxYAcEHfS3D7O6uxcvdJfLOlGG//erjmY+SXX77zOEYKK4pHmjU51Uxd1r7xxhvYvXs3PB4PiouLMX/+fAYbDUCsfdCrgxD/8ztsVtMBtpiteObKQXjxF0OjPsYZJcPx3i3D0SnPjf/cXN+9keuOfnARa1ZG9miD128Yhk552uuwqJlZ9dammqcDCE7EVvTYRWEnltx0B16/YZjuc/XvkI0HL+kbyl6oD1ziAnUSpLBRQct2Ho96oHhmzhZ8rjElulq5EHDImYM2QsDjD0i6GSm9q6NvthzBff9dpyg8BcK7CbLSlCNN1NX7olrhsUYzHGas3nMCpz45D3+crT+Px7zNRzDq2W8iPs+CH4MZqNJq890+YlBVq7OPYsCRiOGU/1m2G/0fmYtP1h1ApccX9TljfUlHhEjC6w9g1pr9OFhSrTvh3p/nbsEpT8wzVKBpxDdbjuDxzzZFDdwjcZjIcIgZSXWGw+x7Kv6/qvUHsHL3SQDAd9ujF+SqLxb1FqJsDEwPi6XUsFqCEW1vnRVQxZOnuobDCIciYDEW5duFgEPrinlkzzZY8tAFitvU839oEY9PZq44AHNzeqiHEsvUJ00AmDv5HLTLTsPsSaPwyCcb8YdL+4VtI1KnZrOEK3+PLxAs5hRGJ/x0tBJ/+3aH4jFn92oTdsD5+7c/RXzdSo8Pv35rZehv+WQmvu/HKzy6BbN6/b83v7kKQHDNiCfqhmxrzSuRpcpwiJkv9eRePgPzY5glfu2v/scyBKTw4ceiW/+zKupzxjOViVfRzx++j/6ApAhE9ObqMOORT4IzP98zcx2A4HwvAzvm4I2JwwwXbhthj/DGvLFkF579cguy0uyaNVlA/Xf5sU834dJBBXG3R/6O9szPxIThXaJsrc3M8Ub8rNbsLVF8n81+imKgbzb4VhfUN5suFUqdoscuhscX0DwZAsoTpt1mwZndgrUE3Q0Ov1VPA26EGJgYreFQT2+uRbEvJq44AHNTQ4uvE6n1mS472tUVnJ5SmItP7zor6nNHej9KqrxhC9m9vCC8YO/c3m0NXeGIXlu8Ez8drR+FIXcLiQfHE1W1miOfgOgHq4Vbj4Y9tyhb9fmK74PHF1B8PmKQ8fHaA7jzvJ6KrqdYiKnsRB13I02JH43YpeL1KRtU4/Xj4hcXY8/x+uLLZGTDi8s9+GZLcdj7Xy+2F430HV9QN5FaeY0P2TrHLCPPE4tDJfoBZjRm2iIG5+v3leDxz+qnh5AkCSt3n0D3Nhlhy0poqTWQCROJ/5/DMxwMOChOGS47MiJ8b8UrF4fVipx0BzY9frHhVWCdUYa4anFEyXBoMfKfTzwZOu0Nk+GItMuxrGarfj8CUvDqv1xnRk0tZkczAcFpwkVyhkNxQPMFdDMc4lTcWtONy4uf+fwBzeHCmaq6AcV07R5f6IQnSZKiO+ZYRXD+g08MBHOAcpbSWPj8gYhX6KJ4sgLKOhXlieSHXScUwQYQuUvlg5X74LBbMLxba6zcfQLjBhUY3gcgWLOkFXDE2qUiXpgEApLiOyV+tscrI49ijLQPFR6fboZEj9Hh7lrEY5o/ICn+H9d4/fhiwyGUVHsxvFursCzy28v3hH4PSMEMm96SA2ri5HTqwFSL+H/PZrWoMiSNN+BovPMeU8zkeo4Ml93wAUk54sTogbj+d6OPuffCXuiU58a9Y3rrbmONI8PhjjAJmJoyw6F/kDK7HgkQniUKSBL+dNVgAMAvh3c29By922VGnP5ci3ov5ECjVjiIef0B3Su5Vxf+hBU7j+Ph2Rtx5rQFmsNzH/xoPYY/s0BzUbX/rtqHe2auDQU64sFPnuBq3uYjGPz41/hyo7Lvfr2JeRr03hejJ085O6MOJrUeL75VZmssFAGHKiOklSF66osfNddfOVlZiwf/twH3/nc9zv3zt7hn5jr8Z9mesO0iUdffyCLtkSRJut084v9N9b6I3UfR1ivSy6jOKTqEgcLcMUbFkzARj5fqYuPrXl+O+z9cjyc/34zLXlliqOviZJUXJypr8dv314bWDtIijiwzkuEQP8s/zt6IK/5eP+9MY67hYMDRTIgHQjOFT6HHxJCtEE/SRl8yPysNSx66APeM6RV9YxifH2TymF7IdNnxx3GRaytExjMc5t9P9agdSZIwdlABFtx/Lh7/mbGFDttlpeG9W7Ur1PWoZ6CVFy4TD2IeXyDiZ/z6dzvx9vI9OFLmwS9eW4Yznp6vuP+DVftxvLIWb+uc8D5ZdzBU3CqeiOSA49b/rEJ5jQ8bD8S+rlK8/dRyu9RdlFoHazHjpzeXhh5FYWwgvEtFizxaQSSu5yO3YdG2o2HbRWImuyab9N4anP/8Qs22ikFrtdePOUWH8MRnm+EPSIau0mV6xyv5fTCyno5iGgDh8/pk3QGs2q0cQh0ISPh2a7FmMC3+V1dn8NaqZvY1+h18+osf8dn6g7j+jeAEac99tQU/+9sSxZIJ4v8TI/VMlarZj4sO1AfrrOGgBmW20BJQdl0Y7ce0xpDhMKsgx9iMrJPH9MZvL+hlarinzWIs4IhpjhJVO+RjSKT5RNSsVgt65mfhySsG4uEIIy1kkiThwMnwZeqra/34bnv9ycnrlyK+T+KJSawHUauIsKhdcd3BvFa4cqsyuQheJHoHZaOH2sXbjsIfkJDpsuOEMNeBVkChSKv7/FEzXj5/AD8drUTvdpkRR6mU6EwytdfACrOA+coLrdlro5lTFFxm/dN1B2GxAJcOKkBGXReHODKj2usPzc45uFNOWPdRJHrHKzMnTvFzk/8vbzpYGiqclYdQV3h8+GZLMe5+fy3aZDqx6o/KUZbiRyQGWVqZLaPt26eaIG36wmCx7CfrDuAXZ3QOey0j88boZasA1nBQA4ulCEuR4TA6vl5nlEcivHnT6ThRWYuubUysOWOyDUa7VGLLcIR3qcTKaLeKLyDhWIXyqs3rl/DLfy1XdBXU+gIRi2SNXglXRTjoyVeP4pWb2UXwIqn1Bxey23CgFP0KskKV+kZHu0yum3k17HmjHOxrvP6oRZCvfLMDLy3YjhevPUUx4kDdt36iUnuYrXqUwn+W7ca7y8NncBVPgmU1Xvxv9X6MizDaQ2tBQPXz6HnwfxsAAMt3nsDz1wypa2f948QT5r4TVaZGHdmtFqzdexJupw0FOW7NRfyiqRECW7nmRpx3BghmFqYv/Alt6/4/HasID/j8iq6g+ufUCkTjnaRN/L8h/v69xuyhpdVefLhqHy4b3AHtc9Iiru/UmDMc7FJpJsTvfixFbuJVm94IBjVlhiOxAcd5ffLx81M7JfQ51SwGMxwOk4WrQPhVWzyHgH7t9dcfEuc48foDoavYefeeE7pdnQr2+gMRuwbKPcbmm4h00Fu4tRiBgKQ4kKrTwNFEOhF6/QG8vXwPrvj7Ujz00YbQ7fEuzqbVpaI4MUSpRwCAn+rWxvnpaIWiS0V9EtabRrusxocDdfUxa/aexCOfbMLWI5FXBf79rCI8/tlm/DLCuiaVJjJMkiRpvv+frj8Q+l18r8Rgssbnj1q4KP5/W7+/FFe++j0uefE7nP7UfP0HRVCj8d1S197ImQWtrhSZVxFECfun0aWknuxLl85m4vsrfsee+yp88bXff1yEp774Ede9vhxA5M+yMWc4GHA0E/EuFhRLDQcUNRyJDTgak1i6VNQ1HPHMr9Apz61735nCwnEVHl/ogBlpNJDXH4h4Yi7RufJWi5Si/+loJbr/fg6+rZu2HTDfpfKOUPWv5vNL+NeSnQCA2evqJ0OLN+DQqj0QAwUjs5bK70t5jU9ZNOoXr2KP4c3vd+s+x7X/XAYA2Cj0zUfy9ebgMNQdxfrTiOtlrsQ9DgQkPPHZZpz13Lc4WhF+Yhavnr06GY7q2kDUDIfe0ULOXJklBgTyyVusvdGbNVidDRD/lp/T5w9g9PMLwx9rMMMhFoSKzy//VuP1426Nuh3Rt1uC/4/khQcjdqkkYRK9RGHA0UzEG9SKAYfRTGGegUm8mopIWaFYulTUGZ/TuuSZfg6Z1WrBwI7aWQ5xlEWJsABajtuhm7Xx+AIRK+HLDfb1R7pSlInZlUgZES1zNx3Rva/WH1DUw8gnqXgDjnX7SsKu7MXnjDbiAqjvuqjw+JTzcAgn6F++HnmF1f11tTiRgjqxm85IvK/3XOLuegMB/HvpLhwoqcZbGgGReJwRT2ziCb/G549aXBvp3vIa5eyo4lD3LYfLMPOHvWGfkTLg8IetCKyXmVJnmbSyNj8eKtfpfjF2oBRHXynrQoL/yuunRKKeqqDKo/9/iRkOavTEq3ij9Qbd22bi4cv648VrT0lSqxqHeIfFPnZ5fwzsmGPq8er5RD74zQjN7SwWSyjokIsfM5w22KwW3TlJHvxoA9btKzHVHi3FBgIOkdmAo7xGP9Pi9QfQMbc+8yMX5sU7Y+nWI+X4aPV+xW2KgKPuarW4rEa3JkU+sVfU+HBcKEj1+QPYfqQcxzUyB3oiXckqulEjVuUg1J7F247ikhcXhz7/v3+7QzHaRa9LQYs4i+px4YRc4/VHvcqOdIgprfYqut/E1aAvefE7TJlVhK82HlY8Rmzra4t34vy/LMSrC+tn7l28XXtEz3FVICHuf7XXHzbqS2Rk+KqaGHDIx1kjR1v1xUOkVW0bcw0Hi0abiXiXYBBPqmYKHH99Vrf4XriRiHSFGEuXipjhOKNb6whbApPO74E9x6vwuXClc8nA9opttBbiuqh/OwDBDIzX7w+NepBHEaQ77VFP8nnpjoQtDR+Nke6IcS9/h25tMvDCNadg40H9YbNev6SoEyirDp6YPQlIJz/w0QbsPVGF+y7sDYvFospw+HGotBojpn2DDjlp+H7q6LDHy10XGw+WhjIVQDAdPmVWkeIEGk1lhCtZ8b+p0QzHDf/+AQBwxzur8cFvRuDPc7cqtvGaGJ4pBhViEe6sNQc0tjautNqryDRoHY82HSzD2EEF8Ack3D1zrWJVVXnzNUJ2TWuoMYC64K9+gj3xZD1302H85u3VGNVT+/+vkXoeNUUmqO73DAPfB3WG43iEVWQbc4aDAUczEW8Nh1i3Ec+IiqYq0vE63nk4og1TfuDivgCAzzd8ASAY/D0xPvpcHf+4/rTg89d9dnLgkJkmBxzRD2T/mjgMV01fFnU7M+R1f9RKq73YezzykM9NB8uw6WAZSqq8Ea/U/AFJcfCWh2HG26Uie+WbHTi1cx5G9mytuJKt8QawdEdwFMFB1RotR8s9ePCj9aGCz/2qIcpyVsFMpidihkP4P29kduB3V9TXxJRVezWv3v80t75gMXrAkZzjxOZDpXjof0Whv+XshZhRkif4+277UUNdEnqOVaozHPX7LAdO8uetZmY1aZkYdMvfAyNF/upN1KPRRI05w8EuFQrTiGuOkiZiDUcsXSqK2VLDnzvSCIyJI7rorpkjkgt15eyUXFOR6TIecDhtxq+2jdIrWH1t8U6c8+dvDT3Hkh3R15ART8byyS9RAQcA3PTmStzwxg+KgOPLjYcUdTPi6/3pqy34dqv+ZFyxrMkSaTSC4itk4KnF7gK304ZSjczW+z/sC/1eG2XyLm+STmwvzleuK1Tt9SMQkHC4rD7Ak0+qJXFm546pugXNnKzFYlCjtEa+GOmaUX+86q4gkT8gYU7RoYQOQ08UBhzNRCxpfz3McATdPboXctMdeOCiPqafL54F6IZH6YJRkzMwL8zbBqC+/sNQwBFDMBVNvOucGCWOupDT+4kMOABgxa4TiuecteaAInAQR4VESnMDsS1vXxGpS6Xu330nqgx1V4nSHDaciFAHAETPcKinak8UrdV9f/POavz6zfpVkOX30ux+q6nXeTEzk2wsGY4PV9UHdHJAEO07K0mSRpdK5DqgO99dE7EGKlUYcDQTvzijMwZ0yMY9o41NGa6lR93KssO7tUpUs5oMrYvP+y7sjTV/vBCdW6ebfj6xG8XoRGrLpl6AGTedjtH98jXv76rTDnWXj7wWR4aBRa9iWZgumjaZ8a34KuqVrz8z6w/ClNXy1XYshXzRqDMBq/ecDP1+6cvf4fXFweG50eaiiVSAqCfS5GqQgnUhZ//pW9NTrqc5bIq6By16AcfWw+V44eutoZO+0RWp4zFv8xHFwoRyZkNrfgwzVu85qcg2+g3MkCpfTMRSw/GWsByA3KUSLVPiC4QvohgpwyFTL1vfGLCGo5nIdNnxxd1nx/UccyefgxpfwPTqjM2D9ski1vlFxKyG0YnUCnLcKMjRn3Nj5m0jcOa0BeGvpQoa5NqCeDMcFot+MXKO26F7xZ7IDMdHt4/EkCe+jrqdzx+APyApgoFEOViqrMVQz5+xYtdx3HpO96i1PmYzHIGAFHFYrAQJi4R5TsxwO2y6M53K9K68L39liSKwS8XxQi6QNjJMOZLlO09g1poDuHhge3i8fpww0EVjr1udNZYuFVGVwQyHxxcIuyCKlk0DAJej8eUTWuKZhXTYbVZkJrBrpimJYwVyTUbXaTGjvc66MuruNHkBO62RLWqRes+6t8nQXUtlSGEuFussHtY6IzEZDqfdihyDc714/RLmbjocfcMYHNZI8YvkETLRioPNLp5257trsOWw/gyj/oCE3PTY3mur1RJxaCWgnz1QZ5EyDHzPjOjRVv/7plZSnZgMBxBcbfWRTzYangnXbrXAg9i6VETV3uD3IVrAUesLKEYh+QOSoa7DRHazJ0rjaxFRCiRznlSt5y5sZb6bRk9WWv0B32m3hoYqR8twDOuSp5jLQq1XfhbGCsNz04QrpkE6E5EBQLpwxfvMlYNwRoQuujO7699nZk0NXyCA3ccjn6y6q9blaZdtfI2aSEqqgyfuRC9g+FWUAKrWH4h5dFp1rS9qwGE0QDLSdRfNWT3bYPKY3oa3P1xaA0mS4q7hAIKZADPT7stdKvHWC9V3qURfv0c5a6qxtjbG2Z8ZcBAlQbqr/mSfrXHifPkXQ3F+n7a6E3qZIY5o6dwqPdTfKwYcbTJdinqNC/u3w0d3jIx4UMrLcOCaYYWhv1tn1J+gT+1cP3Pqw5f1x79vHFbfHuEEdN0ZhbhUNaeISHxONTMBh9cf0Bx1IdaTzL5rFO67sP6k1irCa5shD0d26qy508Hgisdm1foCEefpiKTS40dZlC4eo6vLZrqM1QoM6KAfpA4pzEHHCFP4qx0oqcamg2Wms0ZazA57l7ePN8Mhz0gbLXCpqvUrLlpiqR1pLNilQoTEd6k4bFYsnXIBJElCmsaMn13bZGDGTWdoPNK8bCHDIc4uKnapPH/NEOw8WoHHP9sMwNjolA45bsVomzaZztAcEwM65ODdW4Yj02XHkMLc0O0AcHq3VhjdNx8d89zBmVAjvFarCN0vuXUBh91qiZpl8PolzRqJ3HRnaFrqDKcdnYXMUquM+oDmwUv6oFW6E1NmFYU9RzSlVV6crKzVnV+jV7ussDk7EsHjC5hen0ZW7fVHDVYMBxxpxk4jkdb3yU5zoJNGtq1TnjtsPhPZZa8sQfvs8GDu56d2NDT52IjurbFs53HNQDWSUNFonDUccrAUrdC50uNTFI3WxPm6qcQMBxGMTQ1tVsdcNzrlJa7rRI+Y4RADjhNCYVnf9lmKIMNl4KquINetGHmRmWbHE+MH4A+X9kP7nDSM6tkGQwpzAQAOYbt0pw1v3Hg6nhg/MHif6rVW/3FM6PfcCDUacoZDXk48kk/WHdAspGsl1DjYrBZF95OY4bjt7O7o1U5/RIyWa4YFVzOu9Qdw5rQF+ERYRE7Uo21mxJlAH7u8v6nXlVV6ogcN+o/1hQKKF64Zorkukt5y9mpil8q5vdvqbtcmQnCZlebQLDa2WS2447weytcTMnfi3BwAMHVsXzx/9RBcfVr0lablmigzI5t+ObxzfYYjzkyDPGzVE6VbqNLjg9hb15QzHAw4iJD4DEdDEk+iacLBuHf74JTNNqsF7bLTFCd+YxmONEWGw2mz4oYRXXHrOd3DtlU8tyrAEP922CyKjI84wqFzq3SM7ls/JFgOOPI1rmLVlu44jnl1K6bmCwGKekiz2L0lFrcG156JfKWepqr6d9qtoX2LlF7PdNkiTuR246humDK2b8TX1nKswoPP1msHOdF4fIHQMN3OrdLRKz8rbBujBZmZQiZN/R6JImWzst12ze49SQLSVMM7O7fWH4ab7rTBYrHgz1cPidRkAOaLm38+tCMe/9kAIcMR24l/0vnBAKqy1h8sAI0S8FR4fIoaDrNrEjUmDDiI0LQDDvEk6hYO+NcOK8ST4wdg2ZQLAAAue/SA47sHzw/9np/tUgYcEYIUxbwjqhOHeJ/TZlU8j1sIkGbdORJ3nl9/NSvvV76BDIdIHM0zpl8+JgzvHJoqXqxrEU+AFotF0RYtrVQjQn5zTg9Do2hcDlvEEzFgfOi02s5jxkZ1aAnNTJtmVwSqZokZDmeEuR+0aplkckCm/qwlSGHDO/XmowGC77VReqO+9JzfNx8OmzX0fY61S6WdEEBX1PhCGYt7x/TW/D+mruFojBN6GcWAgwjKg0BTI2Y4xLoNp92KX43oGsoQKAIOnS6VTnluDO6Ugx5tM9CldYYq4NA/mIsZDvWoBXVmxa7ofqnf1mG1KrIfcoZDnVIvbBW5uDA/q/6zdNqtePrKQbhhRFcAyvcgT3WFmx2lFkHc/sVrT0Fhq3RDha1Om1WzjkeUioBXvkLPcNqRbuJErSZmqbLS7LpFsgURTvDye//c/w1W3B4IKD8zAOgSIcOht0KymtNmNT2kWO7ykb+/sRasZqc5QvtUVuMNLTiY7bbjqlM7hm3/8Ccb4RfGryeiUDZVWDRKLdrrNwzDx2v3mxqSl0paBZTZQro+0oktx11/gNXLVlgsFsy+cxQCkgS7zaoY6hlpXH+aw4YnrxgIvz8Qljp3qgIOsQCud7tM3DyqG6wWICfdgeOV9e2X6ztO7ZyHH34/GnOKDqGwVTpOKczFmr0luPU/qzTbIg53VdePdG+TiQv65iPX7VCMpgGCRY2XDS5QrNorEvdLDvKMDAl12q1h3QKyPu2CXRliyrx9dlpYbUIyZbrsilFVZrUV3u/WGU7MuOkMPP/1Vnxd18UFAF/cfVZYvUmbTFdoETI5w3F+n3wsnzo6NMGdVtF1h1z9wCVSwDFheGe8u2IvgOB3y+y0/m2zgp9/vIvWuZ02ZLsdOFruQVmNNzRKRW9m0PIanyLIiGXG2saCAQe1aBf2b4cL65Z5bwrcDhvKVaMHsnRGqajlCaMyIh1srVYLrHVJXKNdKgDwqzO7aN6uVTvy5BUDcbCkGoM65mBwp9z69jvDMxxAsI7jxlHdQn9f2L8d/nH9aXjy882KETIFOWnoJsy3oZ4bw2q14N83ng4AqKr1obCVG6cU1g/xfeW6odh6uBzbhTVStNojBxrqoEWL027V7VKZPWkUAOX7HMvJ/5IB7XH1sE749VvaQVgkGS67oVlp9bQTMkp56U70aZ+F124Yhq5TgqsfZzhtGNAhB5IkYVTP1qHVVzvluUMBh/j6YleHhPAMR6SsUqRuMTET0yrDaXpiLDnDEe9wWLfDhqw0O46We7Bo29FQ7ZHRAKgpZzjYpULUhMy46XTkZ7nw91+eGrpNHMURaTrjPCGFbHTuAbH+Qn3gN8qhquEAgsHJQ5f0DVsjwq1TUKrlkoHtw4LFq4cVqmoK9Psq0p12LPrd+XjluqGh2ywWi+7aIFptyzAQHDhtVt3aAvkEKZa9RJq5s1OeG989eD5eFtoMBIeCju4XW+DstFvjmi1UPMm31lhHRx42a7FY8PzVp4RuFzNG6tFK8oR05/XJD8twXNBXe60hQJnhUxfiit13uekO01N/y4FOtNoN8Xt7fp/wUTvpzvoi4j99tTV0u/GAo+lmOBhwEDUhw7q2wg9/GINxgwtCt/VuVz/C4GCJ9pwFgPLK0OgsiWYyHHocimLVyCdo8YRhpABQvfhcq3SHIjCKFliZmY1R7MqS38tMl/bV9u3n1tedBDMc0Wo4lMOK9fzqzC4obJUeNtlWG5OFtWqRMgOdo8yKq6iL0aiLEANA8e3OTXfgrZvPwH9uPiNsGv4Pbx+BRy/vjz+O66d4/oW/Ow9ZaQ4s/N15mm0RM0m3n9sDGx+/OPS3GNvmpTsNDQ0XyZ9RpGGpD1/WX/FZa33uaQ6bZr2QOqAfoxNA/uXr4KrQDbFoXqIx4CBq4sSTqnoZa5F48DM6YZTdGp6dMEtdwxGJkcJWkV21TbrLrtjPWKYbF9eXEff/x0Nlod871c2KqTXL5sOX9ceIHq1Dfzvt1qjZITGwi1QXIgcGYkaiICcNpwjdUrEQ6wfU856c2jnyc0cLOMRuJzHAS3PYcG7vtjhHY+6ODrlu3DSqGzJcdsV3ulVdBkVvxIu6SzHDaUOv/Ex0zHUrutpy051h38W/XjsEW5+6BG/dHD4hn9hGdZeKOIOqOgDW+r6nO22ac46og5PKKBOv9WibiTUPX4ghnXIibteYMOAgagbevOl0jOrZGpPH9DK0vTiW/5fDOwOAZi1LQjIcQlAQ7arSYrGErv4GRFivJdQ+VYCV6bKrhv+aH/4hlgSKbb9ndPC9fWL8gNDVrlZwkO60oWOucqSMSCsmFK/8I2U45G4I8XWvHlYY87oZI+sCI/FE+b/bR6JNphOdW6Vj5m1nRpxDBFDun1YXkzLDEV8XnfwYva4s9UnbYrHgy3vOxje/O1dxX4bTFva5+APBwEs9EderE07Fi9eeEvpbPW/GbcK8NOoAVyvD5nbaNCeza6sKQn5xRmHYNiKX3YpWGU48f030OUcaCxaNEjUD5/XJx3l99Pu21cSA45HL+mNMv3yc2b112HaKUSqJqOEw8BzLpo5GrS+gGH2jx69a7jbDZVcEIWbXyQCUGQ6HzQJ5xvSxgwqw6fGLFSdQu8aJ3m61oCCnfuhurS8ASXjSaVcOwpRZRbhZKIIVu1QivUfyXCBinYA4d0W/gmxFJiaaf00MroEjBpYdctPww+/HhIKYrzfVjza5/dwe2HWsAj3aZuLVhT/h12d1Q7rTDrfDhmqvX3M9FLGtNoP7qUfOeullv7RmrrXbrLBD+Vm5nbawUSHn1dVbqDMYgzvlRJy0THweu9WC313UG1NmFeEXpxeG1SgBQLrDrjm3jHpekJ8N6YD/LNuD1XtORnzdnvlZ6N42AzsNrrSbSgw4iFqQQR1zUHSgFONP6RC6Lc1hwwV9tfuL1TONxsLsDKcZLjuMrqsWUA0RznTZlF0iMbRZzIqoAxZ1RkNrjReHzarYrtLjU7TpF2d0xjm92yrmpdDrCnvokr5oneHEg//bAKB+LhBxJIuYnv/o9hHYdawSt7y1KjS0NivNjj//32Dc/s6asOeXuyDE98lmtShOlA7h/Zg4sgsKctwIBCRcOqgAfdtnwWa1YGXddPVaQzszFV0q9bfrDQONRG6X1okcQFgtiMim6s4Rv4tPXTEw9D6O7BEeeEeiqBmyW3Dl0EKM6NEahXnpePyzTWHbpzmtmhkO9fTyFosFw7u1UgQcPdpm4Ke6wEKsV9Ebdt3YsEuFqAX54DcjMHfyOYazIeJVobp/2iinyboMM9Qn/HSnXfF6sbR5yiX90DHXjT+O66cYpaNFnWEB6k9sN4/qhp75mbhoQPuw7TrkunVPmqI7zuuhGPkhX2mrJ9uSZbjsGNgxRzECY/0jF+GSgfVFxiK5DeJaOOp2OVXBCBCsxRjYMScUqGS67GGjisYNCr7mLWfXdznE0qVyWtfgsOVIM4waochwqAIOsfakdaYLax++MPR3pOwGoPx+yxnBLq0zYLVaNDNsTptV8zktFgvUXyd199pFA9oL29ffrh52/WTdzLqNjakMx7Rp0zBr1ixs2bIFbrcbI0eOxHPPPYc+ffokq31ElEBupw192oevm6HHJpxwY60TEA+6kVaOjYU/LMNhV8xTEEuA07l1OpbWTQe/+VAZZq05oDtE168xCZQc5DwiLMoWZbFbBDQCF5m4poncZSAWR2pN0S2OSJI/t1l3jsTXm45g3ubDoatkWaRMkNitZqYI95XrhuKZKwcppn+PJWOWnebAxscv1pyPQ2uFYD02VZeK+PpZqlEjeRlOLHnofPj8UsSsCRA5KNfq4gnWKUXvLgTCa1KuPq0Tpi/8CYDyMxa3++EPoxWz7TYmpv43Llq0CJMmTcLy5csxb948eL1eXHTRRaisbPx9R0RknngwjXW9DzFLYHbBrGjUJ+oMl11xYomlS0X06OUDcPfoXvjkrlGa908c2RVOmxX9CuoLXLVOylKEgEJNvXKxGEDJ3RDBOS2G4I/j+qFH2/BVbv9y9RAUtnLjL8IiZqd2zsOUsX01uzIiZQ/EONPMd8BqtYStNSM+3EwNR6bLHpYtiJZ5UBO/h26HTZEF0ipC7ZSXjq5tog89FYNom+qz11stenCnHNw4siuuOrUTMpw2PHCx9kW7erhyxzw3Zt52JsYNKsCVQ+tXxBUDjsYabAAmMxxfffWV4u8333wT+fn5WL16Nc4555yENoyIUk88ecea4Uh32JCX7sDJKi9+fVa36A8wQd2lkuGy4bgwSahWUacZOW4H7rtQf9r7wlbpWP/oRThQUo0xLywKvqZGN4w6ExNGEn9VbtteZ52fqyIswd67XRa+e/CCaC8VMqxrKzwxfoBi6KhM/NxtMXarhR4fZ0GvKC/dgV1mXlsIBtIcygxHtHlS9IwbXKAYFqvuAtEqogWCAeNjPwt2ezx31SDdwFj8/l5/Zme47Dac2b11WIG3kSn2G4O4WllaWgoAaNWqle42Ho8HHo8n9HdZmfEKaiJKLfFqPfYMhxWzJ42CJAVP0ImkLhp12qyKg3yia0a0BEc8RJ5sLFKXCaAMMgZ0yMGsNQdCf5/Xpy3+cGk/DE7QfAt62RZ5gTs1se4i3gAuEcOsZa1UlcU/Hxq+8JkofJRK/evrZSK03H5uD/xj0U94+sqBmDA8OJ3/rWd3w/biirBAoJMq4ND6PkbKwnmFLrvHfzZQd7uHLumD5TuPY+II7eUFGouYA45AIIDJkydj1KhRGDhQ/42YNm0aHn/88VhfhohSSJFOj+NkE2mFz3ioMxwWiwXpTjtW/H40bFZLzFkZs5SFg+Gv+eAlffHzV7/XzfCIu/GrM7ugxuvHWT3bAAju063CXA/xihb8qInnw3i+A4Dx4b9GPHhJHyzcWowrhnbE+X3ycX7f8AnERDZV0ajFYsE395+LWn/A0Kq/socu6RMarSP7w7j+mtuquzcizeiqxSvM+RHpve+Ul44ffj/aUCFyKsUccEyaNAkbN27EkiVLIm43depU3HfffaG/y8rKUFgYeUITImocxANYQ528zRCL/eSF0ACgnU43RLKIV65aB/1TO+eFzeGh+1x2Kyad3zOh7RO1y07DtiPhi9PpETMcsWa5tMSbferdLgtFj12MNIfV0IlWPUoFALpr1L9EY7Eo51mJxGa14OkrB+IPH29UvK5RZrp6GnuwAcQYcNx11134/PPPsXjxYnTqpN+PCAAulwsuV3zz/BNR6iXyZJMod1/QC1sOleOq0zrhlMLclLVDvFrXyyBECjZMJh3i8syVg/DQ/zYYzppYkxR0xroYoMhMxkA5SqXhZoSYMLxLKOAwuyrvlUM74osNh3BO7zbJaFqDMxVwSJKE3/72t/j444+xcOFCdOuW2AIwImq8GqAcwrS8DCfev+3MVDdDcfI022UR62NiVdgqHe/davw9S1ZiKxEBhxliPVIsk44lQrSMhbprJ81hwzu3DE9mkxqUqYBj0qRJeO+99/DJJ58gKysLhw8fBgDk5OTA7TaWYiKipklrYS4KEgv/GjJb0RDirdtQu3RQe+w8WonTu+kPNkgGMUFntpYiUaK97p3n98Smg2WKmYCbE1MBx/Tp0wEA5513nuL2GTNm4MYbb0xUm4ioEXnuqkH46WglzmjgE0RL0phjlETX7rw64TRIktTgNQfi0GSztRSJEu11c9yOZpXRUDPdpUJELcu1p3dOdROahGuGdcK2IxWxBWaN+Niqt85LPFJR4ChO0BXrvBvxSlVmpbFoGrOFEBE1cn/6v9iXCW+84UbyajgaWsdcNyaP6YXsNEfCu4mMSlVmpbFgwEFElGKNOMGBTJfxOSoau8lj9GeNbQjZ7pZ9ym3Ze09E1Ag05u7qiwe0w4X922Fo59xUN6XJeuDiPvho9X7cMzq1AU+qMeAgIkqxaEutpJLdZsXrNwxLdTOatEnn90zqZG5NRSMcWU9E1LL0aZ+V6iYQJR0zHEREKTaqZxs8f/UQBh7UrDHgICJqBCItN0/UHLBLhYiIiJKOAQcRERElHQMOIiIiSjoGHERERJR0DDiIiIgo6RhwEBERUdIx4CAiIqKkY8BBRERESceAg4iIiJKOAQcRERElHQMOIiIiSjoGHERERJR0DDiIiIgo6Rp8tVhJkgAAZWVlDf3SREREFCP5vC2fx81q8ICjvLwcAFBYWNjQL01ERERxKi8vR05OjunHWaRYQ5UYBQIBHDx4EFlZWbBYLAl73rKyMhQWFmLfvn3Izs5O2PM2Ni1lP4GWs6/cz+alpewn0HL2lfsZJEkSysvL0aFDB1it5isyGjzDYbVa0alTp6Q9f3Z2drP+Qshayn4CLWdfuZ/NS0vZT6Dl7Cv3EzFlNmQsGiUiIqKkY8BBRERESddsAg6Xy4VHH30ULpcr1U1Jqpayn0DL2VfuZ/PSUvYTaDn7yv1MjAYvGiUiIqKWp9lkOIiIiKjxYsBBRERESceAg4iIiJKOAQcRERElXbMJOP7+97+ja9euSEtLw/Dhw/HDDz+kukmmLF68GJdffjk6dOgAi8WC2bNnK+6XJAmPPPIICgoK4Ha7MWbMGGzfvl2xzYkTJzBhwgRkZ2cjNzcXv/71r1FRUdGAexHdtGnTcPrppyMrKwv5+fm44oorsHXrVsU2NTU1mDRpElq3bo3MzExcddVVOHLkiGKbvXv3Yty4cUhPT0d+fj4eeOAB+Hy+htyViKZPn47BgweHJtAZMWIEvvzyy9D9zWEftTz77LOwWCyYPHly6LbmsK+PPfYYLBaL4qdv376h+5vDPsoOHDiA66+/Hq1bt4bb7cagQYOwatWq0P3N5VjUtWvXsM/UYrFg0qRJAJrPZ+r3+/Hwww+jW7ducLvd6NGjB5588knFeigN9plKzcDMmTMlp9Mp/fvf/5Y2bdok3XrrrVJubq505MiRVDfNsDlz5kh/+MMfpFmzZkkApI8//lhx/7PPPivl5ORIs2fPltavXy/97Gc/k7p16yZVV1eHtrnkkkukIUOGSMuXL5e+++47qWfPntJ1113XwHsS2cUXXyzNmDFD2rhxo7Ru3Trp0ksvlTp37ixVVFSEtrn99tulwsJCacGCBdKqVaukM888Uxo5cmTofp/PJw0cOFAaM2aMtHbtWmnOnDlSmzZtpKlTp6ZilzR9+umn0hdffCFt27ZN2rp1q/T73/9ecjgc0saNGyVJah77qPbDDz9IXbt2lQYPHizdc889odubw74++uij0oABA6RDhw6Ffo4ePRq6vznsoyRJ0okTJ6QuXbpIN954o7RixQpp586d0ty5c6UdO3aEtmkux6Li4mLF5zlv3jwJgPTtt99KktR8PtOnn35aat26tfT5559Lu3btkj788EMpMzNTeumll0LbNNRn2iwCjjPOOEOaNGlS6G+/3y916NBBmjZtWgpbFTt1wBEIBKT27dtLf/7zn0O3lZSUSC6XS3r//fclSZKkzZs3SwCklStXhrb58ssvJYvFIh04cKDB2m5WcXGxBEBatGiRJEnB/XI4HNKHH34Y2ubHH3+UAEjLli2TJCkYnFmtVunw4cOhbaZPny5lZ2dLHo+nYXfAhLy8POlf//pXs9zH8vJyqVevXtK8efOkc889NxRwNJd9ffTRR6UhQ4Zo3tdc9lGSJOmhhx6SzjrrLN37m/Ox6J577pF69OghBQKBZvWZjhs3Trr55psVt/385z+XJkyYIElSw36mTb5Lpba2FqtXr8aYMWNCt1mtVowZMwbLli1LYcsSZ9euXTh8+LBiH3NycjB8+PDQPi5btgy5ubkYNmxYaJsxY8bAarVixYoVDd5mo0pLSwEArVq1AgCsXr0aXq9Xsa99+/ZF586dFfs6aNAgtGvXLrTNxRdfjLKyMmzatKkBW2+M3+/HzJkzUVlZiREjRjTLfZw0aRLGjRun2CegeX2e27dvR4cOHdC9e3dMmDABe/fuBdC89vHTTz/FsGHDcPXVVyM/Px9Dhw7F66+/Hrq/uR6Lamtr8c477+Dmm2+GxWJpVp/pyJEjsWDBAmzbtg0AsH79eixZsgRjx44F0LCfaYMv3pZox44dg9/vV3zoANCuXTts2bIlRa1KrMOHDwOA5j7K9x0+fBj5+fmK++12O1q1ahXaprEJBAKYPHkyRo0ahYEDBwII7ofT6URubq5iW/W+ar0X8n2NRVFREUaMGIGamhpkZmbi448/Rv/+/bFu3bpms48AMHPmTKxZswYrV64Mu6+5fJ7Dhw/Hm2++iT59+uDQoUN4/PHHcfbZZ2Pjxo3NZh8BYOfOnZg+fTruu+8+/P73v8fKlStx9913w+l0YuLEic32WDR79myUlJTgxhtvBNB8vrcAMGXKFJSVlaFv376w2Wzw+/14+umnMWHCBAANe35p8gEHNV2TJk3Cxo0bsWTJklQ3JSn69OmDdevWobS0FB999BEmTpyIRYsWpbpZCbVv3z7cc889mDdvHtLS0lLdnKSRrwYBYPDgwRg+fDi6dOmCDz74AG63O4UtS6xAIIBhw4bhmWeeAQAMHToUGzduxD/+8Q9MnDgxxa1LnjfeeANjx45Fhw4dUt2UhPvggw/w7rvv4r333sOAAQOwbt06TJ48GR06dGjwz7TJd6m0adMGNpstrHr4yJEjaN++fYpalVjyfkTax/bt26O4uFhxv8/nw4kTJxrl+3DXXXfh888/x7fffotOnTqFbm/fvj1qa2tRUlKi2F69r1rvhXxfY+F0OtGzZ0+cdtppmDZtGoYMGYKXXnqpWe3j6tWrUVxcjFNPPRV2ux12ux2LFi3Cyy+/DLvdjnbt2jWbfRXl5uaid+/e2LFjR7P6PAsKCtC/f3/Fbf369Qt1HzXHY9GePXswf/583HLLLaHbmtNn+sADD2DKlCn4xS9+gUGDBuFXv/oV7r33XkybNg1Aw36mTT7gcDqdOO2007BgwYLQbYFAAAsWLMCIESNS2LLE6datG9q3b6/Yx7KyMqxYsSK0jyNGjEBJSQlWr14d2uabb75BIBDA8OHDG7zNeiRJwl133YWPP/4Y33zzDbp166a4/7TTToPD4VDs69atW7F3717FvhYVFSn+A8ybNw/Z2dlhB8vGJBAIwOPxNKt9HD16NIqKirBu3brQz7BhwzBhwoTQ781lX0UVFRX46aefUFBQ0Kw+z1GjRoUNU9+2bRu6dOkCoHkdi2QzZsxAfn4+xo0bF7qtOX2mVVVVsFqVp3qbzYZAIACggT/TOIpfG42ZM2dKLpdLevPNN6XNmzdLt912m5Sbm6uoHm7sysvLpbVr10pr166VAEgvvPCCtHbtWmnPnj2SJAWHLeXm5kqffPKJtGHDBmn8+PGaw5aGDh0qrVixQlqyZInUq1evRjcU7Y477pBycnKkhQsXKoakVVVVhba5/fbbpc6dO0vffPONtGrVKmnEiBHSiBEjQvfLw9Euuugiad26ddJXX30ltW3btlENR5syZYq0aNEiadeuXdKGDRukKVOmSBaLRfr6668lSWoe+6hHHKUiSc1jX++//35p4cKF0q5du6SlS5dKY8aMkdq0aSMVFxdLktQ89lGSgkOb7Xa79PTTT0vbt2+X3n33XSk9PV165513Qts0l2ORJAVHNHbu3Fl66KGHwu5rLp/pxIkTpY4dO4aGxc6aNUtq06aN9OCDD4a2aajPtFkEHJIkSa+88orUuXNnyel0SmeccYa0fPnyVDfJlG+//VYCEPYzceJESZKCQ5cefvhhqV27dpLL5ZJGjx4tbd26VfEcx48fl6677jopMzNTys7Olm666SapvLw8BXujT2sfAUgzZswIbVNdXS3deeedUl5enpSeni5deeWV0qFDhxTPs3v3bmns2LGS2+2W2rRpI91///2S1+tt4L3Rd/PNN0tdunSRnE6n1LZtW2n06NGhYEOSmsc+6lEHHM1hX6+99lqpoKBAcjqdUseOHaVrr71WMTdFc9hH2WeffSYNHDhQcrlcUt++faXXXntNcX9zORZJkiTNnTtXAhDWfklqPp9pWVmZdM8990idO3eW0tLSpO7du0t/+MMfFEN3G+oz5fL0RERElHRNvoaDiIiIGj8GHERERJR0DDiIiIgo6RhwEBERUdIx4CAiIqKkY8BBRERESceAg4iIiJKOAQcRERElHQMOIiIiSjoGHERERJR0DDiIiIgo6RhwEBERUdL9P2/UE/9cCEr/AAAAAElFTkSuQmCC"},"metadata":{}},{"name":"stderr","text":"Epoch 1/100:  69%|██████▉   | 782/1131 [01:49<00:49,  7.12it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[128], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# visualizing training process\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     clear_output(\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\n# trained_model = best_model\ntrained_model = ft_emb_lstm_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:23.144558Z","iopub.execute_input":"2024-08-10T07:08:23.145586Z","iopub.status.idle":"2024-08-10T07:08:23.155948Z","shell.execute_reply.started":"2024-08-10T07:08:23.145549Z","shell.execute_reply":"2024-08-10T07:08:23.154502Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:26.910228Z","iopub.execute_input":"2024-08-10T07:08:26.911398Z","iopub.status.idle":"2024-08-10T07:08:26.947659Z","shell.execute_reply.started":"2024-08-10T07:08:26.911332Z","shell.execute_reply":"2024-08-10T07:08:26.946626Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:28.166314Z","iopub.execute_input":"2024-08-10T07:08:28.167237Z","iopub.status.idle":"2024-08-10T07:08:28.172647Z","shell.execute_reply.started":"2024-08-10T07:08:28.167195Z","shell.execute_reply":"2024-08-10T07:08:28.171464Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"code","source":"def generate_sample(ft_emb_torch_rnn, seed_phrase=' ',temperature=1.0, quotes_train_dataset=quotes_train_dataset):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    x_sequence = prepare_sent(seed_phrase).to(ft_emb_torch_rnn.device)\n    \n    # Initialize the hidden state\n    hid_state = ft_emb_torch_rnn.initial_state(batch_size=1)\n    \n    # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n    if seed_phrase.strip() != '':\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(quotes_train_dataset.max_len - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, hid_state = ft_emb_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(quotes_train_dataset.token_to_id), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(quotes_train_dataset.id_to_token[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(ft_emb_torch_rnn.device)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:45:06.019006Z","iopub.execute_input":"2024-08-08T12:45:06.019446Z","iopub.status.idle":"2024-08-08T12:45:06.031247Z","shell.execute_reply.started":"2024-08-08T12:45:06.019412Z","shell.execute_reply":"2024-08-08T12:45:06.030037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in np.linspace(1, 1.5, 10):\n    print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:45:07.868053Z","iopub.execute_input":"2024-08-08T12:45:07.869036Z","iopub.status.idle":"2024-08-08T12:45:10.418905Z","shell.execute_reply.started":"2024-08-08T12:45:07.868997Z","shell.execute_reply":"2024-08-08T12:45:10.417911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}