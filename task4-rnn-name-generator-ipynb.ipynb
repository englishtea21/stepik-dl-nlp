{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Генерация текста с помощью RNN\n","metadata":{}},{"cell_type":"markdown","source":"(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/englishtea21/stepik-dl-nlp.git\n# !pip install -r stepik-dl-nlp/requirements.txt\nimport sys;","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T05:19:15.119964Z","iopub.execute_input":"2024-08-10T05:19:15.120349Z","iopub.status.idle":"2024-08-10T05:19:30.542711Z","shell.execute_reply.started":"2024-08-10T05:19:15.120317Z","shell.execute_reply":"2024-08-10T05:19:30.541818Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'stepik-dl-nlp'...\nremote: Enumerating objects: 455, done.\u001b[K\nremote: Counting objects: 100% (161/161), done.\u001b[K\nremote: Compressing objects: 100% (102/102), done.\u001b[K\nremote: Total 455 (delta 90), reused 121 (delta 58), pack-reused 294\u001b[K\nReceiving objects: 100% (455/455), 166.52 MiB | 28.50 MiB/s, done.\nResolving deltas: 100% (224/224), done.\nUpdating files: 100% (69/69), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/stepik-dl-nlp","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:19:34.955775Z","iopub.execute_input":"2024-08-10T05:19:34.956174Z","iopub.status.idle":"2024-08-10T05:19:34.963436Z","shell.execute_reply.started":"2024-08-10T05:19:34.956141Z","shell.execute_reply":"2024-08-10T05:19:34.962522Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/stepik-dl-nlp\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:19:36.232701Z","iopub.execute_input":"2024-08-10T05:19:36.233396Z","iopub.status.idle":"2024-08-10T05:19:36.245276Z","shell.execute_reply.started":"2024-08-10T05:19:36.233361Z","shell.execute_reply":"2024-08-10T05:19:36.244439Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:19:37.843594Z","iopub.execute_input":"2024-08-10T05:19:37.844078Z","iopub.status.idle":"2024-08-10T05:19:40.011279Z","shell.execute_reply.started":"2024-08-10T05:19:37.844044Z","shell.execute_reply":"2024-08-10T05:19:40.010091Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"englishtea21@mail.ru\"\n!git config --global user.name \"englishtea21\"","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:19:40.013285Z","iopub.execute_input":"2024-08-10T05:19:40.013610Z","iopub.status.idle":"2024-08-10T05:19:42.049302Z","shell.execute_reply.started":"2024-08-10T05:19:40.013583Z","shell.execute_reply":"2024-08-10T05:19:42.048167Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:20:34.854793Z","start_time":"2019-11-05T18:20:34.372865Z"},"execution":{"iopub.status.busy":"2024-08-10T05:19:42.321332Z","iopub.execute_input":"2024-08-10T05:19:42.321662Z","iopub.status.idle":"2024-08-10T05:19:42.328086Z","shell.execute_reply.started":"2024-08-10T05:19:42.321635Z","shell.execute_reply":"2024-08-10T05:19:42.327142Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Данные\nДатасет содержит ~9k имен, все написаны латиницей.","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/russian_names.txt') as input_file:\n    names = input_file.read()[:-1].split('\\n')","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.509714Z","start_time":"2019-11-05T18:21:03.491489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.946758Z","start_time":"2019-11-05T18:21:03.938432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на распределение длин имен:","metadata":{}},{"cell_type":"markdown","source":"# Препроцессинг","metadata":{}},{"cell_type":"code","source":"len(list(filter(str.isalpha, names))), len(names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nonly_latin_letters=re.compile(r'[A-Za-z]+')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_tmp = list(filter(only_latin_letters.fullmatch, names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(names_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names=names_tmp\ndel names_tmp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Name length distribution')\nplt.hist(list(map(len, names)), bins=25);","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:05.420060Z","start_time":"2019-11-05T18:21:05.179513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = [' ' + line for line in names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all unique characters go here\ntokens = list(set(''.join(names)))\n\nnum_tokens = len(tokens)\nprint ('num_tokens = ', num_tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.335188Z","start_time":"2019-11-05T18:21:07.320148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Мы специально добавляем пробел к нашим именам в тренировочных данных. Зачем? <br>\nEсли не будет специального символа, с которого начинается генерация, то мы лишим нашу модель способности выбирать первый символ последовательности","metadata":{}},{"cell_type":"markdown","source":"После того, как мы обучим нашу нейронную сеть, мы сможем генерировать имена, которые соответствуют некоторым условиям — например, имена, которые начинаются на букву \"a\" или на буквы \"abc\", или какие-либо другие условия. Если же мы захотим генерировать любые имена, начинающиеся с любой буквы, мы просто передадим нашей функции пробел в качестве первого символа. Таким образом, сможем сгенерировать имена, начинающиеся на любую букву. Отлично! С этой небольшой хитростью в коде разобрались.","metadata":{}},{"cell_type":"markdown","source":"### Символы -> id\n\nСоздадим словарь < символ > -> < id >","metadata":{}},{"cell_type":"code","source":"token_to_id = {token: idx for idx, token in enumerate(tokens)}","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.674548Z","start_time":"2019-11-05T18:21:07.671129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_to_id.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n\nfor i in range(num_tokens):\n    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n\nprint(\"Seems alright!\")","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.838814Z","start_time":"2019-11-05T18:21:07.833611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Отлично! И теперь мы хотим преобразовать наши входные данные, а именно — наши 9 с небольшим хвостиком тысяч имён в некоторое численное представление, то есть вместо имени мы хотим получить численный вектор. Сделать это мы можем с помощью функций \"to_matrix\", которая будет преобразовывать наше имя из буквенного, человеко-читаемого формата в формат \"вектор с числами\".","metadata":{}},{"cell_type":"code","source":"def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n    max_len = max_len or max(map(len, data))\n    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n    for i in range(len(data)):\n        line_ix = [token_to_id[c] for c in data[i]]\n        data_ix[i, :len(line_ix)] = line_ix\n        \n    if not batch_first: # convert [batch, time] into [time, batch]\n        data_ix = np.transpose(data_ix)\n\n    return data_ix","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.988093Z","start_time":"2019-11-05T18:21:07.977722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Example: cast 4 names to matrices, pad with zeros\nprint('\\n'.join(names[::2000]))\nprint(to_matrix(names[::2000], token_to_id))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:08.136936Z","start_time":"2019-11-05T18:21:08.131609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"22 - id пробела, этот токен встречается вначале (по умолчанию как знак начала), так и может быть в конце - паддинг до равной длины строк в матрице","metadata":{}},{"cell_type":"markdown","source":"# Рекуррентные нейронные сети\n\n<img src=\"img/rnn.png\" width=480>","metadata":{}},{"cell_type":"code","source":"import torch, torch.nn as nn\nimport torch.nn.functional as F\n# from torch.autograd import Variable","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.739438Z","start_time":"2019-11-05T18:21:09.661222Z"},"execution":{"iopub.status.busy":"2024-08-10T05:19:49.192368Z","iopub.execute_input":"2024-08-10T05:19:49.193080Z","iopub.status.idle":"2024-08-10T05:19:55.665579Z","shell.execute_reply.started":"2024-08-10T05:19:49.193041Z","shell.execute_reply":"2024-08-10T05:19:55.664618Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CharRNNCell(nn.Module):\n    \"\"\"\n    Implement the scheme above as torch module\n    \"\"\"\n    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n        super(self.__class__,self).__init__()\n        self.num_units = rnn_num_units\n        \n        self.embedding = nn.Embedding(num_tokens, embedding_size)\n        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward(self, x, h_prev):\n        \"\"\"\n        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n        We'll call it repeatedly to produce the whole sequence.\n        \n        :param x: batch of character ids, variable containing vector of int64\n        :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n        \"\"\"\n        # get vector embedding of x\n        x_emb = self.embedding(x)\n        \n        # compute next hidden state using self.rnn_update\n        x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n        h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n        \n        h_next = F.tanh(h_next)\n        \n        assert h_next.size() == h_prev.size()\n        \n        #compute logits for next character probs\n        logits = self.rnn_to_logits(h_next)\n        \n        return h_next, F.log_softmax(logits, -1)\n    \n    def initial_state(self, batch_size):\n        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n        return torch.zeros(batch_size, self.num_units)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.751862Z","start_time":"2019-11-05T18:21:10.741772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_rnn = CharRNNCell()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.071002Z","start_time":"2019-11-05T18:21:11.052377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети, RNN loop","metadata":{}},{"cell_type":"code","source":"def rnn_loop(rnn, batch_index):\n    \"\"\"\n    Computes log P(next_character) for all time-steps in names_ix\n    :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n    \"\"\"\n    batch_size, max_length = batch_index.size()\n    hid_state = rnn.initial_state(batch_size)\n    logprobs = []\n\n    for x_t in batch_index.transpose(0,1):\n        hid_state, logp_next = rnn(x_t, hid_state)  \n        logprobs.append(logp_next)\n        \n    return torch.stack(logprobs, dim=1)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.521078Z","start_time":"2019-11-05T18:21:11.510175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom random import sample\n\nchar_rnn = CharRNNCell()\nopt = torch.optim.Adam(char_rnn.parameters())\ncriterion = nn.NLLLoss()\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:12.120106Z","start_time":"2019-11-05T18:21:12.109585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\nfor i in range(1000):\n\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = rnn_loop(char_rnn, batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    # visualizing training process\n    history.append(loss.data.numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history,label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.521061Z","start_time":"2019-11-05T18:21:12.302892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RNN: генерация имен","metadata":{}},{"cell_type":"code","source":"def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    hid_state = char_rnn.initial_state(batch_size=1)\n    \n    #feed the seed phrase, if any\n    for i in range(len(seed_phrase) - 1):\n        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n    \n    #start generating\n    for _ in range(max_length - len(seed_phrase)):\n        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n        \n        # sample next token and push it back into x_sequence\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n        \n    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.540765Z","start_time":"2019-11-05T18:21:23.524503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.625562Z","start_time":"2019-11-05T18:21:23.544968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_stripped = [name.strip() for name in names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_stripped[::2000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[matched for el in sampled if el.strip() in names_stripped]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Ar'))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.702249Z","start_time":"2019-11-05T18:21:23.629226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[matched for el in sampled if el.strip() in names_stripped]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Как видно, наша модель именно генерирует новые имена, а не вспоминает запоменнные","metadata":{}},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Koval'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"При маленькой температуре сеть генерирует фамилии, в которых она наиболее уверена <br>\nПри большой - очень разнообразные фамилии","metadata":{}},{"cell_type":"code","source":"sampled=[]\nfor _ in range(10):\n    sampled.append(generate_sample(char_rnn, seed_phrase=' Podo', temperature=1.2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Более простое решение\n\n* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n\nКроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n\nПерепишем наш пример с генерацией имен с помощью средств PyTorch.","metadata":{}},{"cell_type":"code","source":"class CharRNNLoop(nn.Module):\n    def __init__(self, num_tokens=num_tokens, emb_size=32, rnn_num_units=64):\n        super(self.__class__, self).__init__()\n        self.num_units = rnn_num_units\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        return torch.zeros(1, batch_size, self.num_units) \n    \nmodel = CharRNNLoop()\nopt = torch.optim.Adam(model.parameters())\ncriterion = nn.NLLLoss()\nhistory = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.713285Z","start_time":"2019-11-05T18:21:23.704755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=model.to('cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the model applies over the whole sequence\nbatch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n# batch_ix = torch.LongTensor(batch_ix).to('cuda')\nbatch_ix = torch.LongTensor(batch_ix)\n\nlogp_seq = model(batch_ix)\n\n# compute loss\nloss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, names))\n\n\nfor i in range(1000):\n    batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n#     batch_ix = torch.tensor(batch_ix, dtype=torch.int64).to('cuda')\n    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n    logp_seq = model(batch_ix)\n    \n    # compute loss\n    predictions_logp = logp_seq[:, :-1]\n    actual_next_tokens = batch_ix[:, 1:]\n\n    loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n    # train with backprop\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    \n    history.append(loss.data.cpu().numpy())\n    if (i + 1) % 100 == 0:\n        clear_output(True)\n        plt.plot(history, label='loss')\n        plt.legend()\n        plt.show()\n\nassert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    if seed_phrase!=' ':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model=model.cpu()\nfor _ in range(10):\n    print(generate_sample(model,seed_phrase='Stryk', temperature=1))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.526436Z","start_time":"2019-11-05T18:21:31.469965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Домашнее задание: мотивационные лозунги","metadata":{}},{"cell_type":"markdown","source":"Возможно стоит учить эмбеддинги n-грамм","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/author_quotes.txt') as input_file:\n    quotes = input_file.read()[:-1].split('\\n')\n    quotes = [' ' + line for line in quotes]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.570320Z","start_time":"2019-11-05T18:21:31.528673Z"},"execution":{"iopub.status.busy":"2024-08-10T05:58:34.275360Z","iopub.execute_input":"2024-08-10T05:58:34.275748Z","iopub.status.idle":"2024-08-10T05:58:34.328824Z","shell.execute_reply.started":"2024-08-10T05:58:34.275719Z","shell.execute_reply":"2024-08-10T05:58:34.327867Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"quotes[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.575286Z","start_time":"2019-11-05T18:21:31.571798Z"},"execution":{"iopub.status.busy":"2024-08-08T14:09:26.722427Z","iopub.execute_input":"2024-08-08T14:09:26.722800Z","iopub.status.idle":"2024-08-08T14:09:26.729542Z","shell.execute_reply.started":"2024-08-08T14:09:26.722771Z","shell.execute_reply":"2024-08-08T14:09:26.728417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = list(set(''.join(quotes)))\ntoken_to_id = {token: idx for idx, token in enumerate(tokens)}\nnum_tokens = len(tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.653673Z","start_time":"2019-11-05T18:21:31.578424Z"},"execution":{"iopub.status.busy":"2024-08-08T13:36:28.263267Z","iopub.execute_input":"2024-08-08T13:36:28.263925Z","iopub.status.idle":"2024-08-08T13:36:28.336508Z","shell.execute_reply.started":"2024-08-08T13:36:28.263890Z","shell.execute_reply":"2024-08-08T13:36:28.335644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = max(map(len, quotes))\nMAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2024-08-08T13:36:29.301620Z","iopub.execute_input":"2024-08-08T13:36:29.302015Z","iopub.status.idle":"2024-08-08T13:36:29.310273Z","shell.execute_reply.started":"2024-08-08T13:36:29.301983Z","shell.execute_reply":"2024-08-08T13:36:29.309273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens, \nnum_tokens","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:10.090559Z","iopub.execute_input":"2024-08-08T12:34:10.090962Z","iopub.status.idle":"2024-08-08T12:34:10.097270Z","shell.execute_reply.started":"2024-08-08T12:34:10.090931Z","shell.execute_reply":"2024-08-08T12:34:10.096258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:10.939322Z","iopub.execute_input":"2024-08-08T12:34:10.939680Z","iopub.status.idle":"2024-08-08T12:34:10.999455Z","shell.execute_reply.started":"2024-08-08T12:34:10.939652Z","shell.execute_reply":"2024-08-08T12:34:10.998273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CharRNNLoop(nn.Module):\n    def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n        super(self.__class__, self).__init__()\n        self.device=device\n        self.num_layers=num_layers\n        self.hidden_size = hidden_size\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n        self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n#         return torch.zeros(1, batch_size, self.hidden_size, device=self.device) \n        return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n    \nquotes_baseline_model = CharRNNLoop(num_layers=4)\nopt = torch.optim.Adam(quotes_baseline_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=1)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:16.485102Z","iopub.execute_input":"2024-08-08T12:34:16.485797Z","iopub.status.idle":"2024-08-08T12:34:17.928856Z","shell.execute_reply.started":"2024-08-08T12:34:16.485764Z","shell.execute_reply":"2024-08-08T12:34:17.927459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nquotes_baseline_model = quotes_baseline_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    quotes_baseline_model = nn.DataParallel(quotes_baseline_model)\n    \nquotes_baseline_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:18.847974Z","iopub.execute_input":"2024-08-08T12:34:18.848506Z","iopub.status.idle":"2024-08-08T12:34:19.164620Z","shell.execute_reply.started":"2024-08-08T12:34:18.848473Z","shell.execute_reply":"2024-08-08T12:34:19.163603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport copy","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:21.664325Z","iopub.execute_input":"2024-08-08T12:34:21.665033Z","iopub.status.idle":"2024-08-08T12:34:21.669710Z","shell.execute_reply.started":"2024-08-08T12:34:21.664998Z","shell.execute_reply":"2024-08-08T12:34:21.668605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n    max_len = max_len or max(map(len, data))\n    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n    for i in range(len(data)):\n        line_ix = [token_to_id[c] for c in data[i]]\n        data_ix[i, :len(line_ix)] = line_ix\n        \n    if not batch_first: # convert [batch, time] into [time, batch]\n        data_ix = np.transpose(data_ix)\n\n    return data_ix","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:22.522131Z","iopub.execute_input":"2024-08-08T12:34:22.522508Z","iopub.status.idle":"2024-08-08T12:34:22.530354Z","shell.execute_reply.started":"2024-08-08T12:34:22.522479Z","shell.execute_reply":"2024-08-08T12:34:22.529213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QuotesDataset(Dataset):\n    def __init__(self, quotes_list: list[str], token_to_id: dict, max_len: int):\n        self.quotes_list=copy.deepcopy(quotes_list)\n        self.token_to_id=copy.deepcopy(token_to_id)\n        self.max_len=max_len\n        self.quotes_tensors=torch.LongTensor(to_matrix(self.quotes_list, self.token_to_id, self.max_len)) \n\n    def __len__(self):\n        return len(self.quotes_list)\n\n    def __getitem__(self, idx):\n        return self.quotes_tensors[idx, :]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:23.649784Z","iopub.execute_input":"2024-08-08T12:34:23.650567Z","iopub.status.idle":"2024-08-08T12:34:23.657867Z","shell.execute_reply.started":"2024-08-08T12:34:23.650531Z","shell.execute_reply":"2024-08-08T12:34:23.656659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train = QuotesDataset(quotes, token_to_id, MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:24.583396Z","iopub.execute_input":"2024-08-08T12:34:24.583792Z","iopub.status.idle":"2024-08-08T12:34:25.552001Z","shell.execute_reply.started":"2024-08-08T12:34:24.583755Z","shell.execute_reply":"2024-08-08T12:34:25.550940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(quotes)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:09.808431Z","iopub.execute_input":"2024-08-08T10:48:09.808753Z","iopub.status.idle":"2024-08-08T10:48:09.817348Z","shell.execute_reply.started":"2024-08-08T10:48:09.808726Z","shell.execute_reply":"2024-08-08T10:48:09.816352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:10.242266Z","iopub.execute_input":"2024-08-08T10:48:10.242613Z","iopub.status.idle":"2024-08-08T10:48:10.247010Z","shell.execute_reply.started":"2024-08-08T10:48:10.242585Z","shell.execute_reply":"2024-08-08T10:48:10.246105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataloader = DataLoader(\n    quotes_train, \n    batch_size=128, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:11.216910Z","iopub.execute_input":"2024-08-08T10:48:11.217293Z","iopub.status.idle":"2024-08-08T10:48:11.222631Z","shell.execute_reply.started":"2024-08-08T10:48:11.217265Z","shell.execute_reply":"2024-08-08T10:48:11.221497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix=batch_ix.to(device)\nlogp_seq = quotes_baseline_model(batch_ix)\n\n# compute loss\nloss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:30.068634Z","iopub.execute_input":"2024-08-08T12:34:30.069288Z","iopub.status.idle":"2024-08-08T12:34:30.073897Z","shell.execute_reply.started":"2024-08-08T12:34:30.069255Z","shell.execute_reply":"2024-08-08T12:34:30.072810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:30.646491Z","iopub.execute_input":"2024-08-08T12:34:30.647300Z","iopub.status.idle":"2024-08-08T12:34:30.651955Z","shell.execute_reply.started":"2024-08-08T12:34:30.647266Z","shell.execute_reply":"2024-08-08T12:34:30.650714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=20\nfor epoch in range(num_epochs):\n    quotes_baseline_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = quotes_baseline_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 25 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n\n     # Validate the model and calculate the metric\n    quotes_baseline_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch_ix.to(device)\n\n            logp_seq = quotes_baseline_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    print(f'Val loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n    \n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\ntrained_model = quotes_baseline_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id[token] for token in seed_phrase]\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    if seed_phrase!=' ':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(tokens), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:21.958473Z","iopub.execute_input":"2024-08-08T10:48:21.959217Z","iopub.status.idle":"2024-08-08T10:48:21.969597Z","shell.execute_reply.started":"2024-08-08T10:48:21.959184Z","shell.execute_reply":"2024-08-08T10:48:21.968545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_sample(trained_model, seed_phrase='Life is ', temperature=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Наша посимвольная модель страдает фигней... <br>\nПопробуем вместо эмбеддинга символов использовать эмбеддинги FastText <br>\nТакже вместо rnn воспользуемся lstm или gru","metadata":{}},{"cell_type":"markdown","source":"## LSTM-based посимвольная модель","metadata":{}},{"cell_type":"code","source":"class CharLSTMLoop(nn.Module):\n    def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n        super(self.__class__, self).__init__()\n        self.device=device\n        self.num_layers=num_layers\n        self.hidden_size = hidden_size\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state=self.initial_state(x.shape[0])\n        \n        h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        return next_logits\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        # Initialize both hidden state (h_0) and cell state (c_0)\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        return (h_0, c_0)\n    \nquotes_char_lstm_model = CharLSTMLoop(emb_size=128, hidden_size=128, num_layers=2)\nopt = torch.optim.Adam(quotes_char_lstm_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:35.392888Z","iopub.execute_input":"2024-08-08T12:34:35.393614Z","iopub.status.idle":"2024-08-08T12:34:35.413283Z","shell.execute_reply.started":"2024-08-08T12:34:35.393571Z","shell.execute_reply":"2024-08-08T12:34:35.412320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nquotes_char_lstm_model = quotes_char_lstm_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    quotes_char_lstm_model = nn.DataParallel(quotes_char_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:36.526797Z","iopub.execute_input":"2024-08-08T12:34:36.527740Z","iopub.status.idle":"2024-08-08T12:34:36.536427Z","shell.execute_reply.started":"2024-08-08T12:34:36.527703Z","shell.execute_reply":"2024-08-08T12:34:36.535250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataloader = DataLoader(\n    quotes_train, \n    batch_size=256, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:34:37.356553Z","iopub.execute_input":"2024-08-08T12:34:37.357255Z","iopub.status.idle":"2024-08-08T12:34:37.780171Z","shell.execute_reply.started":"2024-08-08T12:34:37.357224Z","shell.execute_reply":"2024-08-08T12:34:37.778871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_ix=batch_ix.to(device)\nlogp_seq = quotes_char_lstm_model(batch_ix)\n\n# compute loss\nloss = criterion(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n                  batch_ix[:, :-1].contiguous().view(-1))\n\nloss.backward()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:33.778697Z","iopub.execute_input":"2024-08-08T10:48:33.779413Z","iopub.status.idle":"2024-08-08T10:48:33.783736Z","shell.execute_reply.started":"2024-08-08T10:48:33.779379Z","shell.execute_reply":"2024-08-08T10:48:33.782634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:34.293087Z","iopub.execute_input":"2024-08-08T10:48:34.293875Z","iopub.status.idle":"2024-08-08T10:48:34.297778Z","shell.execute_reply.started":"2024-08-08T10:48:34.293846Z","shell.execute_reply":"2024-08-08T10:48:34.296758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbest_model=None\nbest_loss=float('inf')\nfor epoch in range(num_epochs):\n    quotes_char_lstm_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = quotes_char_lstm_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 25 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n    \n    # Validate the model and calculate the metric\n    quotes_char_lstm_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch.to(device)\n\n            logp_seq = quotes_char_lstm_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    \n    if val_loss<best_loss:\n        print(f'Новый лучший лосс: {val_loss}')\n        best_loss=val_loss\n        best_model=copy.deepcopy(quotes_char_lstm_model)\n    \n    print(f'Текущий loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n\n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\ntrained_model = best_model\n# train_model = quotes_char_lstm_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n    x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n    x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    \n    # Initialize the hidden state\n    hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n    # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n    if seed_phrase.strip() != '':\n        _, hid_state = char_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(max_length - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, hid_state = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(token_to_id), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(tokens[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(char_torch_rnn.device)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T10:48:38.423069Z","iopub.execute_input":"2024-08-08T10:48:38.423730Z","iopub.status.idle":"2024-08-08T10:48:38.434648Z","shell.execute_reply.started":"2024-08-08T10:48:38.423678Z","shell.execute_reply":"2024-08-08T10:48:38.433723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in np.linspace(1, 1.5, 10):\n    print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Получается туфта, возьмем эмбеддинги fasttext","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:35:36.217066Z","iopub.execute_input":"2024-08-10T05:35:36.217827Z","iopub.status.idle":"2024-08-10T05:35:36.260767Z","shell.execute_reply.started":"2024-08-10T05:35:36.217793Z","shell.execute_reply":"2024-08-10T05:35:36.259889Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:20:19.278410Z","iopub.execute_input":"2024-08-10T05:20:19.279290Z","iopub.status.idle":"2024-08-10T05:20:36.313773Z","shell.execute_reply.started":"2024-08-10T05:20:19.279252Z","shell.execute_reply":"2024-08-10T05:20:36.312350Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"--2024-08-10 05:20:20--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.11, 3.162.163.19, 3.162.163.34, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.11|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: 'cc.en.300.bin.gz'\n\ncc.en.300.bin.gz    100%[===================>]   4.19G   278MB/s    in 16s     \n\n2024-08-10 05:20:36 (271 MB/s) - 'cc.en.300.bin.gz' saved [4503593528/4503593528]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!gunzip cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:20:39.370573Z","iopub.execute_input":"2024-08-10T05:20:39.371499Z","iopub.status.idle":"2024-08-10T05:21:38.901826Z","shell.execute_reply.started":"2024-08-10T05:20:39.371447Z","shell.execute_reply":"2024-08-10T05:21:38.900521Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ft = fasttext.load_model('cc.en.300.bin')\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:35:58.821646Z","iopub.execute_input":"2024-08-10T05:35:58.822350Z","iopub.status.idle":"2024-08-10T05:36:30.277050Z","shell.execute_reply.started":"2024-08-10T05:35:58.822318Z","shell.execute_reply":"2024-08-10T05:36:30.276129Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"markdown","source":"Понизим размерность","metadata":{}},{"cell_type":"code","source":"EMB_SIZE=100","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:36:33.925314Z","iopub.execute_input":"2024-08-10T05:36:33.926162Z","iopub.status.idle":"2024-08-10T05:36:33.929986Z","shell.execute_reply.started":"2024-08-10T05:36:33.926126Z","shell.execute_reply":"2024-08-10T05:36:33.929102Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"fasttext.util.reduce_model(ft, EMB_SIZE)\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:36:35.733392Z","iopub.execute_input":"2024-08-10T05:36:35.733816Z","iopub.status.idle":"2024-08-10T05:36:57.083802Z","shell.execute_reply.started":"2024-08-10T05:36:35.733784Z","shell.execute_reply":"2024-08-10T05:36:57.082846Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T05:42:07.967546Z","iopub.execute_input":"2024-08-10T05:42:07.967933Z","iopub.status.idle":"2024-08-10T05:42:08.105778Z","shell.execute_reply.started":"2024-08-10T05:42:07.967902Z","shell.execute_reply":"2024-08-10T05:42:08.104979Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Мы будем подавать батчи в модель уже после того, как преобразуем каждое слово в батче с помощью fasttext","metadata":{}},{"cell_type":"markdown","source":"Будем нашей моделью предсказывать следующий токен: n-грамму fasttext","metadata":{}},{"cell_type":"markdown","source":"Будем использовать n-граммы различных размеров","metadata":{}},{"cell_type":"code","source":"utils_tokens={'<PAD>': 0}\n\ndef extract_ngrams(text, n):\n    # Function to extract n-grams from text\n    ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n    return ngrams\n\ndef build_ngram_vocab(texts, ns: List[int], utils_tokens=utils_tokens):\n    ngram_to_index = utils_tokens.copy()\n    index = len(ngram_to_index)\n    for n in ns:\n        for text in texts:\n            ngrams = extract_ngrams(text, n)\n            for ngram in ngrams:\n                if ngram not in ngram_to_index:\n                    ngram_to_index[ngram] = index\n                    index += 1\n    return ngram_to_index","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:49:46.594497Z","iopub.execute_input":"2024-08-10T06:49:46.595263Z","iopub.status.idle":"2024-08-10T06:49:46.602054Z","shell.execute_reply.started":"2024-08-10T06:49:46.595226Z","shell.execute_reply":"2024-08-10T06:49:46.601093Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"# Sample text data\n# texts = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\n\n# Build vocabulary for bigrams (2-grams)\nNGRAMS=[2, 3]\nNGRAM_VOCAB = build_ngram_vocab(quotes, NGRAMS)\nNUM_NGRAMS = len(NGRAM_VOCAB)\nNUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:59:07.475105Z","iopub.execute_input":"2024-08-10T06:59:07.475951Z","iopub.status.idle":"2024-08-10T06:59:10.057243Z","shell.execute_reply.started":"2024-08-10T06:59:07.475915Z","shell.execute_reply":"2024-08-10T06:59:10.056297Z"},"trusted":true},"execution_count":225,"outputs":[{"execution_count":225,"output_type":"execute_result","data":{"text/plain":"17923"},"metadata":{}}]},{"cell_type":"code","source":"# Create an embedding matrix for n-grams\ndef create_ngram_embedding_matrix(ngram_vocab, ft):\n    emb_size = ft.get_dimension()\n    embedding_matrix = torch.zeros((len(ngram_vocab), emb_size))\n    for ngram, idx in ngram_vocab.items():\n        embedding_matrix[idx] = torch.tensor(ft.get_word_vector(ngram))\n    return embedding_matrix\n\n# Generate embedding matrix\nEMBEDDING_MATRIX = create_ngram_embedding_matrix(NGRAM_VOCAB, ft)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:59:11.298310Z","iopub.execute_input":"2024-08-10T06:59:11.298677Z","iopub.status.idle":"2024-08-10T06:59:11.760787Z","shell.execute_reply.started":"2024-08-10T06:59:11.298649Z","shell.execute_reply":"2024-08-10T06:59:11.759805Z"},"trusted":true},"execution_count":226,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T06:59:12.213225Z","iopub.execute_input":"2024-08-10T06:59:12.213583Z","iopub.status.idle":"2024-08-10T06:59:12.219725Z","shell.execute_reply.started":"2024-08-10T06:59:12.213553Z","shell.execute_reply":"2024-08-10T06:59:12.218775Z"},"trusted":true},"execution_count":227,"outputs":[{"execution_count":227,"output_type":"execute_result","data":{"text/plain":"torch.Size([17923, 100])"},"metadata":{}}]},{"cell_type":"markdown","source":"Каждое предложение предсатвляется всеми возможными n-граммами, разные по размеру n-граммы отделяются специальным токеном \\<NEW_NGRAM\\>","metadata":{}},{"cell_type":"markdown","source":"Мы будем каждый раз разбивать предложение на n-граммы рандомно","metadata":{}},{"cell_type":"code","source":"import random\nfrom typing import List, Dict\n\ndef extract_ngrams_of_random_size(text: str, ns: List[int]) -> List[str]:\n    \"\"\"\n    Extract n-grams of randomly chosen sizes from the text.\n    For each n-gram, the size is sampled from the list of possible sizes.\n    \n    :param text: The input text.\n    :param ns: List of possible n-gram sizes to consider.\n    :return: A list of n-grams of randomly chosen sizes.\n    \"\"\"\n    ngrams = []\n    text_length = len(text)\n    \n    # Iterate over each possible start position of n-grams in the text\n    for start in range(text_length):\n        # Randomly choose an n-gram size for this start position\n        n = random.choice(ns)\n        \n        # Ensure the size does not exceed the remaining length of the text\n        if start + n <= text_length:\n            ngram = text[start:start + n]\n            ngrams.append(ngram)\n    \n    return ngrams","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:22:54.134144Z","iopub.execute_input":"2024-08-10T07:22:54.135155Z","iopub.status.idle":"2024-08-10T07:22:54.142020Z","shell.execute_reply.started":"2024-08-10T07:22:54.135116Z","shell.execute_reply":"2024-08-10T07:22:54.140876Z"},"trusted":true},"execution_count":274,"outputs":[]},{"cell_type":"code","source":"def prepare_sent(sentence, ngram_vocab, ns, utils_tokens=utils_tokens):\n    \"\"\"Prepare a sentence as embeddings.\"\"\"\n    ngrams = extract_ngrams_of_random_size(sentence, ns)\n    indices=[ngram_vocab.get(ngram, utils_tokens.get('<PAD>')) for ngram in ngrams]\n\n    return torch.LongTensor(indices)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:24:12.610019Z","iopub.execute_input":"2024-08-10T07:24:12.611075Z","iopub.status.idle":"2024-08-10T07:24:12.616205Z","shell.execute_reply.started":"2024-08-10T07:24:12.611037Z","shell.execute_reply":"2024-08-10T07:24:12.615286Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"sentences = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\nprepare_sent(sentences[0], NGRAM_VOCAB, NGRAMS)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:24:39.220378Z","iopub.execute_input":"2024-08-10T07:24:39.221253Z","iopub.status.idle":"2024-08-10T07:24:39.228668Z","shell.execute_reply.started":"2024-08-10T07:24:39.221217Z","shell.execute_reply":"2024-08-10T07:24:39.227664Z"},"trusted":true},"execution_count":300,"outputs":[{"execution_count":300,"output_type":"execute_result","data":{"text/plain":"tensor([4698, 4595, 2870, 3270, 2975,   13, 2276, 2970,  136,   34,  102,  136,\n         155,  156,   13, 2307, 2308,  115,   55,   52,  396, 7030, 5340,   87,\n        3469,  174,  109, 2966, 5251, 3543,   87,  256,  224,  171, 3001])"},"metadata":{}}]},{"cell_type":"code","source":"NUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:24:41.555304Z","iopub.execute_input":"2024-08-10T07:24:41.555685Z","iopub.status.idle":"2024-08-10T07:24:41.561729Z","shell.execute_reply.started":"2024-08-10T07:24:41.555656Z","shell.execute_reply":"2024-08-10T07:24:41.560770Z"},"trusted":true},"execution_count":301,"outputs":[{"execution_count":301,"output_type":"execute_result","data":{"text/plain":"17923"},"metadata":{}}]},{"cell_type":"code","source":"class FastTextEmbLSTMLoop(nn.Module):\n    def __init__(self, device='cpu', num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=128, num_layers=2, embedding_matrix=None):\n        super(FastTextEmbLSTMLoop, self).__init__()\n        self.device = device\n        self.num_tokens = num_tokens\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        \n        # Initialize n-gram embedding layer\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        if embedding_matrix is not None:\n            assert emb_size==embedding_matrix.shape[1]\n            assert num_tokens==embedding_matrix.shape[0]\n            \n            self.emb.weight.data.copy_(embedding_matrix)\n            self.emb.weight.requires_grad = False  # Optional: set to True if you want to fine-tune embeddings\n        \n        # RNN and Linear layers\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state = self.initial_state(x.shape[0])\n        \n        # Apply embedding layer\n        x_embedded = self.emb(x)\n        \n        # RNN forward pass\n        h_seq, hidden_state = self.rnn(x_embedded, hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        return next_logits\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        return (h_0, c_0)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:24:43.678095Z","iopub.execute_input":"2024-08-10T07:24:43.678462Z","iopub.status.idle":"2024-08-10T07:24:43.691251Z","shell.execute_reply.started":"2024-08-10T07:24:43.678434Z","shell.execute_reply":"2024-08-10T07:24:43.690337Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"markdown","source":"Т.к. изменили слой эмбеддинга, изменин класс датасета","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport re\n\nfrom typing import List, Dict","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:24:46.061359Z","iopub.execute_input":"2024-08-10T07:24:46.061742Z","iopub.status.idle":"2024-08-10T07:24:46.066465Z","shell.execute_reply.started":"2024-08-10T07:24:46.061713Z","shell.execute_reply":"2024-08-10T07:24:46.065545Z"},"trusted":true},"execution_count":303,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass NGramDataset(Dataset):\n    def __init__(self, sentences, ngram_vocab, ns=NGRAMS):\n        \"\"\"\n        Args:\n            sentences (list of str): List of sentences (each sentence is a string).\n            ngram_vocab (dict): Dictionary mapping n-grams to indices.\n            n (int): Size of the n-grams (e.g., 2 for bigrams).\n        \"\"\"\n        self.sentences = sentences\n        self.ngram_vocab = ngram_vocab\n        self.ns = ns\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        # Prepare the batch for the single sentence\n        return prepare_sent(sentence, self.ngram_vocab, self.ns)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:14:26.724814Z","iopub.execute_input":"2024-08-10T07:14:26.725976Z","iopub.status.idle":"2024-08-10T07:14:26.733760Z","shell.execute_reply.started":"2024-08-10T07:14:26.725937Z","shell.execute_reply":"2024-08-10T07:14:26.732677Z"},"trusted":true},"execution_count":272,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:05:17.909458Z","iopub.execute_input":"2024-08-10T07:05:17.910117Z","iopub.status.idle":"2024-08-10T07:05:17.914572Z","shell.execute_reply.started":"2024-08-10T07:05:17.910083Z","shell.execute_reply":"2024-08-10T07:05:17.913551Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataset = NGramDataset(quotes_prepared, NGRAM_VOCAB,)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:32.872135Z","iopub.execute_input":"2024-08-10T07:06:32.872802Z","iopub.status.idle":"2024-08-10T07:06:32.876761Z","shell.execute_reply.started":"2024-08-10T07:06:32.872767Z","shell.execute_reply":"2024-08-10T07:06:32.875900Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:35.320480Z","iopub.execute_input":"2024-08-10T07:06:35.321193Z","iopub.status.idle":"2024-08-10T07:06:35.325243Z","shell.execute_reply.started":"2024-08-10T07:06:35.321156Z","shell.execute_reply":"2024-08-10T07:06:35.324309Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-length sequences.\"\"\"\n    # `batch` is a list of tensors, each representing a sentence\n    # Pad sequences to the maximum length in this batch\n    return nn.utils.rnn.pad_sequence(batch, batch_first=True)\n\nquotes_train_dataloader = DataLoader(\n    quotes_train_dataset, \n    batch_size=16, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:37.655739Z","iopub.execute_input":"2024-08-10T07:06:37.656612Z","iopub.status.idle":"2024-08-10T07:06:37.662216Z","shell.execute_reply.started":"2024-08-10T07:06:37.656578Z","shell.execute_reply":"2024-08-10T07:06:37.661301Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:38.349378Z","iopub.execute_input":"2024-08-10T07:06:38.349751Z","iopub.status.idle":"2024-08-10T07:06:38.355690Z","shell.execute_reply.started":"2024-08-10T07:06:38.349721Z","shell.execute_reply":"2024-08-10T07:06:38.354720Z"},"trusted":true},"execution_count":259,"outputs":[{"execution_count":259,"output_type":"execute_result","data":{"text/plain":"torch.Size([17923, 100])"},"metadata":{}}]},{"cell_type":"code","source":"ft_emb_lstm_model = FastTextEmbLSTMLoop(device, num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=128, num_layers=2, embedding_matrix=EMBEDDING_MATRIX)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:39.111319Z","iopub.execute_input":"2024-08-10T07:06:39.111941Z","iopub.status.idle":"2024-08-10T07:06:39.162888Z","shell.execute_reply.started":"2024-08-10T07:06:39.111907Z","shell.execute_reply":"2024-08-10T07:06:39.162127Z"},"trusted":true},"execution_count":260,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam(ft_emb_lstm_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:40.285332Z","iopub.execute_input":"2024-08-10T07:06:40.285695Z","iopub.status.idle":"2024-08-10T07:06:40.293442Z","shell.execute_reply.started":"2024-08-10T07:06:40.285668Z","shell.execute_reply":"2024-08-10T07:06:40.292505Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nft_emb_lstm_model = ft_emb_lstm_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ft_emb_lstm_model = nn.DataParallel(ft_emb_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:41.050278Z","iopub.execute_input":"2024-08-10T07:06:41.051059Z","iopub.status.idle":"2024-08-10T07:06:41.062800Z","shell.execute_reply.started":"2024-08-10T07:06:41.051028Z","shell.execute_reply":"2024-08-10T07:06:41.061865Z"},"trusted":true},"execution_count":262,"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Теперь для подсчета лосса мне нужно разобрать каждое предложение на n-граммы","metadata":{}},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T07:06:42.638450Z","iopub.execute_input":"2024-08-10T07:06:42.639109Z","iopub.status.idle":"2024-08-10T07:06:43.181204Z","shell.execute_reply.started":"2024-08-10T07:06:42.639074Z","shell.execute_reply":"2024-08-10T07:06:43.179974Z"},"trusted":true},"execution_count":263,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:43.484692Z","iopub.execute_input":"2024-08-10T07:06:43.485758Z","iopub.status.idle":"2024-08-10T07:06:43.493241Z","shell.execute_reply.started":"2024-08-10T07:06:43.485703Z","shell.execute_reply":"2024-08-10T07:06:43.492175Z"},"trusted":true},"execution_count":264,"outputs":[{"execution_count":264,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"curr_batch=batch_ix.to(device)\n\nlogp_seq = ft_emb_lstm_model(curr_batch)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:44.305537Z","iopub.execute_input":"2024-08-10T07:06:44.306225Z","iopub.status.idle":"2024-08-10T07:06:44.321042Z","shell.execute_reply.started":"2024-08-10T07:06:44.306188Z","shell.execute_reply":"2024-08-10T07:06:44.320096Z"},"trusted":true},"execution_count":265,"outputs":[]},{"cell_type":"code","source":"# compute loss\npredictions_logp = logp_seq[:, :-1]\nactual_next_tokens = curr_batch[:, 1:]\n\nloss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\nloss.backward()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:45.280030Z","iopub.execute_input":"2024-08-10T07:06:45.280413Z","iopub.status.idle":"2024-08-10T07:06:45.294706Z","shell.execute_reply.started":"2024-08-10T07:06:45.280383Z","shell.execute_reply":"2024-08-10T07:06:45.293891Z"},"trusted":true},"execution_count":266,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:06:46.325211Z","iopub.execute_input":"2024-08-10T07:06:46.325557Z","iopub.status.idle":"2024-08-10T07:06:46.330256Z","shell.execute_reply.started":"2024-08-10T07:06:46.325529Z","shell.execute_reply":"2024-08-10T07:06:46.329343Z"},"trusted":true},"execution_count":267,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbest_model=None\nbest_loss=float('inf')\nfor epoch in range(num_epochs):\n    ft_emb_lstm_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = ft_emb_lstm_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 25 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n    \n    # Validate the model and calculate the metric\n    ft_emb_lstm_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch.to(device)\n\n            logp_seq = ft_emb_lstm_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    \n    if val_loss<best_loss:\n        print(f'Новый лучший лосс: {val_loss}')\n        best_loss=val_loss\n        best_model=copy.deepcopy(ft_emb_lstm_model)\n    \n    print(f'Текущий loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n\n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T07:06:47.035037Z","iopub.execute_input":"2024-08-10T07:06:47.035369Z","iopub.status.idle":"2024-08-10T07:07:48.799122Z","shell.execute_reply.started":"2024-08-10T07:06:47.035342Z","shell.execute_reply":"2024-08-10T07:07:48.797668Z"},"trusted":true},"execution_count":268,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stderr","text":"Epoch 1/100:  15%|█▍        | 328/2261 [01:01<06:03,  5.32it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[268], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# visualizing training process\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m     clear_output(\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\n# trained_model = best_model\ntrained_model = ft_emb_lstm_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:23.144558Z","iopub.execute_input":"2024-08-10T07:08:23.145586Z","iopub.status.idle":"2024-08-10T07:08:23.155948Z","shell.execute_reply.started":"2024-08-10T07:08:23.145549Z","shell.execute_reply":"2024-08-10T07:08:23.154502Z"},"trusted":true},"execution_count":269,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:26.910228Z","iopub.execute_input":"2024-08-10T07:08:26.911398Z","iopub.status.idle":"2024-08-10T07:08:26.947659Z","shell.execute_reply.started":"2024-08-10T07:08:26.911332Z","shell.execute_reply":"2024-08-10T07:08:26.946626Z"},"trusted":true},"execution_count":270,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T07:08:28.166314Z","iopub.execute_input":"2024-08-10T07:08:28.167237Z","iopub.status.idle":"2024-08-10T07:08:28.172647Z","shell.execute_reply.started":"2024-08-10T07:08:28.167195Z","shell.execute_reply":"2024-08-10T07:08:28.171464Z"},"trusted":true},"execution_count":271,"outputs":[]},{"cell_type":"code","source":"def generate_sample(ft_emb_torch_rnn, seed_phrase=' ',temperature=1.0, quotes_train_dataset=quotes_train_dataset):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    x_sequence = prepare_sent(seed_phrase).to(ft_emb_torch_rnn.device)\n    \n    # Initialize the hidden state\n    hid_state = ft_emb_torch_rnn.initial_state(batch_size=1)\n    \n    # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n    if seed_phrase.strip() != '':\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(quotes_train_dataset.max_len - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, hid_state = ft_emb_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(quotes_train_dataset.token_to_id), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(quotes_train_dataset.id_to_token[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(ft_emb_torch_rnn.device)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:45:06.019006Z","iopub.execute_input":"2024-08-08T12:45:06.019446Z","iopub.status.idle":"2024-08-08T12:45:06.031247Z","shell.execute_reply.started":"2024-08-08T12:45:06.019412Z","shell.execute_reply":"2024-08-08T12:45:06.030037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in np.linspace(1, 1.5, 10):\n    print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"execution":{"iopub.status.busy":"2024-08-08T12:45:07.868053Z","iopub.execute_input":"2024-08-08T12:45:07.869036Z","iopub.status.idle":"2024-08-08T12:45:10.418905Z","shell.execute_reply.started":"2024-08-08T12:45:07.868997Z","shell.execute_reply":"2024-08-08T12:45:10.417911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}