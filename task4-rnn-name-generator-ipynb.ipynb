{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Генерация текста с помощью RNN\n","metadata":{}},{"cell_type":"markdown","source":"(по мотивам [семинара](https://github.com/neychev/harbour_dlia2019/blob/master/day02_Simple_RNN/Day_2_Simple_RNN_pytorch.ipynb)\n [курса \"Deep Learning in Applications\"](https://in.harbour.space/data-science/deep-learning-in-applications-radoslav-neychev-anastasia-ianina/))","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/englishtea21/stepik-dl-nlp.git\n# !pip install -r stepik-dl-nlp/requirements.txt\nimport sys;","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-10T12:27:09.364565Z","iopub.execute_input":"2024-08-10T12:27:09.364953Z","iopub.status.idle":"2024-08-10T12:27:10.440353Z","shell.execute_reply.started":"2024-08-10T12:27:09.364921Z","shell.execute_reply":"2024-08-10T12:27:10.438698Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'stepik-dl-nlp' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/stepik-dl-nlp","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:27:15.624086Z","iopub.execute_input":"2024-08-10T12:27:15.624511Z","iopub.status.idle":"2024-08-10T12:27:15.633332Z","shell.execute_reply.started":"2024-08-10T12:27:15.624475Z","shell.execute_reply":"2024-08-10T12:27:15.632033Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/stepik-dl-nlp\n","output_type":"stream"}]},{"cell_type":"code","source":"# from google.colab import userdata\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:40.773952Z","iopub.execute_input":"2024-08-10T12:19:40.774599Z","iopub.status.idle":"2024-08-10T12:19:40.788449Z","shell.execute_reply.started":"2024-08-10T12:19:40.774557Z","shell.execute_reply":"2024-08-10T12:19:40.787297Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!git remote remove origin\n!git remote add origin https://englishtea21:{user_secrets.get_secret('stepik-samsung-nlp-github-token')}@github.com/englishtea21/stepik-dl-nlp.git","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:40.789843Z","iopub.execute_input":"2024-08-10T12:19:40.790230Z","iopub.status.idle":"2024-08-10T12:19:43.927251Z","shell.execute_reply.started":"2024-08-10T12:19:40.790199Z","shell.execute_reply":"2024-08-10T12:19:43.925650Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!git config --global user.email \"englishtea21@mail.ru\"\n!git config --global user.name \"englishtea21\"","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:43.929367Z","iopub.execute_input":"2024-08-10T12:19:43.929729Z","iopub.status.idle":"2024-08-10T12:19:46.204450Z","shell.execute_reply.started":"2024-08-10T12:19:43.929694Z","shell.execute_reply":"2024-08-10T12:19:46.202982Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:20:34.854793Z","start_time":"2019-11-05T18:20:34.372865Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.206223Z","iopub.execute_input":"2024-08-10T12:19:46.206560Z","iopub.status.idle":"2024-08-10T12:19:46.214551Z","shell.execute_reply.started":"2024-08-10T12:19:46.206530Z","shell.execute_reply":"2024-08-10T12:19:46.213356Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Данные\nДатасет содержит ~9k имен, все написаны латиницей.","metadata":{}},{"cell_type":"code","source":"# # Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n# with open('datasets/russian_names.txt') as input_file:\n#     names = input_file.read()[:-1].split('\\n')","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.509714Z","start_time":"2019-11-05T18:21:03.491489Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.215932Z","iopub.execute_input":"2024-08-10T12:19:46.216332Z","iopub.status.idle":"2024-08-10T12:19:46.226277Z","shell.execute_reply.started":"2024-08-10T12:19:46.216301Z","shell.execute_reply":"2024-08-10T12:19:46.225089Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# names[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:03.946758Z","start_time":"2019-11-05T18:21:03.938432Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.227697Z","iopub.execute_input":"2024-08-10T12:19:46.228095Z","iopub.status.idle":"2024-08-10T12:19:46.240486Z","shell.execute_reply.started":"2024-08-10T12:19:46.228055Z","shell.execute_reply":"2024-08-10T12:19:46.239421Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на распределение длин имен:","metadata":{}},{"cell_type":"markdown","source":"# Препроцессинг","metadata":{}},{"cell_type":"code","source":"# len(list(filter(str.isalpha, names))), len(names)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.244742Z","iopub.execute_input":"2024-08-10T12:19:46.245129Z","iopub.status.idle":"2024-08-10T12:19:46.251974Z","shell.execute_reply.started":"2024-08-10T12:19:46.245093Z","shell.execute_reply":"2024-08-10T12:19:46.251098Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# import re\n# only_latin_letters=re.compile(r'[A-Za-z]+')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.253335Z","iopub.execute_input":"2024-08-10T12:19:46.253649Z","iopub.status.idle":"2024-08-10T12:19:46.263957Z","shell.execute_reply.started":"2024-08-10T12:19:46.253621Z","shell.execute_reply":"2024-08-10T12:19:46.262816Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# names_tmp = list(filter(only_latin_letters.fullmatch, names))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.265596Z","iopub.execute_input":"2024-08-10T12:19:46.266586Z","iopub.status.idle":"2024-08-10T12:19:46.274466Z","shell.execute_reply.started":"2024-08-10T12:19:46.266546Z","shell.execute_reply":"2024-08-10T12:19:46.273484Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# len(names_tmp)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.275691Z","iopub.execute_input":"2024-08-10T12:19:46.275999Z","iopub.status.idle":"2024-08-10T12:19:46.285742Z","shell.execute_reply.started":"2024-08-10T12:19:46.275972Z","shell.execute_reply":"2024-08-10T12:19:46.284560Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# names=names_tmp\n# del names_tmp","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.287142Z","iopub.execute_input":"2024-08-10T12:19:46.287774Z","iopub.status.idle":"2024-08-10T12:19:46.296017Z","shell.execute_reply.started":"2024-08-10T12:19:46.287742Z","shell.execute_reply":"2024-08-10T12:19:46.294946Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# plt.title('Name length distribution')\n# plt.hist(list(map(len, names)), bins=25);","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:05.420060Z","start_time":"2019-11-05T18:21:05.179513Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.297665Z","iopub.execute_input":"2024-08-10T12:19:46.298011Z","iopub.status.idle":"2024-08-10T12:19:46.307353Z","shell.execute_reply.started":"2024-08-10T12:19:46.297983Z","shell.execute_reply":"2024-08-10T12:19:46.306310Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# names = [' ' + line for line in names]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.308654Z","iopub.execute_input":"2024-08-10T12:19:46.308979Z","iopub.status.idle":"2024-08-10T12:19:46.318085Z","shell.execute_reply.started":"2024-08-10T12:19:46.308950Z","shell.execute_reply":"2024-08-10T12:19:46.316900Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# #all unique characters go here\n# tokens = list(set(''.join(names)))\n\n# num_tokens = len(tokens)\n# print ('num_tokens = ', num_tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.335188Z","start_time":"2019-11-05T18:21:07.320148Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.319604Z","iopub.execute_input":"2024-08-10T12:19:46.320100Z","iopub.status.idle":"2024-08-10T12:19:46.329100Z","shell.execute_reply.started":"2024-08-10T12:19:46.320062Z","shell.execute_reply":"2024-08-10T12:19:46.328009Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# tokens","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.330528Z","iopub.execute_input":"2024-08-10T12:19:46.330856Z","iopub.status.idle":"2024-08-10T12:19:46.339130Z","shell.execute_reply.started":"2024-08-10T12:19:46.330828Z","shell.execute_reply":"2024-08-10T12:19:46.338097Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Мы специально добавляем пробел к нашим именам в тренировочных данных. Зачем? <br>\nEсли не будет специального символа, с которого начинается генерация, то мы лишим нашу модель способности выбирать первый символ последовательности","metadata":{}},{"cell_type":"markdown","source":"После того, как мы обучим нашу нейронную сеть, мы сможем генерировать имена, которые соответствуют некоторым условиям — например, имена, которые начинаются на букву \"a\" или на буквы \"abc\", или какие-либо другие условия. Если же мы захотим генерировать любые имена, начинающиеся с любой буквы, мы просто передадим нашей функции пробел в качестве первого символа. Таким образом, сможем сгенерировать имена, начинающиеся на любую букву. Отлично! С этой небольшой хитростью в коде разобрались.","metadata":{}},{"cell_type":"markdown","source":"### Символы -> id\n\nСоздадим словарь < символ > -> < id >","metadata":{}},{"cell_type":"code","source":"# token_to_id = {token: idx for idx, token in enumerate(tokens)}","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.674548Z","start_time":"2019-11-05T18:21:07.671129Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.340665Z","iopub.execute_input":"2024-08-10T12:19:46.340982Z","iopub.status.idle":"2024-08-10T12:19:46.350012Z","shell.execute_reply.started":"2024-08-10T12:19:46.340948Z","shell.execute_reply":"2024-08-10T12:19:46.348972Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# token_to_id.keys()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.352003Z","iopub.execute_input":"2024-08-10T12:19:46.353085Z","iopub.status.idle":"2024-08-10T12:19:46.360003Z","shell.execute_reply.started":"2024-08-10T12:19:46.353026Z","shell.execute_reply":"2024-08-10T12:19:46.358949Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n\n# for i in range(num_tokens):\n#     assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n\n# print(\"Seems alright!\")","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.838814Z","start_time":"2019-11-05T18:21:07.833611Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.361684Z","iopub.execute_input":"2024-08-10T12:19:46.362083Z","iopub.status.idle":"2024-08-10T12:19:46.370894Z","shell.execute_reply.started":"2024-08-10T12:19:46.362026Z","shell.execute_reply":"2024-08-10T12:19:46.369891Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Отлично! И теперь мы хотим преобразовать наши входные данные, а именно — наши 9 с небольшим хвостиком тысяч имён в некоторое численное представление, то есть вместо имени мы хотим получить численный вектор. Сделать это мы можем с помощью функций \"to_matrix\", которая будет преобразовывать наше имя из буквенного, человеко-читаемого формата в формат \"вектор с числами\".","metadata":{}},{"cell_type":"code","source":"# def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n#     \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n#     max_len = max_len or max(map(len, data))\n#     data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n#     for i in range(len(data)):\n#         line_ix = [token_to_id[c] for c in data[i]]\n#         data_ix[i, :len(line_ix)] = line_ix\n        \n#     if not batch_first: # convert [batch, time] into [time, batch]\n#         data_ix = np.transpose(data_ix)\n\n#     return data_ix","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:07.988093Z","start_time":"2019-11-05T18:21:07.977722Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.372235Z","iopub.execute_input":"2024-08-10T12:19:46.372571Z","iopub.status.idle":"2024-08-10T12:19:46.382017Z","shell.execute_reply.started":"2024-08-10T12:19:46.372544Z","shell.execute_reply":"2024-08-10T12:19:46.380952Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# #Example: cast 4 names to matrices, pad with zeros\n# print('\\n'.join(names[::2000]))\n# print(to_matrix(names[::2000], token_to_id))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:08.136936Z","start_time":"2019-11-05T18:21:08.131609Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.383413Z","iopub.execute_input":"2024-08-10T12:19:46.383760Z","iopub.status.idle":"2024-08-10T12:19:46.395490Z","shell.execute_reply.started":"2024-08-10T12:19:46.383724Z","shell.execute_reply":"2024-08-10T12:19:46.394374Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"22 - id пробела, этот токен встречается вначале (по умолчанию как знак начала), так и может быть в конце - паддинг до равной длины строк в матрице","metadata":{}},{"cell_type":"markdown","source":"# Рекуррентные нейронные сети\n\n<img src=\"img/rnn.png\" width=480>","metadata":{}},{"cell_type":"code","source":"# import torch, torch.nn as nn\n# import torch.nn.functional as F\n# # from torch.autograd import Variable","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.739438Z","start_time":"2019-11-05T18:21:09.661222Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.396907Z","iopub.execute_input":"2024-08-10T12:19:46.397967Z","iopub.status.idle":"2024-08-10T12:19:46.406428Z","shell.execute_reply.started":"2024-08-10T12:19:46.397923Z","shell.execute_reply":"2024-08-10T12:19:46.405383Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# class CharRNNCell(nn.Module):\n#     \"\"\"\n#     Implement the scheme above as torch module\n#     \"\"\"\n#     def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n#         super(self.__class__,self).__init__()\n#         self.num_units = rnn_num_units\n        \n#         self.embedding = nn.Embedding(num_tokens, embedding_size)\n#         self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n#         self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n#     def forward(self, x, h_prev):\n#         \"\"\"\n#         This method computes h_next(x, h_prev) and log P(x_next | h_next)\n#         We'll call it repeatedly to produce the whole sequence.\n        \n#         :param x: batch of character ids, variable containing vector of int64\n#         :param h_prev: previous rnn hidden states, variable containing matrix [batch, rnn_num_units] of float32\n#         \"\"\"\n#         # get vector embedding of x\n#         x_emb = self.embedding(x)\n        \n#         # compute next hidden state using self.rnn_update\n#         x_and_h = torch.cat([x_emb, h_prev], dim=1) #YOUR CODE HERE\n#         h_next = self.rnn_update(x_and_h) #YOUR CODE HERE\n        \n#         h_next = F.tanh(h_next)\n        \n#         assert h_next.size() == h_prev.size()\n        \n#         #compute logits for next character probs\n#         logits = self.rnn_to_logits(h_next)\n        \n#         return h_next, F.log_softmax(logits, -1)\n    \n#     def initial_state(self, batch_size):\n#         \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n#         return torch.zeros(batch_size, self.num_units)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:10.751862Z","start_time":"2019-11-05T18:21:10.741772Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.408058Z","iopub.execute_input":"2024-08-10T12:19:46.408385Z","iopub.status.idle":"2024-08-10T12:19:46.417917Z","shell.execute_reply.started":"2024-08-10T12:19:46.408359Z","shell.execute_reply":"2024-08-10T12:19:46.416855Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# char_rnn = CharRNNCell()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.071002Z","start_time":"2019-11-05T18:21:11.052377Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.419460Z","iopub.execute_input":"2024-08-10T12:19:46.419810Z","iopub.status.idle":"2024-08-10T12:19:46.433568Z","shell.execute_reply.started":"2024-08-10T12:19:46.419772Z","shell.execute_reply":"2024-08-10T12:19:46.432534Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети, RNN loop","metadata":{}},{"cell_type":"code","source":"# def rnn_loop(rnn, batch_index):\n#     \"\"\"\n#     Computes log P(next_character) for all time-steps in names_ix\n#     :param names_ix: an int32 matrix of shape [batch, time], output of to_matrix(names)\n#     \"\"\"\n#     batch_size, max_length = batch_index.size()\n#     hid_state = rnn.initial_state(batch_size)\n#     logprobs = []\n\n#     for x_t in batch_index.transpose(0,1):\n#         hid_state, logp_next = rnn(x_t, hid_state)  \n#         logprobs.append(logp_next)\n        \n#     return torch.stack(logprobs, dim=1)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:11.521078Z","start_time":"2019-11-05T18:21:11.510175Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.434985Z","iopub.execute_input":"2024-08-10T12:19:46.435353Z","iopub.status.idle":"2024-08-10T12:19:46.446299Z","shell.execute_reply.started":"2024-08-10T12:19:46.435324Z","shell.execute_reply":"2024-08-10T12:19:46.445275Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Тренировка сети","metadata":{}},{"cell_type":"code","source":"# from IPython.display import clear_output\n# from random import sample\n\n# char_rnn = CharRNNCell()\n# opt = torch.optim.Adam(char_rnn.parameters())\n# criterion = nn.NLLLoss()\n# history = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:12.120106Z","start_time":"2019-11-05T18:21:12.109585Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.457223Z","iopub.execute_input":"2024-08-10T12:19:46.457647Z","iopub.status.idle":"2024-08-10T12:19:46.462667Z","shell.execute_reply.started":"2024-08-10T12:19:46.457616Z","shell.execute_reply":"2024-08-10T12:19:46.461543Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# MAX_LENGTH = max(map(len, names))\n\n# for i in range(1000):\n\n#     batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n#     batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n#     logp_seq = rnn_loop(char_rnn, batch_ix)\n    \n#     # compute loss\n#     predictions_logp = logp_seq[:, :-1]\n#     actual_next_tokens = batch_ix[:, 1:]\n\n#     loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n#     # train with backprop\n#     loss.backward()\n#     opt.step()\n#     opt.zero_grad()\n    \n#     # visualizing training process\n#     history.append(loss.data.numpy())\n#     if (i + 1) % 100 == 0:\n#         clear_output(True)\n#         plt.plot(history,label='loss')\n#         plt.legend()\n#         plt.show()\n\n# assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.521061Z","start_time":"2019-11-05T18:21:12.302892Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.464111Z","iopub.execute_input":"2024-08-10T12:19:46.464455Z","iopub.status.idle":"2024-08-10T12:19:46.473693Z","shell.execute_reply.started":"2024-08-10T12:19:46.464428Z","shell.execute_reply":"2024-08-10T12:19:46.472700Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### RNN: генерация имен","metadata":{}},{"cell_type":"code","source":"# def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n#     '''\n#     The function generates text given a phrase of length at least SEQ_LENGTH.\n#     :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n#     :param max_length: maximum output length, including seed_phrase\n#     :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n#                         smaller temperature converges to the single most likely output\n#     '''\n    \n#     x_sequence = [token_to_id[token] for token in seed_phrase]\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n#     hid_state = char_rnn.initial_state(batch_size=1)\n    \n#     #feed the seed phrase, if any\n#     for i in range(len(seed_phrase) - 1):\n#         hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n    \n#     #start generating\n#     for _ in range(max_length - len(seed_phrase)):\n#         hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n#         p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n        \n#         # sample next token and push it back into x_sequence\n#         next_ix = np.random.choice(len(tokens), p=p_next)\n#         next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n#         x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n        \n#     return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.540765Z","start_time":"2019-11-05T18:21:23.524503Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.475205Z","iopub.execute_input":"2024-08-10T12:19:46.475587Z","iopub.status.idle":"2024-08-10T12:19:46.487418Z","shell.execute_reply.started":"2024-08-10T12:19:46.475547Z","shell.execute_reply":"2024-08-10T12:19:46.486334Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# sampled=[]\n# for _ in range(10):\n#     sampled.append(generate_sample(char_rnn))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.625562Z","start_time":"2019-11-05T18:21:23.544968Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.488794Z","iopub.execute_input":"2024-08-10T12:19:46.489150Z","iopub.status.idle":"2024-08-10T12:19:46.503362Z","shell.execute_reply.started":"2024-08-10T12:19:46.489120Z","shell.execute_reply":"2024-08-10T12:19:46.502349Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# sampled","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.504660Z","iopub.execute_input":"2024-08-10T12:19:46.504992Z","iopub.status.idle":"2024-08-10T12:19:46.514188Z","shell.execute_reply.started":"2024-08-10T12:19:46.504961Z","shell.execute_reply":"2024-08-10T12:19:46.513306Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# names_stripped = [name.strip() for name in names]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.515583Z","iopub.execute_input":"2024-08-10T12:19:46.515923Z","iopub.status.idle":"2024-08-10T12:19:46.525244Z","shell.execute_reply.started":"2024-08-10T12:19:46.515894Z","shell.execute_reply":"2024-08-10T12:19:46.524163Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# names_stripped[::2000]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.526755Z","iopub.execute_input":"2024-08-10T12:19:46.527223Z","iopub.status.idle":"2024-08-10T12:19:46.536308Z","shell.execute_reply.started":"2024-08-10T12:19:46.527185Z","shell.execute_reply":"2024-08-10T12:19:46.535196Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# [matched for el in sampled if el.strip() in names_stripped]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.537778Z","iopub.execute_input":"2024-08-10T12:19:46.538302Z","iopub.status.idle":"2024-08-10T12:19:46.546226Z","shell.execute_reply.started":"2024-08-10T12:19:46.538263Z","shell.execute_reply":"2024-08-10T12:19:46.545192Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# sampled=[]\n# for _ in range(10):\n#     sampled.append(generate_sample(char_rnn, seed_phrase=' Ar'))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.702249Z","start_time":"2019-11-05T18:21:23.629226Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.547405Z","iopub.execute_input":"2024-08-10T12:19:46.547680Z","iopub.status.idle":"2024-08-10T12:19:46.555990Z","shell.execute_reply.started":"2024-08-10T12:19:46.547656Z","shell.execute_reply":"2024-08-10T12:19:46.554957Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# sampled","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.557297Z","iopub.execute_input":"2024-08-10T12:19:46.557635Z","iopub.status.idle":"2024-08-10T12:19:46.566097Z","shell.execute_reply.started":"2024-08-10T12:19:46.557603Z","shell.execute_reply":"2024-08-10T12:19:46.565199Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# [matched for el in sampled if el.strip() in names_stripped]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.567242Z","iopub.execute_input":"2024-08-10T12:19:46.567551Z","iopub.status.idle":"2024-08-10T12:19:46.576950Z","shell.execute_reply.started":"2024-08-10T12:19:46.567524Z","shell.execute_reply":"2024-08-10T12:19:46.575915Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Как видно, наша модель именно генерирует новые имена, а не вспоминает запоменнные","metadata":{}},{"cell_type":"code","source":"# sampled=[]\n# for _ in range(10):\n#     sampled.append(generate_sample(char_rnn, seed_phrase=' Koval'))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.578040Z","iopub.execute_input":"2024-08-10T12:19:46.578376Z","iopub.status.idle":"2024-08-10T12:19:46.587845Z","shell.execute_reply.started":"2024-08-10T12:19:46.578348Z","shell.execute_reply":"2024-08-10T12:19:46.586865Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# sampled","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.589164Z","iopub.execute_input":"2024-08-10T12:19:46.590186Z","iopub.status.idle":"2024-08-10T12:19:46.598679Z","shell.execute_reply.started":"2024-08-10T12:19:46.590145Z","shell.execute_reply":"2024-08-10T12:19:46.597739Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"При маленькой температуре сеть генерирует фамилии, в которых она наиболее уверена <br>\nПри большой - очень разнообразные фамилии","metadata":{}},{"cell_type":"code","source":"# sampled=[]\n# for _ in range(10):\n#     sampled.append(generate_sample(char_rnn, seed_phrase=' Podo', temperature=1.2))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.600286Z","iopub.execute_input":"2024-08-10T12:19:46.601359Z","iopub.status.idle":"2024-08-10T12:19:46.609749Z","shell.execute_reply.started":"2024-08-10T12:19:46.601324Z","shell.execute_reply":"2024-08-10T12:19:46.608634Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# sampled","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.611159Z","iopub.execute_input":"2024-08-10T12:19:46.611471Z","iopub.status.idle":"2024-08-10T12:19:46.623811Z","shell.execute_reply.started":"2024-08-10T12:19:46.611443Z","shell.execute_reply":"2024-08-10T12:19:46.622897Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"### Более простое решение\n\n* `nn.RNNCell(emb_size, rnn_num_units)` - шаг RNN. Алгоритм: concat-linear-tanh\n* `nn.RNN(emb_size, rnn_num_units` - весь rnn_loop.\n\nКроме того, в PyTorch есть `nn.LSTMCell`, `nn.LSTM`, `nn.GRUCell`, `nn.GRU`, etc. etc.\n\nПерепишем наш пример с генерацией имен с помощью средств PyTorch.","metadata":{}},{"cell_type":"code","source":"# class CharRNNLoop(nn.Module):\n#     def __init__(self, num_tokens=num_tokens, emb_size=32, rnn_num_units=64):\n#         super(self.__class__, self).__init__()\n#         self.num_units = rnn_num_units\n#         self.emb = nn.Embedding(num_tokens, emb_size)\n#         self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n#         self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n        \n#     def forward_logits(self, x, hidden_state=None):\n#         if hidden_state is None:\n#             hidden_state=self.initial_state(x.shape[0])\n        \n#         h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n#         next_logits = self.hid_to_logits(h_seq)\n#         return next_logits, hidden_state\n        \n#     def forward_hidden(self, x, hidden_state=None):\n#         next_logits, hidden_state = self.forward_logits(x, hidden_state)\n#         next_logp = F.log_softmax(next_logits, dim=-1)\n#         return next_logp, hidden_state\n    \n#     def forward(self, x):\n#         next_logits, _ = self.forward_logits(x)\n#         next_logp = F.log_softmax(next_logits, dim=-1)\n#         return next_logp\n    \n#     def initial_state(self, batch_size):\n#         \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n#         return torch.zeros(1, batch_size, self.num_units) \n    \n# model = CharRNNLoop()\n# opt = torch.optim.Adam(model.parameters())\n# criterion = nn.NLLLoss()\n# history = []","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.713285Z","start_time":"2019-11-05T18:21:23.704755Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.625133Z","iopub.execute_input":"2024-08-10T12:19:46.625512Z","iopub.status.idle":"2024-08-10T12:19:46.635179Z","shell.execute_reply.started":"2024-08-10T12:19:46.625483Z","shell.execute_reply":"2024-08-10T12:19:46.634085Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# model=model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.636486Z","iopub.execute_input":"2024-08-10T12:19:46.636813Z","iopub.status.idle":"2024-08-10T12:19:46.646184Z","shell.execute_reply.started":"2024-08-10T12:19:46.636786Z","shell.execute_reply":"2024-08-10T12:19:46.645097Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# # the model applies over the whole sequence\n# batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n# # batch_ix = torch.LongTensor(batch_ix).to('cuda')\n# batch_ix = torch.LongTensor(batch_ix)\n\n# logp_seq = model(batch_ix)\n\n# # compute loss\n# loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n#                   batch_ix[:, :-1].contiguous().view(-1))\n\n# loss.backward()","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.647504Z","iopub.execute_input":"2024-08-10T12:19:46.647801Z","iopub.status.idle":"2024-08-10T12:19:46.657347Z","shell.execute_reply.started":"2024-08-10T12:19:46.647773Z","shell.execute_reply":"2024-08-10T12:19:46.656344Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# MAX_LENGTH = max(map(len, names))\n\n\n# for i in range(1000):\n#     batch_ix = to_matrix(sample(names, 32), token_to_id, max_len=MAX_LENGTH)\n# #     batch_ix = torch.tensor(batch_ix, dtype=torch.int64).to('cuda')\n#     batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n    \n#     logp_seq = model(batch_ix)\n    \n#     # compute loss\n#     predictions_logp = logp_seq[:, :-1]\n#     actual_next_tokens = batch_ix[:, 1:]\n\n#     loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n    \n#     # train with backprop\n#     loss.backward()\n#     opt.step()\n#     opt.zero_grad()\n    \n#     history.append(loss.data.cpu().numpy())\n#     if (i + 1) % 100 == 0:\n#         clear_output(True)\n#         plt.plot(history, label='loss')\n#         plt.legend()\n#         plt.show()\n\n# assert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.658609Z","iopub.execute_input":"2024-08-10T12:19:46.658900Z","iopub.status.idle":"2024-08-10T12:19:46.668825Z","shell.execute_reply.started":"2024-08-10T12:19:46.658874Z","shell.execute_reply":"2024-08-10T12:19:46.667878Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n#     '''\n#     The function generates text given a phrase of length at least SEQ_LENGTH.\n#     :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n#     :param max_length: maximum output length, including seed_phrase\n#     :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n#                         smaller temperature converges to the single most likely output\n#     '''\n    \n#     # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id[token] for token in seed_phrase]\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n#     # Initialize the hidden state\n#     hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n#     if seed_phrase!=' ':\n#         _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n#     # Start generating text\n#     generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n#     for _ in range(max_length - len(seed_phrase)):\n#         # Get the logits for the next character\n#         next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n#         # Apply temperature to logits\n#         next_logits = next_logits / temperature\n        \n#         # Calculate probabilities using softmax\n#         p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n#         # Sample the next character index from the probability distribution\n#         next_ix = np.random.choice(len(tokens), p=p_next)\n        \n#         # Append the sampled character to the generated sequence\n#         generated_sequence.append(tokens[next_ix])\n        \n#         # Update the input sequence with the new character\n#         next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n#         x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n#         # Update hidden state for the next character\n#         _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n#     return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.670451Z","iopub.execute_input":"2024-08-10T12:19:46.671223Z","iopub.status.idle":"2024-08-10T12:19:46.684497Z","shell.execute_reply.started":"2024-08-10T12:19:46.671192Z","shell.execute_reply":"2024-08-10T12:19:46.683610Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# # model=model.cpu()\n# for _ in range(10):\n#     print(generate_sample(model,seed_phrase='Stryk', temperature=1))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.526436Z","start_time":"2019-11-05T18:21:31.469965Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.685879Z","iopub.execute_input":"2024-08-10T12:19:46.686386Z","iopub.status.idle":"2024-08-10T12:19:46.699387Z","shell.execute_reply.started":"2024-08-10T12:19:46.686348Z","shell.execute_reply":"2024-08-10T12:19:46.698442Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Домашнее задание: мотивационные лозунги","metadata":{}},{"cell_type":"markdown","source":"Возможно стоит учить эмбеддинги n-грамм","metadata":{}},{"cell_type":"code","source":"# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\nwith open('datasets/author_quotes.txt') as input_file:\n    quotes = input_file.read()[:-1].split('\\n')\n    quotes = [' ' + line for line in quotes]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.570320Z","start_time":"2019-11-05T18:21:31.528673Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.700767Z","iopub.execute_input":"2024-08-10T12:19:46.701134Z","iopub.status.idle":"2024-08-10T12:19:46.753803Z","shell.execute_reply.started":"2024-08-10T12:19:46.701103Z","shell.execute_reply":"2024-08-10T12:19:46.752693Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"quotes[:5]","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.575286Z","start_time":"2019-11-05T18:21:31.571798Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.755445Z","iopub.execute_input":"2024-08-10T12:19:46.755754Z","iopub.status.idle":"2024-08-10T12:19:46.763522Z","shell.execute_reply.started":"2024-08-10T12:19:46.755728Z","shell.execute_reply":"2024-08-10T12:19:46.762389Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"[' If you live to be a hundred, I want to live to be a hundred minus one day so I never have to live without you.',\n \" Promise me you'll always remember: You're braver than you believe, and stronger than you seem, and smarter than you think.\",\n ' Did you ever stop to think, and forget to start again?',\n ' Organizing is what you do before you do something, so that when you do it, it is not all mixed up.',\n ' Weeds are flowers too, once you get to know them.']"},"metadata":{}}]},{"cell_type":"code","source":"# tokens = list(set(''.join(quotes)))\n# token_to_id = {token: idx for idx, token in enumerate(tokens)}\n# num_tokens = len(tokens)","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.653673Z","start_time":"2019-11-05T18:21:31.578424Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.764804Z","iopub.execute_input":"2024-08-10T12:19:46.765168Z","iopub.status.idle":"2024-08-10T12:19:46.774963Z","shell.execute_reply.started":"2024-08-10T12:19:46.765126Z","shell.execute_reply":"2024-08-10T12:19:46.773915Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# MAX_LENGTH = max(map(len, quotes))\n# MAX_LENGTH","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.776188Z","iopub.execute_input":"2024-08-10T12:19:46.776496Z","iopub.status.idle":"2024-08-10T12:19:46.786078Z","shell.execute_reply.started":"2024-08-10T12:19:46.776470Z","shell.execute_reply":"2024-08-10T12:19:46.785102Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# # tokens, \n# num_tokens","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.787239Z","iopub.execute_input":"2024-08-10T12:19:46.788188Z","iopub.status.idle":"2024-08-10T12:19:46.797663Z","shell.execute_reply.started":"2024-08-10T12:19:46.788158Z","shell.execute_reply":"2024-08-10T12:19:46.796765Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.798949Z","iopub.execute_input":"2024-08-10T12:19:46.799318Z","iopub.status.idle":"2024-08-10T12:19:46.808302Z","shell.execute_reply.started":"2024-08-10T12:19:46.799290Z","shell.execute_reply":"2024-08-10T12:19:46.807220Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# class CharRNNLoop(nn.Module):\n#     def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n#         super(self.__class__, self).__init__()\n#         self.device=device\n#         self.num_layers=num_layers\n#         self.hidden_size = hidden_size\n#         self.emb = nn.Embedding(num_tokens, emb_size)\n#         self.rnn = nn.RNN(emb_size, hidden_size, num_layers, batch_first=True)\n#         self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n#     def forward_logits(self, x, hidden_state=None):\n#         if hidden_state is None:\n#             hidden_state=self.initial_state(x.shape[0])\n        \n#         h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n#         next_logits = self.hid_to_logits(h_seq)\n#         return next_logits, hidden_state\n        \n#     def forward_hidden(self, x, hidden_state=None):\n#         next_logits, hidden_state = self.forward_logits(x, hidden_state)\n#         next_logp = F.log_softmax(next_logits, dim=-1)\n#         return next_logp, hidden_state\n    \n#     def forward(self, x):\n#         next_logits, _ = self.forward_logits(x)\n#         next_logp = F.log_softmax(next_logits, dim=-1)\n#         return next_logp\n    \n#     def initial_state(self, batch_size):\n#         \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n# #         return torch.zeros(1, batch_size, self.hidden_size, device=self.device) \n#         return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n    \n# quotes_baseline_model = CharRNNLoop(num_layers=4)\n# opt = torch.optim.Adam(quotes_baseline_model.parameters(), lr=1e-4)\n# sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=1)\n# criterion = nn.CrossEntropyLoss()\n\n# history=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.809830Z","iopub.execute_input":"2024-08-10T12:19:46.810515Z","iopub.status.idle":"2024-08-10T12:19:46.821349Z","shell.execute_reply.started":"2024-08-10T12:19:46.810476Z","shell.execute_reply":"2024-08-10T12:19:46.820352Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# # Move model to the first GPU device\n# quotes_baseline_model = quotes_baseline_model.to(device)\n\n# # Use DataParallel to utilize multiple GPUs\n# if torch.cuda.device_count() > 1:\n#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#     quotes_baseline_model = nn.DataParallel(quotes_baseline_model)\n    \n# quotes_baseline_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.822506Z","iopub.execute_input":"2024-08-10T12:19:46.822812Z","iopub.status.idle":"2024-08-10T12:19:46.836914Z","shell.execute_reply.started":"2024-08-10T12:19:46.822788Z","shell.execute_reply":"2024-08-10T12:19:46.835807Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import Dataset, DataLoader\n# import copy","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.838098Z","iopub.execute_input":"2024-08-10T12:19:46.838423Z","iopub.status.idle":"2024-08-10T12:19:46.854961Z","shell.execute_reply.started":"2024-08-10T12:19:46.838395Z","shell.execute_reply":"2024-08-10T12:19:46.853896Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# def to_matrix(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n#     \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n    \n#     max_len = max_len or max(map(len, data))\n#     data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n\n#     for i in range(len(data)):\n#         line_ix = [token_to_id[c] for c in data[i]]\n#         data_ix[i, :len(line_ix)] = line_ix\n        \n#     if not batch_first: # convert [batch, time] into [time, batch]\n#         data_ix = np.transpose(data_ix)\n\n#     return data_ix","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.856194Z","iopub.execute_input":"2024-08-10T12:19:46.856502Z","iopub.status.idle":"2024-08-10T12:19:46.868944Z","shell.execute_reply.started":"2024-08-10T12:19:46.856474Z","shell.execute_reply":"2024-08-10T12:19:46.867999Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# class QuotesDataset(Dataset):\n#     def __init__(self, quotes_list: list[str], token_to_id: dict, max_len: int):\n#         self.quotes_list=copy.deepcopy(quotes_list)\n#         self.token_to_id=copy.deepcopy(token_to_id)\n#         self.max_len=max_len\n#         self.quotes_tensors=torch.LongTensor(to_matrix(self.quotes_list, self.token_to_id, self.max_len)) \n\n#     def __len__(self):\n#         return len(self.quotes_list)\n\n#     def __getitem__(self, idx):\n#         return self.quotes_tensors[idx, :]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.870645Z","iopub.execute_input":"2024-08-10T12:19:46.871065Z","iopub.status.idle":"2024-08-10T12:19:46.881114Z","shell.execute_reply.started":"2024-08-10T12:19:46.871010Z","shell.execute_reply":"2024-08-10T12:19:46.880067Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# quotes_train = QuotesDataset(quotes, token_to_id, MAX_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.882509Z","iopub.execute_input":"2024-08-10T12:19:46.883448Z","iopub.status.idle":"2024-08-10T12:19:46.891532Z","shell.execute_reply.started":"2024-08-10T12:19:46.883408Z","shell.execute_reply":"2024-08-10T12:19:46.890635Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# len(quotes)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.892825Z","iopub.execute_input":"2024-08-10T12:19:46.893254Z","iopub.status.idle":"2024-08-10T12:19:46.903101Z","shell.execute_reply.started":"2024-08-10T12:19:46.893223Z","shell.execute_reply":"2024-08-10T12:19:46.902158Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.904411Z","iopub.execute_input":"2024-08-10T12:19:46.904723Z","iopub.status.idle":"2024-08-10T12:19:46.913887Z","shell.execute_reply.started":"2024-08-10T12:19:46.904695Z","shell.execute_reply":"2024-08-10T12:19:46.912688Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# quotes_train_dataloader = DataLoader(\n#     quotes_train, \n#     batch_size=128, \n#     shuffle=True, \n#     num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n#     pin_memory=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.915371Z","iopub.execute_input":"2024-08-10T12:19:46.915670Z","iopub.status.idle":"2024-08-10T12:19:46.924904Z","shell.execute_reply.started":"2024-08-10T12:19:46.915644Z","shell.execute_reply":"2024-08-10T12:19:46.923942Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.926177Z","iopub.execute_input":"2024-08-10T12:19:46.926536Z","iopub.status.idle":"2024-08-10T12:19:46.935330Z","shell.execute_reply.started":"2024-08-10T12:19:46.926508Z","shell.execute_reply":"2024-08-10T12:19:46.934168Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# batch_ix.device","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.936487Z","iopub.execute_input":"2024-08-10T12:19:46.936761Z","iopub.status.idle":"2024-08-10T12:19:46.945640Z","shell.execute_reply.started":"2024-08-10T12:19:46.936736Z","shell.execute_reply":"2024-08-10T12:19:46.944548Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# batch_ix=batch_ix.to(device)\n# logp_seq = quotes_baseline_model(batch_ix)\n\n# # compute loss\n# loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n#                   batch_ix[:, :-1].contiguous().view(-1))\n\n# loss.backward()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.946930Z","iopub.execute_input":"2024-08-10T12:19:46.947386Z","iopub.status.idle":"2024-08-10T12:19:46.956301Z","shell.execute_reply.started":"2024-08-10T12:19:46.947355Z","shell.execute_reply":"2024-08-10T12:19:46.955291Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.960476Z","iopub.execute_input":"2024-08-10T12:19:46.960896Z","iopub.status.idle":"2024-08-10T12:19:46.971331Z","shell.execute_reply.started":"2024-08-10T12:19:46.960865Z","shell.execute_reply":"2024-08-10T12:19:46.970330Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.972672Z","iopub.execute_input":"2024-08-10T12:19:46.973069Z","iopub.status.idle":"2024-08-10T12:19:46.982890Z","shell.execute_reply.started":"2024-08-10T12:19:46.973011Z","shell.execute_reply":"2024-08-10T12:19:46.981901Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# num_epochs=20\n# for epoch in range(num_epochs):\n#     quotes_baseline_model.train()\n#     total_batches = len(quotes_train_dataloader)\n\n#     # Wrap DataLoader iterator with tqdm\n#     for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n# #         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n# #         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n#         curr_batch=batch_ix.to(device)\n\n#         logp_seq = quotes_baseline_model(curr_batch)\n\n#         # compute loss\n#         predictions_logp = logp_seq[:, :-1]\n#         actual_next_tokens = curr_batch[:, 1:]\n\n#         loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n#         # train with backprop\n#         loss.backward()\n#         opt.step()\n#         opt.zero_grad()\n\n#         # visualizing training process\n#         history.append(loss.cpu().data.numpy())\n#         if (i + 1) % 25 == 0:\n#             clear_output(True)\n#             plt.plot(history,label='loss')\n#             plt.legend()\n#             plt.show()\n\n#      # Validate the model and calculate the metric\n#     quotes_baseline_model.eval()\n#     val_loss = 0.0\n#     with torch.no_grad():\n#         for batch in quotes_train_dataloader:\n#             curr_batch=batch_ix.to(device)\n\n#             logp_seq = quotes_baseline_model(curr_batch)\n\n#             # compute loss\n#             predictions_logp = logp_seq[:, :-1]\n#             actual_next_tokens = curr_batch[:, 1:]\n\n#             loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n#             val_loss += loss.item()\n\n#     val_loss /= len(quotes_train_dataloader)\n#     print(f'Val loss: {val_loss}')\n    \n#     # Step the scheduler\n#     sched.step(val_loss)\n    \n#     assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:46.984381Z","iopub.execute_input":"2024-08-10T12:19:46.984669Z","iopub.status.idle":"2024-08-10T12:19:46.995332Z","shell.execute_reply.started":"2024-08-10T12:19:46.984643Z","shell.execute_reply":"2024-08-10T12:19:46.994409Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# # Assume your trained model is wrapped in DataParallel\n# trained_model = quotes_baseline_model\n\n# # Check if the model is wrapped with DataParallel\n# if isinstance(trained_model, nn.DataParallel):\n#     # Extract the original model\n#     trained_model = trained_model.module","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:46.996564Z","iopub.execute_input":"2024-08-10T12:19:46.996865Z","iopub.status.idle":"2024-08-10T12:19:47.009708Z","shell.execute_reply.started":"2024-08-10T12:19:46.996838Z","shell.execute_reply":"2024-08-10T12:19:47.008606Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# trained_model_cpu=trained_model.cpu()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.011203Z","iopub.execute_input":"2024-08-10T12:19:47.011567Z","iopub.status.idle":"2024-08-10T12:19:47.023532Z","shell.execute_reply.started":"2024-08-10T12:19:47.011537Z","shell.execute_reply":"2024-08-10T12:19:47.022448Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# trained_model_cpu.device=torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.024724Z","iopub.execute_input":"2024-08-10T12:19:47.025077Z","iopub.status.idle":"2024-08-10T12:19:47.034748Z","shell.execute_reply.started":"2024-08-10T12:19:47.025010Z","shell.execute_reply":"2024-08-10T12:19:47.033653Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n#     '''\n#     The function generates text given a phrase of length at least SEQ_LENGTH.\n#     :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n#     :param max_length: maximum output length, including seed_phrase\n#     :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n#                         smaller temperature converges to the single most likely output\n#     '''\n    \n#     # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id[token] for token in seed_phrase]\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n    \n#     # Initialize the hidden state\n#     hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n#     if seed_phrase!=' ':\n#         _, hid_state = char_torch_rnn.forward_hidden(x_sequence[:, len(seed_phrase)-2].unsqueeze(0))\n    \n#     # Start generating text\n#     generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n#     for _ in range(max_length - len(seed_phrase)):\n#         # Get the logits for the next character\n#         next_logits, _ = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(0), hid_state)\n        \n#         # Apply temperature to logits\n#         next_logits = next_logits / temperature\n        \n#         # Calculate probabilities using softmax\n#         p_next = F.softmax(next_logits, dim=-1).data.cpu().numpy().flatten()\n#         # Sample the next character index from the probability distribution\n#         next_ix = np.random.choice(len(tokens), p=p_next)\n        \n#         # Append the sampled character to the generated sequence\n#         generated_sequence.append(tokens[next_ix])\n        \n#         # Update the input sequence with the new character\n#         next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64)\n#         x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n#         # Update hidden state for the next character\n#         _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n#     return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.036191Z","iopub.execute_input":"2024-08-10T12:19:47.036505Z","iopub.status.idle":"2024-08-10T12:19:47.047279Z","shell.execute_reply.started":"2024-08-10T12:19:47.036477Z","shell.execute_reply":"2024-08-10T12:19:47.046234Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.048686Z","iopub.execute_input":"2024-08-10T12:19:47.049001Z","iopub.status.idle":"2024-08-10T12:19:47.060765Z","shell.execute_reply.started":"2024-08-10T12:19:47.048974Z","shell.execute_reply":"2024-08-10T12:19:47.059796Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# generate_sample(trained_model, seed_phrase='Life is ', temperature=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.061997Z","iopub.execute_input":"2024-08-10T12:19:47.062355Z","iopub.status.idle":"2024-08-10T12:19:47.071594Z","shell.execute_reply.started":"2024-08-10T12:19:47.062317Z","shell.execute_reply":"2024-08-10T12:19:47.070550Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"Наша посимвольная модель страдает фигней... <br>\nПопробуем вместо эмбеддинга символов использовать эмбеддинги FastText <br>\nТакже вместо rnn воспользуемся lstm или gru","metadata":{}},{"cell_type":"markdown","source":"## LSTM-based посимвольная модель","metadata":{}},{"cell_type":"code","source":"# class CharLSTMLoop(nn.Module):\n#     def __init__(self, device=device, num_tokens=num_tokens, emb_size=64, hidden_size=64, num_layers=1):\n#         super(self.__class__, self).__init__()\n#         self.device=device\n#         self.num_layers=num_layers\n#         self.hidden_size = hidden_size\n#         self.emb = nn.Embedding(num_tokens, emb_size)\n#         self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n#         self.hid_to_logits = nn.Linear(hidden_size, num_tokens)\n        \n#     def forward_logits(self, x, hidden_state=None):\n#         if hidden_state is None:\n#             hidden_state=self.initial_state(x.shape[0])\n        \n#         h_seq, hidden_state = self.rnn(self.emb(x), hidden_state)\n#         next_logits = self.hid_to_logits(h_seq)\n#         return next_logits, hidden_state\n        \n#     def forward_hidden(self, x, hidden_state=None):\n#         next_logits, hidden_state = self.forward_logits(x, hidden_state)\n#         next_logp = F.log_softmax(next_logits, dim=-1)\n#         return next_logp, hidden_state\n    \n#     def forward(self, x):\n#         next_logits, _ = self.forward_logits(x)\n#         return next_logits\n    \n#     def initial_state(self, batch_size):\n#         \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n#         # Initialize both hidden state (h_0) and cell state (c_0)\n#         h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n#         c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n#         return (h_0, c_0)\n    \n# quotes_char_lstm_model = CharLSTMLoop(emb_size=128, hidden_size=128, num_layers=2)\n# opt = torch.optim.Adam(quotes_char_lstm_model.parameters(), lr=1e-4)\n# sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\n# criterion = nn.CrossEntropyLoss()\n\n# history=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.072987Z","iopub.execute_input":"2024-08-10T12:19:47.073613Z","iopub.status.idle":"2024-08-10T12:19:47.082973Z","shell.execute_reply.started":"2024-08-10T12:19:47.073576Z","shell.execute_reply":"2024-08-10T12:19:47.081889Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# # Move model to the first GPU device\n# quotes_char_lstm_model = quotes_char_lstm_model.to(device)\n\n# # Use DataParallel to utilize multiple GPUs\n# if torch.cuda.device_count() > 1:\n#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#     quotes_char_lstm_model = nn.DataParallel(quotes_char_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.084629Z","iopub.execute_input":"2024-08-10T12:19:47.085480Z","iopub.status.idle":"2024-08-10T12:19:47.096425Z","shell.execute_reply.started":"2024-08-10T12:19:47.085436Z","shell.execute_reply":"2024-08-10T12:19:47.095519Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# quotes_train_dataloader = DataLoader(\n#     quotes_train, \n#     batch_size=256, \n#     shuffle=True, \n#     num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n#     pin_memory=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.097701Z","iopub.execute_input":"2024-08-10T12:19:47.098059Z","iopub.status.idle":"2024-08-10T12:19:47.110612Z","shell.execute_reply.started":"2024-08-10T12:19:47.098010Z","shell.execute_reply":"2024-08-10T12:19:47.109616Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:47.111977Z","iopub.execute_input":"2024-08-10T12:19:47.112713Z","iopub.status.idle":"2024-08-10T12:19:47.121534Z","shell.execute_reply.started":"2024-08-10T12:19:47.112673Z","shell.execute_reply":"2024-08-10T12:19:47.120523Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"# batch_ix.device","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.122811Z","iopub.execute_input":"2024-08-10T12:19:47.123175Z","iopub.status.idle":"2024-08-10T12:19:47.132384Z","shell.execute_reply.started":"2024-08-10T12:19:47.123129Z","shell.execute_reply":"2024-08-10T12:19:47.131325Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# batch_ix=batch_ix.to(device)\n# logp_seq = quotes_char_lstm_model(batch_ix)\n\n# # compute loss\n# loss = criterion(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n#                   batch_ix[:, :-1].contiguous().view(-1))\n\n# loss.backward()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.133822Z","iopub.execute_input":"2024-08-10T12:19:47.134153Z","iopub.status.idle":"2024-08-10T12:19:47.144932Z","shell.execute_reply.started":"2024-08-10T12:19:47.134125Z","shell.execute_reply":"2024-08-10T12:19:47.143873Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.161763Z","iopub.execute_input":"2024-08-10T12:19:47.162300Z","iopub.status.idle":"2024-08-10T12:19:47.166963Z","shell.execute_reply.started":"2024-08-10T12:19:47.162266Z","shell.execute_reply":"2024-08-10T12:19:47.165879Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.168450Z","iopub.execute_input":"2024-08-10T12:19:47.168798Z","iopub.status.idle":"2024-08-10T12:19:47.178499Z","shell.execute_reply.started":"2024-08-10T12:19:47.168771Z","shell.execute_reply":"2024-08-10T12:19:47.177488Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# num_epochs=100\n# best_model=None\n# best_loss=float('inf')\n# for epoch in range(num_epochs):\n#     quotes_char_lstm_model.train()\n#     total_batches = len(quotes_train_dataloader)\n\n#     # Wrap DataLoader iterator with tqdm\n#     for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n# #         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n# #         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n#         curr_batch=batch_ix.to(device)\n\n#         logp_seq = quotes_char_lstm_model(curr_batch)\n\n#         # compute loss\n#         predictions_logp = logp_seq[:, :-1]\n#         actual_next_tokens = curr_batch[:, 1:]\n\n#         loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n#         # train with backprop\n#         loss.backward()\n#         opt.step()\n#         opt.zero_grad()\n\n#         # visualizing training process\n#         history.append(loss.cpu().data.numpy())\n#         if (i + 1) % 25 == 0:\n#             clear_output(True)\n#             plt.plot(history,label='loss')\n#             plt.legend()\n#             plt.show()\n    \n#     # Validate the model and calculate the metric\n#     quotes_char_lstm_model.eval()\n#     val_loss = 0.0\n#     with torch.no_grad():\n#         for batch in quotes_train_dataloader:\n#             curr_batch=batch.to(device)\n\n#             logp_seq = quotes_char_lstm_model(curr_batch)\n\n#             # compute loss\n#             predictions_logp = logp_seq[:, :-1]\n#             actual_next_tokens = curr_batch[:, 1:]\n\n#             loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n#             val_loss += loss.item()\n\n#     val_loss /= len(quotes_train_dataloader)\n    \n#     if val_loss<best_loss:\n#         print(f'Новый лучший лосс: {val_loss}')\n#         best_loss=val_loss\n#         best_model=copy.deepcopy(quotes_char_lstm_model)\n    \n#     print(f'Текущий loss: {val_loss}')\n    \n#     # Step the scheduler\n#     sched.step(val_loss)\n\n#     assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T12:19:47.180776Z","iopub.execute_input":"2024-08-10T12:19:47.181115Z","iopub.status.idle":"2024-08-10T12:19:47.191333Z","shell.execute_reply.started":"2024-08-10T12:19:47.181074Z","shell.execute_reply":"2024-08-10T12:19:47.190105Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# # Assume your trained model is wrapped in DataParallel\n# trained_model = best_model\n# # train_model = quotes_char_lstm_model\n\n# # Check if the model is wrapped with DataParallel\n# if isinstance(trained_model, nn.DataParallel):\n#     # Extract the original model\n#     trained_model = trained_model.module","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.192854Z","iopub.execute_input":"2024-08-10T12:19:47.193465Z","iopub.status.idle":"2024-08-10T12:19:47.207455Z","shell.execute_reply.started":"2024-08-10T12:19:47.193422Z","shell.execute_reply":"2024-08-10T12:19:47.206426Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"# trained_model_cpu=trained_model.cpu()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.208641Z","iopub.execute_input":"2024-08-10T12:19:47.208979Z","iopub.status.idle":"2024-08-10T12:19:47.218879Z","shell.execute_reply.started":"2024-08-10T12:19:47.208949Z","shell.execute_reply":"2024-08-10T12:19:47.217794Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# trained_model_cpu.device=torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.220103Z","iopub.execute_input":"2024-08-10T12:19:47.220416Z","iopub.status.idle":"2024-08-10T12:19:47.230261Z","shell.execute_reply.started":"2024-08-10T12:19:47.220376Z","shell.execute_reply":"2024-08-10T12:19:47.229239Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# def generate_sample(char_torch_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n#     '''\n#     The function generates text given a phrase of length at least SEQ_LENGTH.\n#     :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n#     :param max_length: maximum output length, including seed_phrase\n#     :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n#                         smaller temperature converges to the single most likely output\n#     '''\n    \n#     # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    \n#     # Initialize the hidden state\n#     hid_state = char_torch_rnn.initial_state(batch_size=1)\n    \n#     # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n#     if seed_phrase.strip() != '':\n#         _, hid_state = char_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n#     # Start generating text\n#     generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n#     for _ in range(max_length - len(seed_phrase)):\n#         # Get the logits for the next character\n#         next_logits, hid_state = char_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n#         # Apply temperature to logits\n#         next_logits = next_logits / temperature\n        \n#         # Calculate probabilities using softmax\n#         p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n#         # Sample the next character index from the probability distribution\n#         next_ix = np.random.choice(len(token_to_id), p=p_next)\n        \n#         # Append the sampled character to the generated sequence\n#         generated_sequence.append(tokens[next_ix])\n        \n#         # Update the input sequence with the new character\n#         next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(char_torch_rnn.device)\n#         x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n#         # Update hidden state for the next character\n#         _, hid_state = char_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n#     return ''.join(generated_sequence)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.232019Z","iopub.execute_input":"2024-08-10T12:19:47.232465Z","iopub.status.idle":"2024-08-10T12:19:47.243430Z","shell.execute_reply.started":"2024-08-10T12:19:47.232428Z","shell.execute_reply":"2024-08-10T12:19:47.242345Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.244911Z","iopub.execute_input":"2024-08-10T12:19:47.245346Z","iopub.status.idle":"2024-08-10T12:19:47.259417Z","shell.execute_reply.started":"2024-08-10T12:19:47.245309Z","shell.execute_reply":"2024-08-10T12:19:47.258309Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# for t in np.linspace(1, 1.5, 10):\n#     print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.260796Z","iopub.execute_input":"2024-08-10T12:19:47.261657Z","iopub.status.idle":"2024-08-10T12:19:47.270830Z","shell.execute_reply.started":"2024-08-10T12:19:47.261625Z","shell.execute_reply":"2024-08-10T12:19:47.269799Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"## Получается туфта, возьмем эмбеддинги fasttext","metadata":{}},{"cell_type":"code","source":"import fasttext\nimport fasttext.util\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:47.272033Z","iopub.execute_input":"2024-08-10T12:19:47.272345Z","iopub.status.idle":"2024-08-10T12:19:50.447946Z","shell.execute_reply.started":"2024-08-10T12:19:47.272319Z","shell.execute_reply":"2024-08-10T12:19:50.446789Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:19:50.449410Z","iopub.execute_input":"2024-08-10T12:19:50.449950Z","iopub.status.idle":"2024-08-10T12:20:21.867259Z","shell.execute_reply.started":"2024-08-10T12:19:50.449909Z","shell.execute_reply":"2024-08-10T12:20:21.865817Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"--2024-08-10 12:19:51--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.82, 13.35.7.128, 13.35.7.50, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.82|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: 'cc.en.300.bin.gz'\n\ncc.en.300.bin.gz    100%[===================>]   4.19G   134MB/s    in 30s     \n\n2024-08-10 12:20:21 (143 MB/s) - 'cc.en.300.bin.gz' saved [4503593528/4503593528]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!gunzip cc.en.300.bin.gz","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:20:21.868987Z","iopub.execute_input":"2024-08-10T12:20:21.869366Z","iopub.status.idle":"2024-08-10T12:21:51.508502Z","shell.execute_reply.started":"2024-08-10T12:20:21.869333Z","shell.execute_reply":"2024-08-10T12:21:51.505691Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"ft = fasttext.load_model('cc.en.300.bin') \nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:21:51.511832Z","iopub.execute_input":"2024-08-10T12:21:51.512356Z","iopub.status.idle":"2024-08-10T12:22:00.775579Z","shell.execute_reply.started":"2024-08-10T12:21:51.512310Z","shell.execute_reply":"2024-08-10T12:22:00.774422Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"300"},"metadata":{}}]},{"cell_type":"markdown","source":"Понизим размерность","metadata":{}},{"cell_type":"code","source":"EMB_SIZE=100","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:00.777371Z","iopub.execute_input":"2024-08-10T12:22:00.777717Z","iopub.status.idle":"2024-08-10T12:22:00.787375Z","shell.execute_reply.started":"2024-08-10T12:22:00.777688Z","shell.execute_reply":"2024-08-10T12:22:00.786205Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"fasttext.util.reduce_model(ft, EMB_SIZE)\nft.get_dimension()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:00.789005Z","iopub.execute_input":"2024-08-10T12:22:00.789427Z","iopub.status.idle":"2024-08-10T12:22:15.301128Z","shell.execute_reply.started":"2024-08-10T12:22:00.789397Z","shell.execute_reply":"2024-08-10T12:22:15.299974Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}]},{"cell_type":"markdown","source":"Будем нашей моделью предсказывать следующий токен: n-грамму fasttext","metadata":{}},{"cell_type":"markdown","source":"Весь текст токенизируем на слова и пунктуацию","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:15.302999Z","iopub.execute_input":"2024-08-10T12:22:15.303489Z","iopub.status.idle":"2024-08-10T12:22:17.422584Z","shell.execute_reply.started":"2024-08-10T12:22:15.303448Z","shell.execute_reply":"2024-08-10T12:22:17.421457Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"quotes_tokenized = [word_tokenize(quote) for quote in quotes]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:17.424154Z","iopub.execute_input":"2024-08-10T12:22:17.424497Z","iopub.status.idle":"2024-08-10T12:22:31.826009Z","shell.execute_reply.started":"2024-08-10T12:22:17.424469Z","shell.execute_reply":"2024-08-10T12:22:31.824827Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"quotes_tokenized[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:31.827522Z","iopub.execute_input":"2024-08-10T12:22:31.827947Z","iopub.status.idle":"2024-08-10T12:22:31.835803Z","shell.execute_reply.started":"2024-08-10T12:22:31.827853Z","shell.execute_reply":"2024-08-10T12:22:31.834656Z"},"trusted":true},"execution_count":98,"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"['If',\n 'you',\n 'live',\n 'to',\n 'be',\n 'a',\n 'hundred',\n ',',\n 'I',\n 'want',\n 'to',\n 'live',\n 'to',\n 'be',\n 'a',\n 'hundred',\n 'minus',\n 'one',\n 'day',\n 'so',\n 'I',\n 'never',\n 'have',\n 'to',\n 'live',\n 'without',\n 'you',\n '.']"},"metadata":{}}]},{"cell_type":"markdown","source":"Будем использовать n-граммы различных размеров","metadata":{}},{"cell_type":"code","source":"import random\nfrom typing import List, Dict\n\nutils_tokens={'<PAD>': 0, '<UNK>': 1}\n\ndef extract_ngrams(word, n):\n    # Function to extract n-grams from text\n    \n    # артикли и пунктуацию добавляем как есть\n    if len(word)==1:\n        return [word]\n    ngrams = [word[i:i+n] for i in range(len(word) - n + 1)]\n    return ngrams\n\ndef build_ngram_vocab(texts_tokenized, ns: List[int], utils_tokens=utils_tokens):\n    ngram_to_index = utils_tokens.copy()\n    ngram_to_index[' '] = len(ngram_to_index)\n    index = len(ngram_to_index)\n    for n in ns:\n        for text in texts_tokenized:\n            for word in text:\n                ngrams = extract_ngrams(word, n)\n                for ngram in ngrams:\n                    if ngram not in ngram_to_index:\n                        ngram_to_index[ngram] = index\n                        index += 1\n    return ngram_to_index","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:31.837528Z","iopub.execute_input":"2024-08-10T12:22:31.838066Z","iopub.status.idle":"2024-08-10T12:22:31.849474Z","shell.execute_reply.started":"2024-08-10T12:22:31.838008Z","shell.execute_reply":"2024-08-10T12:22:31.848356Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# Sample text data\n# texts = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\n\n# Build vocabulary for bigrams (2-grams)\nNGRAMS=[2, 3]\nNGRAM_VOCAB = build_ngram_vocab(quotes_tokenized, NGRAMS)\nNUM_NGRAMS = len(NGRAM_VOCAB)\nNUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:31.851098Z","iopub.execute_input":"2024-08-10T12:22:31.852088Z","iopub.status.idle":"2024-08-10T12:22:36.473097Z","shell.execute_reply.started":"2024-08-10T12:22:31.852026Z","shell.execute_reply":"2024-08-10T12:22:36.471968Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"10986"},"metadata":{}}]},{"cell_type":"code","source":"for key, val in NGRAM_VOCAB.items():\n    if key==' ':\n        print(val)\n        break","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.474637Z","iopub.execute_input":"2024-08-10T12:22:36.475010Z","iopub.status.idle":"2024-08-10T12:22:36.481221Z","shell.execute_reply.started":"2024-08-10T12:22:36.474981Z","shell.execute_reply":"2024-08-10T12:22:36.479877Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create an embedding matrix for n-grams\ndef create_ngram_embedding_matrix(ngram_vocab, ft):\n    emb_size = ft.get_dimension()\n    embedding_matrix = torch.zeros((len(ngram_vocab), emb_size))\n    for ngram, idx in ngram_vocab.items():\n        embedding_matrix[idx] = torch.tensor(ft.get_word_vector(ngram))\n    return embedding_matrix\n\n# Generate embedding matrix\nEMBEDDING_MATRIX = create_ngram_embedding_matrix(NGRAM_VOCAB, ft)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.483170Z","iopub.execute_input":"2024-08-10T12:22:36.483565Z","iopub.status.idle":"2024-08-10T12:22:36.858734Z","shell.execute_reply.started":"2024-08-10T12:22:36.483535Z","shell.execute_reply":"2024-08-10T12:22:36.857444Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.860554Z","iopub.execute_input":"2024-08-10T12:22:36.860942Z","iopub.status.idle":"2024-08-10T12:22:36.869679Z","shell.execute_reply.started":"2024-08-10T12:22:36.860907Z","shell.execute_reply":"2024-08-10T12:22:36.868347Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"torch.Size([10986, 100])"},"metadata":{}}]},{"cell_type":"markdown","source":"Каждое предложение предсатвляется всеми возможными n-граммами, разные по размеру n-граммы отделяются специальным токеном \\<NEW_NGRAM\\>","metadata":{}},{"cell_type":"markdown","source":"Мы будем каждый раз разбивать предложение на n-граммы рандомно","metadata":{}},{"cell_type":"code","source":"def extract_ngrams_of_random_size(word: str, ns: List[int]) -> List[str]:\n    \"\"\"\n    Extract n-grams of randomly chosen sizes from the word.\n    For each n-gram, the size is sampled from the list of possible sizes.\n    \n    :param text: The input text.\n    :param ns: List of possible n-gram sizes to consider.\n    :return: A list of n-grams of randomly chosen sizes.\n    \"\"\"\n    ngrams = []\n    \n    # Iterate over each possible start position of n-grams in the text\n    for start in range(len(word)):\n        # Randomly choose an n-gram size for this start position\n        n = random.choice(ns)\n        \n        # Ensure the size does not exceed the remaining length of the text\n        if start + n <= len(word):\n            ngram = word[start:start + n]\n            ngrams.append(ngram)\n    \n    return ngrams","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.871302Z","iopub.execute_input":"2024-08-10T12:22:36.871654Z","iopub.status.idle":"2024-08-10T12:22:36.880753Z","shell.execute_reply.started":"2024-08-10T12:22:36.871626Z","shell.execute_reply":"2024-08-10T12:22:36.879425Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"NGRAM_VOCAB[' ']","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.882424Z","iopub.execute_input":"2024-08-10T12:22:36.883454Z","iopub.status.idle":"2024-08-10T12:22:36.895533Z","shell.execute_reply.started":"2024-08-10T12:22:36.883420Z","shell.execute_reply":"2024-08-10T12:22:36.894301Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_sent(sentence_tokenized, ngram_vocab, ns, utils_tokens=utils_tokens):\n    \"\"\"Prepare a sentence as embeddings.\"\"\"\n    ngrams = []\n    for word in sentence_tokenized:\n        ngrams.extend(extract_ngrams_of_random_size(word, ns))\n        ngrams.append(' ')\n    ngrams.pop()\n#     print(ngrams)\n    indices=[ngram_vocab.get(ngram, utils_tokens.get('<UNK>')) for ngram in ngrams]\n\n    return torch.LongTensor(indices)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.897186Z","iopub.execute_input":"2024-08-10T12:22:36.897600Z","iopub.status.idle":"2024-08-10T12:22:36.905507Z","shell.execute_reply.started":"2024-08-10T12:22:36.897569Z","shell.execute_reply":"2024-08-10T12:22:36.904216Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"sentences = [\"sample sentence for n-gram extraction\", \"another example sentence\"]\nprepare_sent(word_tokenize(sentences[0]), NGRAM_VOCAB, NGRAMS)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.906956Z","iopub.execute_input":"2024-08-10T12:22:36.907372Z","iopub.status.idle":"2024-08-10T12:22:36.928346Z","shell.execute_reply.started":"2024-08-10T12:22:36.907342Z","shell.execute_reply":"2024-08-10T12:22:36.927094Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"tensor([3228, 3164, 2087,  189,  187,    2,   46,   95, 2392,   71, 2448, 1831,\n           2, 1787,    2, 2358, 4625, 3630, 3012,  230,    2, 2143, 3585,   63,\n        2603, 2604, 2247, 2248, 2164])"},"metadata":{}}]},{"cell_type":"code","source":"NUM_NGRAMS","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.929760Z","iopub.execute_input":"2024-08-10T12:22:36.930148Z","iopub.status.idle":"2024-08-10T12:22:36.936991Z","shell.execute_reply.started":"2024-08-10T12:22:36.930118Z","shell.execute_reply":"2024-08-10T12:22:36.935873Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"10986"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.938883Z","iopub.execute_input":"2024-08-10T12:22:36.939267Z","iopub.status.idle":"2024-08-10T12:22:36.945072Z","shell.execute_reply.started":"2024-08-10T12:22:36.939237Z","shell.execute_reply":"2024-08-10T12:22:36.943820Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"class FastTextEmbLSTMLoop(nn.Module):\n    def __init__(self, device='cpu', num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=128, num_layers=2, embedding_matrix=None):\n        super(FastTextEmbLSTMLoop, self).__init__()\n        self.device = device\n        self.num_tokens = num_tokens\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        \n        # Initialize n-gram embedding layer\n        self.emb = nn.Embedding(num_tokens, emb_size)\n        if embedding_matrix is not None:\n            assert emb_size==embedding_matrix.shape[1]\n            assert num_tokens==embedding_matrix.shape[0]\n            \n            self.emb.weight.data.copy_(embedding_matrix)\n            self.emb.weight.requires_grad = False  # Optional: set to True if you want to fine-tune embeddings\n        \n        # RNN and Linear layers\n        self.rnn = nn.LSTM(input_size=emb_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n        self.hid_to_logits = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size),  # First hidden layer\n            nn.ReLU(),  # Activation function\n            nn.Dropout(0.5),  # Dropout for regularization\n            nn.Linear(self.hidden_size, self.num_tokens),  # Second hidden layer\n        )\n        \n    def forward_logits(self, x, hidden_state=None):\n        if hidden_state is None:\n            hidden_state = self.initial_state(x.shape[0])\n        \n        # Apply embedding layer\n        x_embedded = self.emb(x)\n        \n        # RNN forward pass\n        h_seq, hidden_state = self.rnn(x_embedded, hidden_state)\n        next_logits = self.hid_to_logits(h_seq)\n        return next_logits, hidden_state\n        \n    def forward_hidden(self, x, hidden_state=None):\n        next_logits, hidden_state = self.forward_logits(x, hidden_state)\n        next_logp = F.log_softmax(next_logits, dim=-1)\n        return next_logp, hidden_state\n    \n    def forward(self, x):\n        next_logits, _ = self.forward_logits(x)\n        return next_logits\n    \n    def initial_state(self, batch_size):\n        \"\"\"Return RNN state before it processes the first input (aka h0)\"\"\"\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=self.device)\n        return (h_0, c_0)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.947101Z","iopub.execute_input":"2024-08-10T12:22:36.947443Z","iopub.status.idle":"2024-08-10T12:22:36.964307Z","shell.execute_reply.started":"2024-08-10T12:22:36.947413Z","shell.execute_reply":"2024-08-10T12:22:36.963013Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Т.к. изменили слой эмбеддинга, изменин класс датасета","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.965772Z","iopub.execute_input":"2024-08-10T12:22:36.966219Z","iopub.status.idle":"2024-08-10T12:22:36.981528Z","shell.execute_reply.started":"2024-08-10T12:22:36.966188Z","shell.execute_reply":"2024-08-10T12:22:36.980313Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass NGramDataset(Dataset):\n    def __init__(self, sentences, ngram_vocab=NGRAM_VOCAB, ns=NGRAMS):\n        \"\"\"\n        Args:\n            sentences (list of str): List of sentences (each sentence is a string).\n            ngram_vocab (dict): Dictionary mapping n-grams to indices.\n            n (int): Size of the n-grams (e.g., 2 for bigrams).\n        \"\"\"\n        self.sentences = sentences\n        self.token_to_id = ngram_vocab\n        \n        self.id_to_token = {val:key for key,val in ngram_vocab.items()}\n        \n        self.ns = ns\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        # Prepare the batch for the single sentence\n        return prepare_sent(sentence, self.token_to_id, self.ns)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.983118Z","iopub.execute_input":"2024-08-10T12:22:36.983494Z","iopub.status.idle":"2024-08-10T12:22:36.994684Z","shell.execute_reply.started":"2024-08-10T12:22:36.983463Z","shell.execute_reply":"2024-08-10T12:22:36.992519Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:36.996649Z","iopub.execute_input":"2024-08-10T12:22:36.997688Z","iopub.status.idle":"2024-08-10T12:22:37.006765Z","shell.execute_reply.started":"2024-08-10T12:22:36.997654Z","shell.execute_reply":"2024-08-10T12:22:37.005257Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"quotes_train_dataset = NGramDataset(quotes_tokenized, NGRAM_VOCAB, NGRAMS)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.008320Z","iopub.execute_input":"2024-08-10T12:22:37.008764Z","iopub.status.idle":"2024-08-10T12:22:37.020876Z","shell.execute_reply.started":"2024-08-10T12:22:37.008732Z","shell.execute_reply":"2024-08-10T12:22:37.019471Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.022997Z","iopub.execute_input":"2024-08-10T12:22:37.023414Z","iopub.status.idle":"2024-08-10T12:22:37.034252Z","shell.execute_reply.started":"2024-08-10T12:22:37.023383Z","shell.execute_reply":"2024-08-10T12:22:37.032950Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle variable-length sequences.\"\"\"\n    # `batch` is a list of tensors, each representing a sentence\n    # Pad sequences to the maximum length in this batch\n    return nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=NGRAM_VOCAB['<PAD>'])\n\nquotes_train_dataloader = DataLoader(\n    quotes_train_dataset, \n    batch_size=64, \n    shuffle=True, \n    num_workers=2 * num_gpus,  # 2 workers per GPU (adjust based on performance)\n    pin_memory=True,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.035756Z","iopub.execute_input":"2024-08-10T12:22:37.036172Z","iopub.status.idle":"2024-08-10T12:22:37.046489Z","shell.execute_reply.started":"2024-08-10T12:22:37.036140Z","shell.execute_reply":"2024-08-10T12:22:37.045332Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_MATRIX.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.051987Z","iopub.execute_input":"2024-08-10T12:22:37.052780Z","iopub.status.idle":"2024-08-10T12:22:37.061857Z","shell.execute_reply.started":"2024-08-10T12:22:37.052739Z","shell.execute_reply":"2024-08-10T12:22:37.060495Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"torch.Size([10986, 100])"},"metadata":{}}]},{"cell_type":"code","source":"ft_emb_lstm_model = FastTextEmbLSTMLoop(device, num_tokens=NUM_NGRAMS, emb_size=EMB_SIZE, hidden_size=256, num_layers=4, embedding_matrix=EMBEDDING_MATRIX)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.063716Z","iopub.execute_input":"2024-08-10T12:22:37.064171Z","iopub.status.idle":"2024-08-10T12:22:37.185465Z","shell.execute_reply.started":"2024-08-10T12:22:37.064136Z","shell.execute_reply":"2024-08-10T12:22:37.184245Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"opt = torch.optim.Adam(ft_emb_lstm_model.parameters(), lr=1e-4)\nsched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.7, patience=2)\ncriterion = nn.CrossEntropyLoss()\n\nhistory=[]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:37.187020Z","iopub.execute_input":"2024-08-10T12:22:37.187434Z","iopub.status.idle":"2024-08-10T12:22:38.678145Z","shell.execute_reply.started":"2024-08-10T12:22:37.187399Z","shell.execute_reply":"2024-08-10T12:22:38.677096Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"# Move model to the first GPU device\nft_emb_lstm_model = ft_emb_lstm_model.to(device)\n\n# Use DataParallel to utilize multiple GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    ft_emb_lstm_model = nn.DataParallel(ft_emb_lstm_model)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:38.679416Z","iopub.execute_input":"2024-08-10T12:22:38.679868Z","iopub.status.idle":"2024-08-10T12:22:38.690277Z","shell.execute_reply.started":"2024-08-10T12:22:38.679838Z","shell.execute_reply":"2024-08-10T12:22:38.689053Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"Теперь для подсчета лосса мне нужно разобрать каждое предложение на n-граммы","metadata":{}},{"cell_type":"code","source":"batch_ix = next(iter(quotes_train_dataloader))","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:23.790047Z","start_time":"2019-11-05T18:21:23.715167Z"},"execution":{"iopub.status.busy":"2024-08-10T12:22:38.691713Z","iopub.execute_input":"2024-08-10T12:22:38.692117Z","iopub.status.idle":"2024-08-10T12:22:38.738879Z","shell.execute_reply.started":"2024-08-10T12:22:38.692080Z","shell.execute_reply":"2024-08-10T12:22:38.737692Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"batch_ix.device","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:38.740345Z","iopub.execute_input":"2024-08-10T12:22:38.740656Z","iopub.status.idle":"2024-08-10T12:22:38.747305Z","shell.execute_reply.started":"2024-08-10T12:22:38.740630Z","shell.execute_reply":"2024-08-10T12:22:38.746260Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"code","source":"curr_batch=batch_ix.to(device)\n\nlogp_seq = ft_emb_lstm_model(curr_batch)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:38.748705Z","iopub.execute_input":"2024-08-10T12:22:38.749066Z","iopub.status.idle":"2024-08-10T12:22:41.095573Z","shell.execute_reply.started":"2024-08-10T12:22:38.749024Z","shell.execute_reply":"2024-08-10T12:22:41.094346Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"# compute loss\npredictions_logp = logp_seq[:, :-1]\nactual_next_tokens = curr_batch[:, 1:]\n\nloss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\nloss.backward()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:41.097086Z","iopub.execute_input":"2024-08-10T12:22:41.097537Z","iopub.status.idle":"2024-08-10T12:22:49.052065Z","shell.execute_reply.started":"2024-08-10T12:22:41.097508Z","shell.execute_reply":"2024-08-10T12:22:49.050935Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:49.053795Z","iopub.execute_input":"2024-08-10T12:22:49.054792Z","iopub.status.idle":"2024-08-10T12:22:49.059908Z","shell.execute_reply.started":"2024-08-10T12:22:49.054749Z","shell.execute_reply":"2024-08-10T12:22:49.058763Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:49.061361Z","iopub.execute_input":"2024-08-10T12:22:49.061742Z","iopub.status.idle":"2024-08-10T12:22:49.073837Z","shell.execute_reply.started":"2024-08-10T12:22:49.061703Z","shell.execute_reply":"2024-08-10T12:22:49.072734Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"import copy","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:22:49.075481Z","iopub.execute_input":"2024-08-10T12:22:49.075820Z","iopub.status.idle":"2024-08-10T12:22:49.084547Z","shell.execute_reply.started":"2024-08-10T12:22:49.075784Z","shell.execute_reply":"2024-08-10T12:22:49.083468Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"num_epochs=100\nbest_model=None\nbest_loss=float('inf')\nfor epoch in range(num_epochs):\n    ft_emb_lstm_model.train()\n    total_batches = len(quotes_train_dataloader)\n\n    # Wrap DataLoader iterator with tqdm\n    for i, batch_ix in enumerate(tqdm(quotes_train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=total_batches)):\n\n#         batch_ix = to_matrix(sample(quotes, 32), token_to_id, max_len=MAX_LENGTH)\n#         batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n        curr_batch=batch_ix.to(device)\n\n        logp_seq = ft_emb_lstm_model(curr_batch)\n\n        # compute loss\n        predictions_logp = logp_seq[:, :-1]\n        actual_next_tokens = curr_batch[:, 1:]\n\n        loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n\n        # train with backprop\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        # visualizing training process\n        history.append(loss.cpu().data.numpy())\n        if (i + 1) % 100 == 0:\n            clear_output(True)\n            plt.plot(history,label='loss')\n            plt.legend()\n            plt.show()\n    \n    # Validate the model and calculate the metric\n    ft_emb_lstm_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in quotes_train_dataloader:\n            curr_batch=batch.to(device)\n\n            logp_seq = ft_emb_lstm_model(curr_batch)\n\n            # compute loss\n            predictions_logp = logp_seq[:, :-1]\n            actual_next_tokens = curr_batch[:, 1:]\n\n            loss = criterion(predictions_logp.permute(0,2,1), actual_next_tokens)\n            val_loss += loss.item()\n\n    val_loss /= len(quotes_train_dataloader)\n    \n    if val_loss<best_loss:\n        print(f'Новый лучший лосс: {val_loss}')\n        best_loss=val_loss\n        best_model=copy.deepcopy(ft_emb_lstm_model)\n    \n    print(f'Текущий loss: {val_loss}')\n    \n    # Step the scheduler\n    sched.step(val_loss)\n\n    assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\"","metadata":{"ExecuteTime":{"end_time":"2019-11-05T18:21:31.468107Z","start_time":"2019-11-05T18:21:23.792092Z"},"execution":{"iopub.status.busy":"2024-08-10T12:22:49.086141Z","iopub.execute_input":"2024-08-10T12:22:49.086483Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/100:   0%|          | 1/566 [00:10<1:35:08, 10.10s/it]","output_type":"stream"}]},{"cell_type":"code","source":"# Assume your trained model is wrapped in DataParallel\ntrained_model = best_model\n# trained_model = ft_emb_lstm_model\n\n# Check if the model is wrapped with DataParallel\nif isinstance(trained_model, nn.DataParallel):\n    # Extract the original model\n    trained_model = trained_model.module","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu=trained_model.cpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nnewpath = 'models/task4_RNN_name_generator'\nif not os.path.exists(newpath):\n    os.makedirs(newpath)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:27:36.183361Z","iopub.execute_input":"2024-08-10T12:27:36.183758Z","iopub.status.idle":"2024-08-10T12:27:36.189402Z","shell.execute_reply.started":"2024-08-10T12:27:36.183729Z","shell.execute_reply":"2024-08-10T12:27:36.188076Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pickle \n\nwith open('models/task4_RNN_name_generator/ngram_vocab_dict.pkl', 'wb') as f:\n    pickle.dump(NGRAM_VOCAB, f)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T12:30:06.693428Z","iopub.execute_input":"2024-08-10T12:30:06.693901Z","iopub.status.idle":"2024-08-10T12:30:06.989780Z","shell.execute_reply.started":"2024-08-10T12:30:06.693864Z","shell.execute_reply":"2024-08-10T12:30:06.988208Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/task4_RNN_name_generator/ngram_vocab_dict.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mNGRAM_VOCAB\u001b[49m, f)\n","\u001b[0;31mNameError\u001b[0m: name 'NGRAM_VOCAB' is not defined"],"ename":"NameError","evalue":"name 'NGRAM_VOCAB' is not defined","output_type":"error"}]},{"cell_type":"code","source":"torch.save(trained_model_cpu.state_dict(), 'models/task4_RNN_name_generator/ft_emb_lstm_baseline.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model_cpu.device=torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_sample(ft_emb_torch_rnn, seed_phrase=' ',temperature=1.0, quotes_train_dataset=quotes_train_dataset):\n    '''\n    The function generates text given a phrase of length at least SEQ_LENGTH.\n    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n    :param max_length: maximum output length, including seed_phrase\n    :param temperature: coefficient for sampling. Higher temperature produces more chaotic outputs,\n                        smaller temperature converges to the single most likely output\n    '''\n    \n    # Convert the seed phrase to a sequence of indices\n#     x_sequence = [token_to_id.get(token, 0) for token in seed_phrase]  # Default to 0 if token not found\n#     x_sequence = torch.tensor([x_sequence], dtype=torch.int64).to(char_torch_rnn.device)\n    x_sequence = prepare_sent(seed_phrase, NGRAM_VOCAB, NGRAMS).unsqueeze(0).to(ft_emb_torch_rnn.device)\n    \n    # Initialize the hidden state\n    hid_state = ft_emb_torch_rnn.initial_state(batch_size=1)\n    \n    # If seed_phrase is not just a space, update hidden state based on the seed_phrase\n    if seed_phrase.strip() != '':\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(x_sequence, hid_state)\n    \n    # Start generating text\n    generated_sequence = list(seed_phrase)  # Initialize with the seed phrase\n\n    for _ in range(quotes_train_dataset.max_len - len(seed_phrase)):\n        # Get the logits for the next character\n        next_logits, hid_state = ft_emb_torch_rnn.forward_logits(x_sequence[:, -1].unsqueeze(1), hid_state)\n        \n        # Apply temperature to logits\n        next_logits = next_logits / temperature\n        \n        # Calculate probabilities using softmax\n        p_next = F.softmax(next_logits.squeeze(1), dim=-1).data.cpu().numpy().flatten()\n        \n        # Sample the next character index from the probability distribution\n        next_ix = np.random.choice(len(quotes_train_dataset.token_to_id), p=p_next)\n        \n        # Append the sampled character to the generated sequence\n        generated_sequence.append(quotes_train_dataset.id_to_token[next_ix])\n        \n        # Update the input sequence with the new character\n        next_ix_tensor = torch.tensor([[next_ix]], dtype=torch.int64).to(ft_emb_torch_rnn.device)\n        x_sequence = torch.cat([x_sequence, next_ix_tensor], dim=1)\n\n        # Update hidden state for the next character\n        _, hid_state = ft_emb_torch_rnn.forward_hidden(next_ix_tensor, hid_state)\n    \n    return ''.join(generated_sequence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quotes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in np.linspace(1, 1.5, 10):\n    print(generate_sample(trained_model, seed_phrase='Life ', temperature=t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}